{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadGloveModel():\n",
    "    dim = \"50\"\n",
    "    f = open('../Models/glove.6B/glove.6B.' + dim + 'd.txt','r', encoding='utf-8')\n",
    "    model = {}\n",
    "    for i, line in enumerate(f):\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    f.close()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_model = loadGloveModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "pos_file_names = glob.glob('./train/train/pos/*')\n",
    "neg_file_names = glob.glob('./train/train/neg/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.TweetTokenizer(preserve_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = []\n",
    "for fn in pos_file_names:\n",
    "    with open(fn, encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    content = tokenizer.tokenize(content)\n",
    "    reviews.append((content,1))\n",
    "    \n",
    "for fn in neg_file_names:\n",
    "    with open(fn, encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    content = tokenizer.tokenize(content)\n",
    "    reviews.append((content,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_df = pd.DataFrame(reviews, columns=['review', 'pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_df.to_csv(\"./data/reviews_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def glove_score(review, adj=False):\n",
    "    ''' Gives a glove score against a review using Glove word embedding\n",
    "    \n",
    "        review - list of Str\n",
    "        \n",
    "        returns DataFrame\n",
    "    '''\n",
    "\n",
    "    list_score_df = []\n",
    "    if adj == True:\n",
    "        pos_tagged = nltk.pos_tag(review)\n",
    "        first = True\n",
    "        for i, curr_tuple in enumerate(pos_tagged):\n",
    "            prev_tag = pos_tagged[i-1][1]\n",
    "            prev_word = pos_tagged[i-1][0]\n",
    "            curr_word = curr_tuple[0]\n",
    "            curr_tag = curr_tuple[1]\n",
    "            if curr_tag == 'JJ':\n",
    "                if first:\n",
    "                    if curr_word in glove_model \\\n",
    "                    and curr_word not in stopwords:\n",
    "                        list_score_df.append(glove_model[curr_word])\n",
    "                        first = False\n",
    "                else:\n",
    "                    if curr_word in glove_model \\\n",
    "                    and curr_word not in stopwords:\n",
    "                        if prev_tag == 'RB' and prev_word in glove_model \\\n",
    "                        and prev_word not in stopwords:\n",
    "                            row_df = glove_model[prev_word]\n",
    "                            list_score_df.append(row_df)\n",
    "                        row_df = glove_model[curr_word]\n",
    "                        list_score_df.append(row_df)\n",
    "        try:\n",
    "            score_df = pd.DataFrame(list_score_df)\n",
    "            score_df = np.sum(score_df)\n",
    "            return pd.DataFrame(score_df).transpose()\n",
    "        except ValueError:\n",
    "            print(\"Value Error with this review: \\n\", review)\n",
    "            return {} \n",
    "    else:\n",
    "        for i, curr_word in enumerate(review):\n",
    "            if i == 0:\n",
    "                if curr_word in glove_model \\\n",
    "                and curr_word not in stopwords:\n",
    "                    list_score_df.append(glove_model[curr_word])\n",
    "            else:\n",
    "                if curr_word in glove_model \\\n",
    "                and curr_word not in stopwords:\n",
    "                    row_df = glove_model[curr_word]\n",
    "                    list_score_df.append(row_df)\n",
    "        try:\n",
    "            score_df = pd.DataFrame(list_score_df)\n",
    "            score_df = np.sum(score_df)\n",
    "            return pd.DataFrame(score_df).transpose()\n",
    "        except ValueError:\n",
    "            print(\"Value Error with this review: \\n\", review)\n",
    "            return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nThkAktSp/w8Yb0ylmTRSogAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ee7ba20978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Trying to decide where to cut off the maximum word length\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.hist(reviews_df.iloc[:,0].apply(lambda lst: len(lst)),bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "718.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(reviews_df.iloc[:,0].apply(lambda lst: len(lst)), 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "280.64652"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(reviews_df.iloc[:,0].apply(lambda lst: len(lst)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think 800 would be a good number to encompass over 95% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def glove_full(review, adj=False):\n",
    "    ''' Takes a review and replaces words with word embeddings. No aggregations.\n",
    "    \n",
    "        review - list of Str\n",
    "        \n",
    "        returns DataFrame\n",
    "    '''\n",
    "    glove_dim = 50\n",
    "    max_length = 800\n",
    "    review_vector = np.zeros((max_length, glove_dim))\n",
    "    ind = 0\n",
    "    for i, curr_word in enumerate(review):       \n",
    "        if ind == 800:\n",
    "            break\n",
    "        elif curr_word in glove_model \\\n",
    "        and curr_word not in stopwords:\n",
    "            review_vector[ind, :] = glove_model[curr_word]\n",
    "            ind += 1\n",
    "        elif curr_word not in stopwords:\n",
    "            review_vector[ind, :] = glove_model['<unk>']   # unknown token\n",
    "            ind += 1\n",
    "    return review_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glove_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-8ca5a5eac931>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mscores_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreviews_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0ms_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglove_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreviews_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'glove_score' is not defined"
     ]
    }
   ],
   "source": [
    "# for aggregated glove\n",
    "scores_df = pd.DataFrame()\n",
    "for i, row in reviews_df.iterrows():\n",
    "    s_df = glove_score(reviews_df.iloc[i,0])\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    scores_df = scores_df.append(s_df,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n"
     ]
    }
   ],
   "source": [
    "# for non aggregated glove\n",
    "list_of_review_scores = []\n",
    "for i, row in reviews_df.iterrows():\n",
    "    review_mat = glove_full(reviews_df.iloc[i,0])\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    list_of_review_scores.append(review_mat)\n",
    "    \n",
    "review_scores = np.asarray(list_of_review_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del list_of_review_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 800, 50)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('./data/review_scores.npy', review_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del glove_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jared\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(reviews_df.iloc[:,0], reviews_df.iloc[:,1],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9396"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3104"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = scores_df.iloc[y_train.index]\n",
    "X_test = scores_df.iloc[y_test.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.to_csv(\"./data/X_train\")\n",
    "X_val.to_csv(\"./data/X_val\")\n",
    "X_test.to_csv(\"./data/X_test\")\n",
    "y_train.to_csv(\"./data/y_train\")\n",
    "y_val.to_csv(\"./data/y_val\")\n",
    "y_test.to_csv(\"./data/y_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"./data/X_train\",index_col=0)\n",
    "X_val = pd.read_csv(\"./data/X_val\", index_col=0)\n",
    "X_test = pd.read_csv(\"./data/X_test\", index_col=0)\n",
    "y_train = pd.read_csv(\"./data/y_train\", index_col=0)\n",
    "y_val = pd.read_csv(\"./data/y_val\", index_col=0)\n",
    "y_test = pd.read_csv(\"./data/y_test\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23466</th>\n",
       "      <td>-9.530130</td>\n",
       "      <td>5.256128</td>\n",
       "      <td>6.528821</td>\n",
       "      <td>-5.576692</td>\n",
       "      <td>1.611942</td>\n",
       "      <td>0.557120</td>\n",
       "      <td>-17.161828</td>\n",
       "      <td>12.029130</td>\n",
       "      <td>-0.591008</td>\n",
       "      <td>-158.001536</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.915202</td>\n",
       "      <td>-14.362643</td>\n",
       "      <td>-10.617706</td>\n",
       "      <td>3.466634</td>\n",
       "      <td>-4.418027</td>\n",
       "      <td>18.719916</td>\n",
       "      <td>-2.894185</td>\n",
       "      <td>-2.608892</td>\n",
       "      <td>-7.436052</td>\n",
       "      <td>8.361118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4830</th>\n",
       "      <td>-9.071622</td>\n",
       "      <td>2.856857</td>\n",
       "      <td>-5.939990</td>\n",
       "      <td>-5.659706</td>\n",
       "      <td>0.378291</td>\n",
       "      <td>13.940015</td>\n",
       "      <td>-5.771536</td>\n",
       "      <td>3.221016</td>\n",
       "      <td>3.105647</td>\n",
       "      <td>-124.297551</td>\n",
       "      <td>...</td>\n",
       "      <td>6.918535</td>\n",
       "      <td>-12.327835</td>\n",
       "      <td>2.293610</td>\n",
       "      <td>1.302110</td>\n",
       "      <td>5.220683</td>\n",
       "      <td>-19.774624</td>\n",
       "      <td>2.891981</td>\n",
       "      <td>-15.062416</td>\n",
       "      <td>-2.457781</td>\n",
       "      <td>15.464681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24948</th>\n",
       "      <td>-6.190051</td>\n",
       "      <td>-3.199577</td>\n",
       "      <td>5.211514</td>\n",
       "      <td>-5.897536</td>\n",
       "      <td>-0.182303</td>\n",
       "      <td>9.425621</td>\n",
       "      <td>-12.014888</td>\n",
       "      <td>18.564904</td>\n",
       "      <td>-1.132379</td>\n",
       "      <td>-108.502450</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.656457</td>\n",
       "      <td>-2.219772</td>\n",
       "      <td>-5.192971</td>\n",
       "      <td>-1.484033</td>\n",
       "      <td>11.546587</td>\n",
       "      <td>-13.245098</td>\n",
       "      <td>9.493588</td>\n",
       "      <td>-17.866740</td>\n",
       "      <td>-3.454605</td>\n",
       "      <td>17.845281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23736</th>\n",
       "      <td>-1.062833</td>\n",
       "      <td>-2.315163</td>\n",
       "      <td>-3.424130</td>\n",
       "      <td>-2.142048</td>\n",
       "      <td>-0.125433</td>\n",
       "      <td>8.022907</td>\n",
       "      <td>-11.455393</td>\n",
       "      <td>16.721467</td>\n",
       "      <td>5.193775</td>\n",
       "      <td>-109.108859</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.850501</td>\n",
       "      <td>-5.386972</td>\n",
       "      <td>-3.186071</td>\n",
       "      <td>-0.280276</td>\n",
       "      <td>0.734114</td>\n",
       "      <td>-8.987849</td>\n",
       "      <td>7.224415</td>\n",
       "      <td>-9.832436</td>\n",
       "      <td>-1.555770</td>\n",
       "      <td>12.914127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2379</th>\n",
       "      <td>-12.053769</td>\n",
       "      <td>-9.009227</td>\n",
       "      <td>6.841530</td>\n",
       "      <td>-12.042793</td>\n",
       "      <td>2.928985</td>\n",
       "      <td>11.874726</td>\n",
       "      <td>-24.734865</td>\n",
       "      <td>21.170251</td>\n",
       "      <td>-3.272141</td>\n",
       "      <td>-161.999388</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.767677</td>\n",
       "      <td>-7.178676</td>\n",
       "      <td>8.009605</td>\n",
       "      <td>-7.917645</td>\n",
       "      <td>2.532413</td>\n",
       "      <td>-3.049287</td>\n",
       "      <td>8.429122</td>\n",
       "      <td>-22.099984</td>\n",
       "      <td>1.411121</td>\n",
       "      <td>31.630693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23526</th>\n",
       "      <td>-0.707302</td>\n",
       "      <td>7.095887</td>\n",
       "      <td>10.054959</td>\n",
       "      <td>-9.228204</td>\n",
       "      <td>-0.063536</td>\n",
       "      <td>23.897032</td>\n",
       "      <td>-58.378764</td>\n",
       "      <td>53.419397</td>\n",
       "      <td>1.213361</td>\n",
       "      <td>-376.578505</td>\n",
       "      <td>...</td>\n",
       "      <td>-15.783971</td>\n",
       "      <td>2.203979</td>\n",
       "      <td>-22.033561</td>\n",
       "      <td>-14.086114</td>\n",
       "      <td>11.929373</td>\n",
       "      <td>-13.709570</td>\n",
       "      <td>26.395440</td>\n",
       "      <td>-25.057489</td>\n",
       "      <td>-13.350622</td>\n",
       "      <td>33.441044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15168</th>\n",
       "      <td>-3.222536</td>\n",
       "      <td>3.121683</td>\n",
       "      <td>4.156988</td>\n",
       "      <td>-6.959905</td>\n",
       "      <td>7.142618</td>\n",
       "      <td>11.815751</td>\n",
       "      <td>-13.505236</td>\n",
       "      <td>17.294490</td>\n",
       "      <td>-0.605547</td>\n",
       "      <td>-94.353434</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.507356</td>\n",
       "      <td>-4.052887</td>\n",
       "      <td>-6.261077</td>\n",
       "      <td>-1.646614</td>\n",
       "      <td>-1.030465</td>\n",
       "      <td>-3.727235</td>\n",
       "      <td>3.517439</td>\n",
       "      <td>-12.853355</td>\n",
       "      <td>2.797053</td>\n",
       "      <td>7.115327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5539</th>\n",
       "      <td>-36.023182</td>\n",
       "      <td>35.783280</td>\n",
       "      <td>23.638059</td>\n",
       "      <td>-33.476221</td>\n",
       "      <td>23.090962</td>\n",
       "      <td>41.879738</td>\n",
       "      <td>-45.798463</td>\n",
       "      <td>55.679137</td>\n",
       "      <td>5.525473</td>\n",
       "      <td>-632.572143</td>\n",
       "      <td>...</td>\n",
       "      <td>-40.299987</td>\n",
       "      <td>-29.439347</td>\n",
       "      <td>-50.505792</td>\n",
       "      <td>13.707982</td>\n",
       "      <td>18.225685</td>\n",
       "      <td>6.371740</td>\n",
       "      <td>-11.089697</td>\n",
       "      <td>-36.399549</td>\n",
       "      <td>-11.360035</td>\n",
       "      <td>26.103479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7575</th>\n",
       "      <td>-1.955456</td>\n",
       "      <td>-2.541169</td>\n",
       "      <td>3.302431</td>\n",
       "      <td>-12.418295</td>\n",
       "      <td>-18.010573</td>\n",
       "      <td>13.837541</td>\n",
       "      <td>-16.857491</td>\n",
       "      <td>20.017110</td>\n",
       "      <td>1.982764</td>\n",
       "      <td>-128.073439</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.488233</td>\n",
       "      <td>0.155041</td>\n",
       "      <td>-8.392291</td>\n",
       "      <td>-9.963056</td>\n",
       "      <td>12.395026</td>\n",
       "      <td>-5.209892</td>\n",
       "      <td>0.391239</td>\n",
       "      <td>-14.379635</td>\n",
       "      <td>-1.777981</td>\n",
       "      <td>28.843793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14242</th>\n",
       "      <td>-8.261153</td>\n",
       "      <td>-4.963909</td>\n",
       "      <td>-0.235948</td>\n",
       "      <td>-3.660342</td>\n",
       "      <td>-3.359656</td>\n",
       "      <td>6.031268</td>\n",
       "      <td>0.092088</td>\n",
       "      <td>-0.482050</td>\n",
       "      <td>9.402231</td>\n",
       "      <td>-99.443276</td>\n",
       "      <td>...</td>\n",
       "      <td>0.755983</td>\n",
       "      <td>-4.640630</td>\n",
       "      <td>-1.060889</td>\n",
       "      <td>7.147705</td>\n",
       "      <td>0.146469</td>\n",
       "      <td>-7.309369</td>\n",
       "      <td>3.796514</td>\n",
       "      <td>-8.246084</td>\n",
       "      <td>1.834856</td>\n",
       "      <td>19.446099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3012</th>\n",
       "      <td>-5.554060</td>\n",
       "      <td>1.896317</td>\n",
       "      <td>3.828266</td>\n",
       "      <td>-5.280357</td>\n",
       "      <td>-1.226471</td>\n",
       "      <td>5.379012</td>\n",
       "      <td>-11.946574</td>\n",
       "      <td>9.848292</td>\n",
       "      <td>4.410375</td>\n",
       "      <td>-142.011628</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.965625</td>\n",
       "      <td>-15.916383</td>\n",
       "      <td>-2.618227</td>\n",
       "      <td>2.235742</td>\n",
       "      <td>8.116533</td>\n",
       "      <td>-10.085681</td>\n",
       "      <td>3.528779</td>\n",
       "      <td>-20.083711</td>\n",
       "      <td>-1.709733</td>\n",
       "      <td>3.075917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4106</th>\n",
       "      <td>-7.731691</td>\n",
       "      <td>-2.459560</td>\n",
       "      <td>-2.054262</td>\n",
       "      <td>-4.834395</td>\n",
       "      <td>-2.748446</td>\n",
       "      <td>12.504790</td>\n",
       "      <td>-13.431412</td>\n",
       "      <td>7.913226</td>\n",
       "      <td>-0.904062</td>\n",
       "      <td>-74.198070</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.241437</td>\n",
       "      <td>-3.160958</td>\n",
       "      <td>0.882059</td>\n",
       "      <td>-2.047996</td>\n",
       "      <td>3.106531</td>\n",
       "      <td>-8.482807</td>\n",
       "      <td>3.680201</td>\n",
       "      <td>-11.099233</td>\n",
       "      <td>0.431697</td>\n",
       "      <td>14.013174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22407</th>\n",
       "      <td>0.881478</td>\n",
       "      <td>-1.758025</td>\n",
       "      <td>1.097718</td>\n",
       "      <td>-0.959351</td>\n",
       "      <td>-3.677681</td>\n",
       "      <td>8.860155</td>\n",
       "      <td>-9.180261</td>\n",
       "      <td>15.639782</td>\n",
       "      <td>0.464764</td>\n",
       "      <td>-98.869372</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.493997</td>\n",
       "      <td>2.315805</td>\n",
       "      <td>-1.809286</td>\n",
       "      <td>-2.076633</td>\n",
       "      <td>8.181276</td>\n",
       "      <td>-11.883284</td>\n",
       "      <td>3.298349</td>\n",
       "      <td>-23.153842</td>\n",
       "      <td>-0.285598</td>\n",
       "      <td>13.349677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19119</th>\n",
       "      <td>-8.418785</td>\n",
       "      <td>6.769827</td>\n",
       "      <td>-0.807128</td>\n",
       "      <td>-8.165559</td>\n",
       "      <td>-12.831338</td>\n",
       "      <td>18.711402</td>\n",
       "      <td>-31.763460</td>\n",
       "      <td>28.586823</td>\n",
       "      <td>-4.600947</td>\n",
       "      <td>-222.282175</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.529511</td>\n",
       "      <td>-12.561794</td>\n",
       "      <td>-13.462274</td>\n",
       "      <td>0.313848</td>\n",
       "      <td>8.381645</td>\n",
       "      <td>-20.502949</td>\n",
       "      <td>18.158500</td>\n",
       "      <td>-16.914925</td>\n",
       "      <td>-10.841430</td>\n",
       "      <td>35.497994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8645</th>\n",
       "      <td>-5.434493</td>\n",
       "      <td>-1.812939</td>\n",
       "      <td>-3.633009</td>\n",
       "      <td>-3.390656</td>\n",
       "      <td>2.282461</td>\n",
       "      <td>8.917680</td>\n",
       "      <td>-6.118245</td>\n",
       "      <td>3.854455</td>\n",
       "      <td>6.916669</td>\n",
       "      <td>-98.319867</td>\n",
       "      <td>...</td>\n",
       "      <td>4.205456</td>\n",
       "      <td>-2.703624</td>\n",
       "      <td>-13.373855</td>\n",
       "      <td>13.167686</td>\n",
       "      <td>-0.799346</td>\n",
       "      <td>-7.678902</td>\n",
       "      <td>3.269016</td>\n",
       "      <td>-8.293049</td>\n",
       "      <td>3.289054</td>\n",
       "      <td>16.729202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13306</th>\n",
       "      <td>-5.847624</td>\n",
       "      <td>-4.752955</td>\n",
       "      <td>-1.508078</td>\n",
       "      <td>-9.046967</td>\n",
       "      <td>-0.240309</td>\n",
       "      <td>7.068756</td>\n",
       "      <td>-9.246573</td>\n",
       "      <td>8.626601</td>\n",
       "      <td>12.275572</td>\n",
       "      <td>-122.976076</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.272587</td>\n",
       "      <td>-8.963465</td>\n",
       "      <td>-6.821433</td>\n",
       "      <td>6.279054</td>\n",
       "      <td>5.900607</td>\n",
       "      <td>0.540821</td>\n",
       "      <td>-2.708883</td>\n",
       "      <td>-17.232246</td>\n",
       "      <td>-1.559909</td>\n",
       "      <td>13.246264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8373</th>\n",
       "      <td>-2.346229</td>\n",
       "      <td>3.308935</td>\n",
       "      <td>-3.367014</td>\n",
       "      <td>-1.188032</td>\n",
       "      <td>-0.769465</td>\n",
       "      <td>1.391796</td>\n",
       "      <td>-7.609906</td>\n",
       "      <td>-0.146840</td>\n",
       "      <td>-2.651701</td>\n",
       "      <td>-45.773696</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.766511</td>\n",
       "      <td>-1.009266</td>\n",
       "      <td>-4.396433</td>\n",
       "      <td>3.415302</td>\n",
       "      <td>-3.536441</td>\n",
       "      <td>2.912895</td>\n",
       "      <td>-4.924084</td>\n",
       "      <td>-5.794489</td>\n",
       "      <td>-0.542816</td>\n",
       "      <td>6.253063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22030</th>\n",
       "      <td>-32.897000</td>\n",
       "      <td>18.896411</td>\n",
       "      <td>13.306919</td>\n",
       "      <td>-6.041707</td>\n",
       "      <td>-19.415635</td>\n",
       "      <td>73.952208</td>\n",
       "      <td>-54.323985</td>\n",
       "      <td>89.823002</td>\n",
       "      <td>-11.531563</td>\n",
       "      <td>-509.971428</td>\n",
       "      <td>...</td>\n",
       "      <td>-52.849037</td>\n",
       "      <td>-30.760297</td>\n",
       "      <td>-28.494524</td>\n",
       "      <td>-22.233624</td>\n",
       "      <td>33.285767</td>\n",
       "      <td>-38.965658</td>\n",
       "      <td>21.433073</td>\n",
       "      <td>-89.992800</td>\n",
       "      <td>2.317492</td>\n",
       "      <td>84.355632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10886</th>\n",
       "      <td>-7.601511</td>\n",
       "      <td>2.204222</td>\n",
       "      <td>-1.132577</td>\n",
       "      <td>-1.324786</td>\n",
       "      <td>-0.729523</td>\n",
       "      <td>13.538328</td>\n",
       "      <td>-12.208080</td>\n",
       "      <td>5.959574</td>\n",
       "      <td>9.980828</td>\n",
       "      <td>-102.652060</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.353372</td>\n",
       "      <td>-8.672554</td>\n",
       "      <td>-6.181036</td>\n",
       "      <td>1.191237</td>\n",
       "      <td>0.318106</td>\n",
       "      <td>-7.293444</td>\n",
       "      <td>-2.095083</td>\n",
       "      <td>-9.531925</td>\n",
       "      <td>-5.926222</td>\n",
       "      <td>6.964731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21010</th>\n",
       "      <td>-6.012786</td>\n",
       "      <td>-8.357166</td>\n",
       "      <td>1.421948</td>\n",
       "      <td>-9.930718</td>\n",
       "      <td>2.707386</td>\n",
       "      <td>0.039566</td>\n",
       "      <td>-10.217367</td>\n",
       "      <td>0.765258</td>\n",
       "      <td>2.118605</td>\n",
       "      <td>-71.417318</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.617392</td>\n",
       "      <td>2.211529</td>\n",
       "      <td>-6.714586</td>\n",
       "      <td>2.965684</td>\n",
       "      <td>-7.075214</td>\n",
       "      <td>9.231201</td>\n",
       "      <td>-2.176942</td>\n",
       "      <td>-2.062894</td>\n",
       "      <td>-6.245191</td>\n",
       "      <td>12.928256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12980</th>\n",
       "      <td>0.396388</td>\n",
       "      <td>0.132779</td>\n",
       "      <td>0.974491</td>\n",
       "      <td>-0.113261</td>\n",
       "      <td>3.862475</td>\n",
       "      <td>1.880374</td>\n",
       "      <td>-6.621835</td>\n",
       "      <td>3.335962</td>\n",
       "      <td>11.374757</td>\n",
       "      <td>-100.504739</td>\n",
       "      <td>...</td>\n",
       "      <td>2.326234</td>\n",
       "      <td>-7.663560</td>\n",
       "      <td>1.846511</td>\n",
       "      <td>8.339030</td>\n",
       "      <td>7.506838</td>\n",
       "      <td>0.315843</td>\n",
       "      <td>4.434928</td>\n",
       "      <td>-13.483728</td>\n",
       "      <td>-8.313598</td>\n",
       "      <td>9.126546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21975</th>\n",
       "      <td>1.957614</td>\n",
       "      <td>-0.169524</td>\n",
       "      <td>3.290817</td>\n",
       "      <td>-2.941709</td>\n",
       "      <td>0.312306</td>\n",
       "      <td>14.353746</td>\n",
       "      <td>-19.177499</td>\n",
       "      <td>28.904262</td>\n",
       "      <td>-0.884821</td>\n",
       "      <td>-161.991833</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.555810</td>\n",
       "      <td>-3.948519</td>\n",
       "      <td>-6.033382</td>\n",
       "      <td>-2.806161</td>\n",
       "      <td>3.020891</td>\n",
       "      <td>-12.469153</td>\n",
       "      <td>7.073688</td>\n",
       "      <td>-27.512704</td>\n",
       "      <td>1.689985</td>\n",
       "      <td>23.879937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9104</th>\n",
       "      <td>-13.422149</td>\n",
       "      <td>12.692577</td>\n",
       "      <td>17.382467</td>\n",
       "      <td>-34.299658</td>\n",
       "      <td>15.938077</td>\n",
       "      <td>42.028622</td>\n",
       "      <td>-49.840546</td>\n",
       "      <td>65.272382</td>\n",
       "      <td>-28.027808</td>\n",
       "      <td>-366.934458</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.182918</td>\n",
       "      <td>-24.589132</td>\n",
       "      <td>-21.684868</td>\n",
       "      <td>6.486776</td>\n",
       "      <td>11.376559</td>\n",
       "      <td>29.340348</td>\n",
       "      <td>18.350683</td>\n",
       "      <td>-77.868315</td>\n",
       "      <td>-38.073706</td>\n",
       "      <td>60.898119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24338</th>\n",
       "      <td>2.192088</td>\n",
       "      <td>2.619486</td>\n",
       "      <td>2.303559</td>\n",
       "      <td>-0.353052</td>\n",
       "      <td>-1.382333</td>\n",
       "      <td>3.706707</td>\n",
       "      <td>-6.628287</td>\n",
       "      <td>5.384666</td>\n",
       "      <td>2.506913</td>\n",
       "      <td>-56.495236</td>\n",
       "      <td>...</td>\n",
       "      <td>0.757246</td>\n",
       "      <td>-3.366980</td>\n",
       "      <td>-4.395236</td>\n",
       "      <td>-0.111896</td>\n",
       "      <td>2.967986</td>\n",
       "      <td>-2.648702</td>\n",
       "      <td>-0.494547</td>\n",
       "      <td>-4.051354</td>\n",
       "      <td>-0.731598</td>\n",
       "      <td>4.311168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19647</th>\n",
       "      <td>-12.090031</td>\n",
       "      <td>5.041478</td>\n",
       "      <td>-4.180249</td>\n",
       "      <td>-1.550399</td>\n",
       "      <td>3.297170</td>\n",
       "      <td>8.866880</td>\n",
       "      <td>-6.379341</td>\n",
       "      <td>18.084499</td>\n",
       "      <td>5.478194</td>\n",
       "      <td>-128.406414</td>\n",
       "      <td>...</td>\n",
       "      <td>2.910437</td>\n",
       "      <td>-11.005637</td>\n",
       "      <td>-6.893380</td>\n",
       "      <td>4.272665</td>\n",
       "      <td>-0.255134</td>\n",
       "      <td>-3.068025</td>\n",
       "      <td>4.884244</td>\n",
       "      <td>-15.787470</td>\n",
       "      <td>-0.519110</td>\n",
       "      <td>23.777695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20702</th>\n",
       "      <td>-7.307460</td>\n",
       "      <td>4.628349</td>\n",
       "      <td>-0.173648</td>\n",
       "      <td>-5.662859</td>\n",
       "      <td>7.411239</td>\n",
       "      <td>-5.214142</td>\n",
       "      <td>-4.160598</td>\n",
       "      <td>13.425172</td>\n",
       "      <td>16.824955</td>\n",
       "      <td>-155.809686</td>\n",
       "      <td>...</td>\n",
       "      <td>2.850884</td>\n",
       "      <td>-7.024557</td>\n",
       "      <td>-6.415440</td>\n",
       "      <td>4.199648</td>\n",
       "      <td>8.099173</td>\n",
       "      <td>3.230869</td>\n",
       "      <td>-4.172433</td>\n",
       "      <td>-16.184178</td>\n",
       "      <td>-3.420766</td>\n",
       "      <td>4.595183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19232</th>\n",
       "      <td>-20.750644</td>\n",
       "      <td>-20.508117</td>\n",
       "      <td>-9.838239</td>\n",
       "      <td>-16.844271</td>\n",
       "      <td>2.455607</td>\n",
       "      <td>28.743316</td>\n",
       "      <td>-36.740112</td>\n",
       "      <td>18.508811</td>\n",
       "      <td>-10.020760</td>\n",
       "      <td>-344.775355</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.040572</td>\n",
       "      <td>-9.460420</td>\n",
       "      <td>5.789862</td>\n",
       "      <td>-0.618516</td>\n",
       "      <td>12.788148</td>\n",
       "      <td>-23.100120</td>\n",
       "      <td>10.157858</td>\n",
       "      <td>-34.759542</td>\n",
       "      <td>-12.670676</td>\n",
       "      <td>61.162983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6885</th>\n",
       "      <td>-13.504982</td>\n",
       "      <td>8.727747</td>\n",
       "      <td>6.811440</td>\n",
       "      <td>-20.369815</td>\n",
       "      <td>8.270415</td>\n",
       "      <td>9.842857</td>\n",
       "      <td>-39.389114</td>\n",
       "      <td>25.467694</td>\n",
       "      <td>-0.150059</td>\n",
       "      <td>-273.738859</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.746224</td>\n",
       "      <td>-3.012283</td>\n",
       "      <td>-12.152158</td>\n",
       "      <td>-3.566240</td>\n",
       "      <td>9.784496</td>\n",
       "      <td>4.542156</td>\n",
       "      <td>6.175564</td>\n",
       "      <td>-19.289295</td>\n",
       "      <td>-20.192969</td>\n",
       "      <td>22.686837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16015</th>\n",
       "      <td>0.309664</td>\n",
       "      <td>-3.233749</td>\n",
       "      <td>3.705785</td>\n",
       "      <td>-0.239360</td>\n",
       "      <td>-3.159931</td>\n",
       "      <td>8.829213</td>\n",
       "      <td>-4.471591</td>\n",
       "      <td>13.152434</td>\n",
       "      <td>-4.098528</td>\n",
       "      <td>-45.815503</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.503885</td>\n",
       "      <td>1.400501</td>\n",
       "      <td>-5.122602</td>\n",
       "      <td>-1.656916</td>\n",
       "      <td>3.998982</td>\n",
       "      <td>-11.292749</td>\n",
       "      <td>3.371133</td>\n",
       "      <td>-12.216825</td>\n",
       "      <td>2.857740</td>\n",
       "      <td>7.545529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14477</th>\n",
       "      <td>-28.453490</td>\n",
       "      <td>5.829622</td>\n",
       "      <td>15.407428</td>\n",
       "      <td>-32.003428</td>\n",
       "      <td>20.243388</td>\n",
       "      <td>29.041429</td>\n",
       "      <td>-89.036082</td>\n",
       "      <td>51.300486</td>\n",
       "      <td>-6.721999</td>\n",
       "      <td>-682.414391</td>\n",
       "      <td>...</td>\n",
       "      <td>-20.049204</td>\n",
       "      <td>-39.344662</td>\n",
       "      <td>-5.305472</td>\n",
       "      <td>21.819538</td>\n",
       "      <td>30.174680</td>\n",
       "      <td>-5.725104</td>\n",
       "      <td>30.880483</td>\n",
       "      <td>-81.438885</td>\n",
       "      <td>-52.801822</td>\n",
       "      <td>99.929113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3691</th>\n",
       "      <td>2.744640</td>\n",
       "      <td>-6.524552</td>\n",
       "      <td>6.699210</td>\n",
       "      <td>-8.405260</td>\n",
       "      <td>-3.912137</td>\n",
       "      <td>11.253881</td>\n",
       "      <td>-11.474327</td>\n",
       "      <td>25.848899</td>\n",
       "      <td>-11.106596</td>\n",
       "      <td>-89.521008</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.087630</td>\n",
       "      <td>2.250200</td>\n",
       "      <td>-4.914206</td>\n",
       "      <td>-5.113750</td>\n",
       "      <td>9.255205</td>\n",
       "      <td>-9.328271</td>\n",
       "      <td>7.577898</td>\n",
       "      <td>-23.522743</td>\n",
       "      <td>-0.869034</td>\n",
       "      <td>10.783981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20273</th>\n",
       "      <td>-3.933806</td>\n",
       "      <td>-3.366899</td>\n",
       "      <td>-0.410038</td>\n",
       "      <td>-6.591527</td>\n",
       "      <td>-0.032318</td>\n",
       "      <td>12.344045</td>\n",
       "      <td>-13.485825</td>\n",
       "      <td>23.172090</td>\n",
       "      <td>-4.348411</td>\n",
       "      <td>-103.051452</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.249912</td>\n",
       "      <td>2.644893</td>\n",
       "      <td>-5.863980</td>\n",
       "      <td>-10.325793</td>\n",
       "      <td>6.971148</td>\n",
       "      <td>-14.710165</td>\n",
       "      <td>5.284609</td>\n",
       "      <td>-26.296990</td>\n",
       "      <td>9.578152</td>\n",
       "      <td>15.921844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24169</th>\n",
       "      <td>-7.112644</td>\n",
       "      <td>-0.353493</td>\n",
       "      <td>-0.772476</td>\n",
       "      <td>-13.336055</td>\n",
       "      <td>-1.578966</td>\n",
       "      <td>7.536137</td>\n",
       "      <td>-16.494847</td>\n",
       "      <td>19.164431</td>\n",
       "      <td>0.164877</td>\n",
       "      <td>-163.247252</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.768125</td>\n",
       "      <td>-7.860130</td>\n",
       "      <td>-7.418351</td>\n",
       "      <td>8.533393</td>\n",
       "      <td>8.632243</td>\n",
       "      <td>1.370027</td>\n",
       "      <td>1.324204</td>\n",
       "      <td>-19.087657</td>\n",
       "      <td>-4.347405</td>\n",
       "      <td>9.538978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1083</th>\n",
       "      <td>-11.651399</td>\n",
       "      <td>-4.325602</td>\n",
       "      <td>1.705322</td>\n",
       "      <td>-9.364930</td>\n",
       "      <td>10.837328</td>\n",
       "      <td>4.326129</td>\n",
       "      <td>-16.583325</td>\n",
       "      <td>3.460597</td>\n",
       "      <td>1.656298</td>\n",
       "      <td>-207.608604</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.454272</td>\n",
       "      <td>-4.694391</td>\n",
       "      <td>-2.185712</td>\n",
       "      <td>5.117630</td>\n",
       "      <td>13.293782</td>\n",
       "      <td>6.781688</td>\n",
       "      <td>-4.249224</td>\n",
       "      <td>-17.030843</td>\n",
       "      <td>-11.562210</td>\n",
       "      <td>17.677011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2956</th>\n",
       "      <td>-0.355247</td>\n",
       "      <td>-2.041020</td>\n",
       "      <td>1.802194</td>\n",
       "      <td>-3.053436</td>\n",
       "      <td>-3.789694</td>\n",
       "      <td>8.674442</td>\n",
       "      <td>-12.139471</td>\n",
       "      <td>9.671207</td>\n",
       "      <td>0.415015</td>\n",
       "      <td>-109.308416</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.924787</td>\n",
       "      <td>-1.195256</td>\n",
       "      <td>-8.928538</td>\n",
       "      <td>-2.260764</td>\n",
       "      <td>-9.292087</td>\n",
       "      <td>-3.871352</td>\n",
       "      <td>4.938271</td>\n",
       "      <td>-0.003243</td>\n",
       "      <td>-5.057008</td>\n",
       "      <td>7.558971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12449</th>\n",
       "      <td>-31.378552</td>\n",
       "      <td>7.912938</td>\n",
       "      <td>14.452253</td>\n",
       "      <td>-23.131662</td>\n",
       "      <td>-1.084160</td>\n",
       "      <td>18.562102</td>\n",
       "      <td>-47.528697</td>\n",
       "      <td>64.981143</td>\n",
       "      <td>2.448383</td>\n",
       "      <td>-514.758425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798693</td>\n",
       "      <td>-18.960250</td>\n",
       "      <td>-34.247080</td>\n",
       "      <td>24.671551</td>\n",
       "      <td>15.406835</td>\n",
       "      <td>-0.063786</td>\n",
       "      <td>3.607893</td>\n",
       "      <td>-45.440681</td>\n",
       "      <td>-28.085283</td>\n",
       "      <td>44.416185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9761</th>\n",
       "      <td>-9.794826</td>\n",
       "      <td>-8.820186</td>\n",
       "      <td>1.018776</td>\n",
       "      <td>-3.450039</td>\n",
       "      <td>-5.107981</td>\n",
       "      <td>7.239627</td>\n",
       "      <td>-8.242550</td>\n",
       "      <td>7.567896</td>\n",
       "      <td>7.481587</td>\n",
       "      <td>-126.254677</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.819538</td>\n",
       "      <td>-8.080878</td>\n",
       "      <td>-0.005063</td>\n",
       "      <td>10.137972</td>\n",
       "      <td>9.684218</td>\n",
       "      <td>-11.427275</td>\n",
       "      <td>-2.986698</td>\n",
       "      <td>-2.049864</td>\n",
       "      <td>-5.626375</td>\n",
       "      <td>16.481657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8776</th>\n",
       "      <td>-24.685092</td>\n",
       "      <td>-7.759567</td>\n",
       "      <td>5.109131</td>\n",
       "      <td>-4.766638</td>\n",
       "      <td>-4.868717</td>\n",
       "      <td>40.401091</td>\n",
       "      <td>-29.430465</td>\n",
       "      <td>43.994741</td>\n",
       "      <td>-6.599786</td>\n",
       "      <td>-230.334011</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.438599</td>\n",
       "      <td>1.801730</td>\n",
       "      <td>-10.042653</td>\n",
       "      <td>0.723458</td>\n",
       "      <td>-4.317535</td>\n",
       "      <td>-7.923604</td>\n",
       "      <td>-2.473023</td>\n",
       "      <td>-22.690875</td>\n",
       "      <td>-2.631586</td>\n",
       "      <td>27.632942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15846</th>\n",
       "      <td>-2.730303</td>\n",
       "      <td>-3.232159</td>\n",
       "      <td>8.799583</td>\n",
       "      <td>-3.278385</td>\n",
       "      <td>-1.042008</td>\n",
       "      <td>10.174993</td>\n",
       "      <td>-17.070490</td>\n",
       "      <td>12.034405</td>\n",
       "      <td>-4.832986</td>\n",
       "      <td>-96.848990</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.464216</td>\n",
       "      <td>-0.973825</td>\n",
       "      <td>-7.130933</td>\n",
       "      <td>-5.594340</td>\n",
       "      <td>-5.955897</td>\n",
       "      <td>-10.028537</td>\n",
       "      <td>0.988557</td>\n",
       "      <td>-12.527120</td>\n",
       "      <td>-1.873470</td>\n",
       "      <td>15.226022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3370</th>\n",
       "      <td>-12.390107</td>\n",
       "      <td>4.482237</td>\n",
       "      <td>0.362834</td>\n",
       "      <td>-8.526055</td>\n",
       "      <td>4.270219</td>\n",
       "      <td>8.209192</td>\n",
       "      <td>-6.045477</td>\n",
       "      <td>3.544666</td>\n",
       "      <td>3.380720</td>\n",
       "      <td>-107.889926</td>\n",
       "      <td>...</td>\n",
       "      <td>4.002229</td>\n",
       "      <td>-6.756899</td>\n",
       "      <td>-6.498502</td>\n",
       "      <td>2.813978</td>\n",
       "      <td>-2.386410</td>\n",
       "      <td>-0.617349</td>\n",
       "      <td>-2.734974</td>\n",
       "      <td>-6.136994</td>\n",
       "      <td>-1.637426</td>\n",
       "      <td>2.392558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23071</th>\n",
       "      <td>-16.441008</td>\n",
       "      <td>-0.971972</td>\n",
       "      <td>5.148430</td>\n",
       "      <td>-10.111019</td>\n",
       "      <td>2.313799</td>\n",
       "      <td>11.186152</td>\n",
       "      <td>-6.530899</td>\n",
       "      <td>3.688398</td>\n",
       "      <td>2.494096</td>\n",
       "      <td>-120.342170</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.446258</td>\n",
       "      <td>-7.888408</td>\n",
       "      <td>-4.652338</td>\n",
       "      <td>5.086521</td>\n",
       "      <td>6.424604</td>\n",
       "      <td>14.565184</td>\n",
       "      <td>-0.445434</td>\n",
       "      <td>-15.801587</td>\n",
       "      <td>-9.528174</td>\n",
       "      <td>16.307462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5320</th>\n",
       "      <td>-5.478957</td>\n",
       "      <td>3.779384</td>\n",
       "      <td>7.270483</td>\n",
       "      <td>-20.059094</td>\n",
       "      <td>-2.463570</td>\n",
       "      <td>17.980507</td>\n",
       "      <td>-19.417351</td>\n",
       "      <td>33.365423</td>\n",
       "      <td>-8.300340</td>\n",
       "      <td>-158.569343</td>\n",
       "      <td>...</td>\n",
       "      <td>-15.687331</td>\n",
       "      <td>-4.402992</td>\n",
       "      <td>-19.383372</td>\n",
       "      <td>2.097127</td>\n",
       "      <td>18.016863</td>\n",
       "      <td>-0.775409</td>\n",
       "      <td>7.353597</td>\n",
       "      <td>-22.191708</td>\n",
       "      <td>0.369790</td>\n",
       "      <td>11.033066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6678</th>\n",
       "      <td>-1.487272</td>\n",
       "      <td>2.143526</td>\n",
       "      <td>0.722507</td>\n",
       "      <td>-2.769477</td>\n",
       "      <td>-1.502194</td>\n",
       "      <td>2.555049</td>\n",
       "      <td>-4.698054</td>\n",
       "      <td>9.237486</td>\n",
       "      <td>2.368931</td>\n",
       "      <td>-59.898915</td>\n",
       "      <td>...</td>\n",
       "      <td>1.048051</td>\n",
       "      <td>1.976198</td>\n",
       "      <td>-4.706003</td>\n",
       "      <td>-0.402661</td>\n",
       "      <td>2.811744</td>\n",
       "      <td>4.698605</td>\n",
       "      <td>-2.704089</td>\n",
       "      <td>-9.996537</td>\n",
       "      <td>1.702047</td>\n",
       "      <td>-4.528905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15344</th>\n",
       "      <td>-18.614854</td>\n",
       "      <td>0.956340</td>\n",
       "      <td>2.551893</td>\n",
       "      <td>-14.389428</td>\n",
       "      <td>20.680227</td>\n",
       "      <td>27.111285</td>\n",
       "      <td>-54.782512</td>\n",
       "      <td>26.390859</td>\n",
       "      <td>16.386493</td>\n",
       "      <td>-473.174670</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.761137</td>\n",
       "      <td>-5.375424</td>\n",
       "      <td>-19.493218</td>\n",
       "      <td>8.167412</td>\n",
       "      <td>5.877929</td>\n",
       "      <td>-39.082685</td>\n",
       "      <td>0.015964</td>\n",
       "      <td>-13.892169</td>\n",
       "      <td>-11.862948</td>\n",
       "      <td>41.642323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>-7.331246</td>\n",
       "      <td>2.582383</td>\n",
       "      <td>-1.294306</td>\n",
       "      <td>1.614919</td>\n",
       "      <td>1.509228</td>\n",
       "      <td>6.794973</td>\n",
       "      <td>-13.181029</td>\n",
       "      <td>0.863070</td>\n",
       "      <td>2.193788</td>\n",
       "      <td>-93.784390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274008</td>\n",
       "      <td>-5.097932</td>\n",
       "      <td>-6.165560</td>\n",
       "      <td>-0.189601</td>\n",
       "      <td>2.751795</td>\n",
       "      <td>1.482555</td>\n",
       "      <td>-2.175515</td>\n",
       "      <td>-9.796681</td>\n",
       "      <td>-1.473156</td>\n",
       "      <td>6.462346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3178</th>\n",
       "      <td>-31.721272</td>\n",
       "      <td>2.493158</td>\n",
       "      <td>1.657660</td>\n",
       "      <td>-24.926093</td>\n",
       "      <td>18.835256</td>\n",
       "      <td>28.757961</td>\n",
       "      <td>-47.106519</td>\n",
       "      <td>4.165757</td>\n",
       "      <td>-1.784678</td>\n",
       "      <td>-369.149379</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.235371</td>\n",
       "      <td>4.203783</td>\n",
       "      <td>11.441400</td>\n",
       "      <td>19.157073</td>\n",
       "      <td>14.072711</td>\n",
       "      <td>-20.563410</td>\n",
       "      <td>3.287357</td>\n",
       "      <td>-51.070610</td>\n",
       "      <td>-8.409236</td>\n",
       "      <td>45.097710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18290</th>\n",
       "      <td>-5.115132</td>\n",
       "      <td>3.080932</td>\n",
       "      <td>2.942164</td>\n",
       "      <td>-10.024862</td>\n",
       "      <td>3.715282</td>\n",
       "      <td>11.249158</td>\n",
       "      <td>-23.958547</td>\n",
       "      <td>19.821458</td>\n",
       "      <td>-9.778334</td>\n",
       "      <td>-143.940945</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.417139</td>\n",
       "      <td>-1.355518</td>\n",
       "      <td>-6.147459</td>\n",
       "      <td>-5.334230</td>\n",
       "      <td>2.155662</td>\n",
       "      <td>-5.405018</td>\n",
       "      <td>0.118511</td>\n",
       "      <td>-21.846290</td>\n",
       "      <td>3.424279</td>\n",
       "      <td>9.629625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23671</th>\n",
       "      <td>1.828256</td>\n",
       "      <td>6.113501</td>\n",
       "      <td>0.804816</td>\n",
       "      <td>-2.037244</td>\n",
       "      <td>0.229026</td>\n",
       "      <td>-0.606844</td>\n",
       "      <td>-8.315474</td>\n",
       "      <td>6.690385</td>\n",
       "      <td>-0.782588</td>\n",
       "      <td>-49.745776</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.960647</td>\n",
       "      <td>-5.610909</td>\n",
       "      <td>-3.836043</td>\n",
       "      <td>-1.456762</td>\n",
       "      <td>-1.045058</td>\n",
       "      <td>2.223278</td>\n",
       "      <td>-2.118481</td>\n",
       "      <td>0.794092</td>\n",
       "      <td>-6.652835</td>\n",
       "      <td>2.338544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18351</th>\n",
       "      <td>-5.129800</td>\n",
       "      <td>-4.810049</td>\n",
       "      <td>5.550137</td>\n",
       "      <td>-6.397499</td>\n",
       "      <td>-5.981508</td>\n",
       "      <td>17.487907</td>\n",
       "      <td>-17.566117</td>\n",
       "      <td>30.192076</td>\n",
       "      <td>-7.875016</td>\n",
       "      <td>-83.183763</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.866989</td>\n",
       "      <td>5.072011</td>\n",
       "      <td>-4.120923</td>\n",
       "      <td>-6.048627</td>\n",
       "      <td>8.511630</td>\n",
       "      <td>0.822876</td>\n",
       "      <td>9.328799</td>\n",
       "      <td>-17.095631</td>\n",
       "      <td>8.346865</td>\n",
       "      <td>1.106841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20279</th>\n",
       "      <td>-1.395811</td>\n",
       "      <td>-1.875431</td>\n",
       "      <td>3.071067</td>\n",
       "      <td>-4.703925</td>\n",
       "      <td>1.530614</td>\n",
       "      <td>7.599783</td>\n",
       "      <td>-7.331894</td>\n",
       "      <td>17.412843</td>\n",
       "      <td>-0.931424</td>\n",
       "      <td>-88.109725</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.706028</td>\n",
       "      <td>-0.895109</td>\n",
       "      <td>-4.725322</td>\n",
       "      <td>-1.329977</td>\n",
       "      <td>7.461731</td>\n",
       "      <td>-6.091389</td>\n",
       "      <td>1.631484</td>\n",
       "      <td>-19.605967</td>\n",
       "      <td>-2.866596</td>\n",
       "      <td>10.358732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12220</th>\n",
       "      <td>-11.821404</td>\n",
       "      <td>-1.330741</td>\n",
       "      <td>7.948573</td>\n",
       "      <td>-19.581229</td>\n",
       "      <td>15.206840</td>\n",
       "      <td>37.106793</td>\n",
       "      <td>-25.279644</td>\n",
       "      <td>56.277808</td>\n",
       "      <td>-5.253833</td>\n",
       "      <td>-282.360372</td>\n",
       "      <td>...</td>\n",
       "      <td>-20.585236</td>\n",
       "      <td>-11.230902</td>\n",
       "      <td>-17.788731</td>\n",
       "      <td>-2.498181</td>\n",
       "      <td>6.265777</td>\n",
       "      <td>-10.279454</td>\n",
       "      <td>3.091951</td>\n",
       "      <td>-31.839357</td>\n",
       "      <td>-6.320727</td>\n",
       "      <td>18.275239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18201</th>\n",
       "      <td>-7.279271</td>\n",
       "      <td>-1.513148</td>\n",
       "      <td>0.303210</td>\n",
       "      <td>-8.544380</td>\n",
       "      <td>2.780081</td>\n",
       "      <td>19.919154</td>\n",
       "      <td>-17.635160</td>\n",
       "      <td>21.094376</td>\n",
       "      <td>2.945053</td>\n",
       "      <td>-217.173449</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.531290</td>\n",
       "      <td>-12.617691</td>\n",
       "      <td>-3.132872</td>\n",
       "      <td>-5.285153</td>\n",
       "      <td>8.263999</td>\n",
       "      <td>-27.444443</td>\n",
       "      <td>5.592849</td>\n",
       "      <td>-22.815487</td>\n",
       "      <td>-4.444835</td>\n",
       "      <td>30.603269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18065</th>\n",
       "      <td>-12.342314</td>\n",
       "      <td>-9.987113</td>\n",
       "      <td>9.357621</td>\n",
       "      <td>-15.737674</td>\n",
       "      <td>8.287498</td>\n",
       "      <td>6.715864</td>\n",
       "      <td>-37.732193</td>\n",
       "      <td>15.442818</td>\n",
       "      <td>10.818756</td>\n",
       "      <td>-283.076171</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.813408</td>\n",
       "      <td>-17.497632</td>\n",
       "      <td>-15.091844</td>\n",
       "      <td>5.633425</td>\n",
       "      <td>13.233349</td>\n",
       "      <td>13.928752</td>\n",
       "      <td>5.323454</td>\n",
       "      <td>-13.184178</td>\n",
       "      <td>-16.859804</td>\n",
       "      <td>29.618361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10481</th>\n",
       "      <td>-18.176127</td>\n",
       "      <td>4.505814</td>\n",
       "      <td>7.759429</td>\n",
       "      <td>-18.703472</td>\n",
       "      <td>9.871421</td>\n",
       "      <td>18.407522</td>\n",
       "      <td>-22.337374</td>\n",
       "      <td>39.453819</td>\n",
       "      <td>-4.948215</td>\n",
       "      <td>-336.097801</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.326902</td>\n",
       "      <td>-5.882550</td>\n",
       "      <td>-10.024254</td>\n",
       "      <td>-1.944443</td>\n",
       "      <td>2.578634</td>\n",
       "      <td>-19.243147</td>\n",
       "      <td>-9.914469</td>\n",
       "      <td>-39.466771</td>\n",
       "      <td>-0.892930</td>\n",
       "      <td>13.044264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17614</th>\n",
       "      <td>-6.344917</td>\n",
       "      <td>2.663526</td>\n",
       "      <td>0.133428</td>\n",
       "      <td>-1.087571</td>\n",
       "      <td>1.088142</td>\n",
       "      <td>5.407431</td>\n",
       "      <td>-7.047140</td>\n",
       "      <td>-0.810899</td>\n",
       "      <td>5.796720</td>\n",
       "      <td>-69.809979</td>\n",
       "      <td>...</td>\n",
       "      <td>2.372293</td>\n",
       "      <td>-0.523255</td>\n",
       "      <td>0.030383</td>\n",
       "      <td>4.173936</td>\n",
       "      <td>1.963436</td>\n",
       "      <td>-3.349098</td>\n",
       "      <td>-6.793965</td>\n",
       "      <td>-3.021869</td>\n",
       "      <td>-1.892759</td>\n",
       "      <td>9.181081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11689</th>\n",
       "      <td>-0.666492</td>\n",
       "      <td>9.380249</td>\n",
       "      <td>10.259265</td>\n",
       "      <td>-16.682173</td>\n",
       "      <td>-0.688762</td>\n",
       "      <td>23.445280</td>\n",
       "      <td>-21.574286</td>\n",
       "      <td>25.920608</td>\n",
       "      <td>-3.317183</td>\n",
       "      <td>-214.294439</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.027970</td>\n",
       "      <td>-8.163681</td>\n",
       "      <td>-7.784205</td>\n",
       "      <td>6.155689</td>\n",
       "      <td>12.633283</td>\n",
       "      <td>4.712896</td>\n",
       "      <td>7.181534</td>\n",
       "      <td>-30.792453</td>\n",
       "      <td>-11.115623</td>\n",
       "      <td>14.599836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11574</th>\n",
       "      <td>-9.513247</td>\n",
       "      <td>0.096570</td>\n",
       "      <td>-1.016378</td>\n",
       "      <td>-13.176734</td>\n",
       "      <td>0.861501</td>\n",
       "      <td>3.631342</td>\n",
       "      <td>-5.413134</td>\n",
       "      <td>5.490658</td>\n",
       "      <td>5.739015</td>\n",
       "      <td>-106.426476</td>\n",
       "      <td>...</td>\n",
       "      <td>1.517371</td>\n",
       "      <td>-9.243353</td>\n",
       "      <td>-0.846005</td>\n",
       "      <td>14.185759</td>\n",
       "      <td>-2.533210</td>\n",
       "      <td>-4.865922</td>\n",
       "      <td>-3.647384</td>\n",
       "      <td>-6.375063</td>\n",
       "      <td>-5.304667</td>\n",
       "      <td>14.495459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24739</th>\n",
       "      <td>-8.386566</td>\n",
       "      <td>0.735110</td>\n",
       "      <td>2.415468</td>\n",
       "      <td>1.463672</td>\n",
       "      <td>5.507385</td>\n",
       "      <td>5.520896</td>\n",
       "      <td>-4.710844</td>\n",
       "      <td>1.252946</td>\n",
       "      <td>4.132724</td>\n",
       "      <td>-90.607181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382904</td>\n",
       "      <td>-1.428945</td>\n",
       "      <td>-0.187184</td>\n",
       "      <td>6.035601</td>\n",
       "      <td>6.606554</td>\n",
       "      <td>2.857801</td>\n",
       "      <td>-3.572452</td>\n",
       "      <td>-8.561798</td>\n",
       "      <td>2.100764</td>\n",
       "      <td>-0.039102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20919</th>\n",
       "      <td>-9.322118</td>\n",
       "      <td>-3.197781</td>\n",
       "      <td>4.942729</td>\n",
       "      <td>-3.339224</td>\n",
       "      <td>5.908590</td>\n",
       "      <td>14.735142</td>\n",
       "      <td>-11.302921</td>\n",
       "      <td>13.459735</td>\n",
       "      <td>3.056048</td>\n",
       "      <td>-176.612398</td>\n",
       "      <td>...</td>\n",
       "      <td>2.650568</td>\n",
       "      <td>-11.255787</td>\n",
       "      <td>-8.430085</td>\n",
       "      <td>13.520830</td>\n",
       "      <td>3.433002</td>\n",
       "      <td>12.394903</td>\n",
       "      <td>-7.694484</td>\n",
       "      <td>-14.119385</td>\n",
       "      <td>-6.469682</td>\n",
       "      <td>14.257515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5393</th>\n",
       "      <td>-0.463657</td>\n",
       "      <td>3.626920</td>\n",
       "      <td>1.404716</td>\n",
       "      <td>-4.813219</td>\n",
       "      <td>9.839500</td>\n",
       "      <td>6.833734</td>\n",
       "      <td>-9.003224</td>\n",
       "      <td>1.399248</td>\n",
       "      <td>1.484418</td>\n",
       "      <td>-102.289520</td>\n",
       "      <td>...</td>\n",
       "      <td>2.767877</td>\n",
       "      <td>-9.765071</td>\n",
       "      <td>-4.011418</td>\n",
       "      <td>-0.681792</td>\n",
       "      <td>-1.224861</td>\n",
       "      <td>-10.958973</td>\n",
       "      <td>6.233689</td>\n",
       "      <td>-7.659970</td>\n",
       "      <td>-1.015630</td>\n",
       "      <td>8.358848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14062 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0          1          2          3          4          5  \\\n",
       "23466  -9.530130   5.256128   6.528821  -5.576692   1.611942   0.557120   \n",
       "4830   -9.071622   2.856857  -5.939990  -5.659706   0.378291  13.940015   \n",
       "24948  -6.190051  -3.199577   5.211514  -5.897536  -0.182303   9.425621   \n",
       "23736  -1.062833  -2.315163  -3.424130  -2.142048  -0.125433   8.022907   \n",
       "2379  -12.053769  -9.009227   6.841530 -12.042793   2.928985  11.874726   \n",
       "23526  -0.707302   7.095887  10.054959  -9.228204  -0.063536  23.897032   \n",
       "15168  -3.222536   3.121683   4.156988  -6.959905   7.142618  11.815751   \n",
       "5539  -36.023182  35.783280  23.638059 -33.476221  23.090962  41.879738   \n",
       "7575   -1.955456  -2.541169   3.302431 -12.418295 -18.010573  13.837541   \n",
       "14242  -8.261153  -4.963909  -0.235948  -3.660342  -3.359656   6.031268   \n",
       "3012   -5.554060   1.896317   3.828266  -5.280357  -1.226471   5.379012   \n",
       "4106   -7.731691  -2.459560  -2.054262  -4.834395  -2.748446  12.504790   \n",
       "22407   0.881478  -1.758025   1.097718  -0.959351  -3.677681   8.860155   \n",
       "19119  -8.418785   6.769827  -0.807128  -8.165559 -12.831338  18.711402   \n",
       "8645   -5.434493  -1.812939  -3.633009  -3.390656   2.282461   8.917680   \n",
       "13306  -5.847624  -4.752955  -1.508078  -9.046967  -0.240309   7.068756   \n",
       "8373   -2.346229   3.308935  -3.367014  -1.188032  -0.769465   1.391796   \n",
       "22030 -32.897000  18.896411  13.306919  -6.041707 -19.415635  73.952208   \n",
       "10886  -7.601511   2.204222  -1.132577  -1.324786  -0.729523  13.538328   \n",
       "21010  -6.012786  -8.357166   1.421948  -9.930718   2.707386   0.039566   \n",
       "12980   0.396388   0.132779   0.974491  -0.113261   3.862475   1.880374   \n",
       "21975   1.957614  -0.169524   3.290817  -2.941709   0.312306  14.353746   \n",
       "9104  -13.422149  12.692577  17.382467 -34.299658  15.938077  42.028622   \n",
       "24338   2.192088   2.619486   2.303559  -0.353052  -1.382333   3.706707   \n",
       "19647 -12.090031   5.041478  -4.180249  -1.550399   3.297170   8.866880   \n",
       "20702  -7.307460   4.628349  -0.173648  -5.662859   7.411239  -5.214142   \n",
       "19232 -20.750644 -20.508117  -9.838239 -16.844271   2.455607  28.743316   \n",
       "6885  -13.504982   8.727747   6.811440 -20.369815   8.270415   9.842857   \n",
       "16015   0.309664  -3.233749   3.705785  -0.239360  -3.159931   8.829213   \n",
       "14477 -28.453490   5.829622  15.407428 -32.003428  20.243388  29.041429   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "3691    2.744640  -6.524552   6.699210  -8.405260  -3.912137  11.253881   \n",
       "20273  -3.933806  -3.366899  -0.410038  -6.591527  -0.032318  12.344045   \n",
       "24169  -7.112644  -0.353493  -0.772476 -13.336055  -1.578966   7.536137   \n",
       "1083  -11.651399  -4.325602   1.705322  -9.364930  10.837328   4.326129   \n",
       "2956   -0.355247  -2.041020   1.802194  -3.053436  -3.789694   8.674442   \n",
       "12449 -31.378552   7.912938  14.452253 -23.131662  -1.084160  18.562102   \n",
       "9761   -9.794826  -8.820186   1.018776  -3.450039  -5.107981   7.239627   \n",
       "8776  -24.685092  -7.759567   5.109131  -4.766638  -4.868717  40.401091   \n",
       "15846  -2.730303  -3.232159   8.799583  -3.278385  -1.042008  10.174993   \n",
       "3370  -12.390107   4.482237   0.362834  -8.526055   4.270219   8.209192   \n",
       "23071 -16.441008  -0.971972   5.148430 -10.111019   2.313799  11.186152   \n",
       "5320   -5.478957   3.779384   7.270483 -20.059094  -2.463570  17.980507   \n",
       "6678   -1.487272   2.143526   0.722507  -2.769477  -1.502194   2.555049   \n",
       "15344 -18.614854   0.956340   2.551893 -14.389428  20.680227  27.111285   \n",
       "1244   -7.331246   2.582383  -1.294306   1.614919   1.509228   6.794973   \n",
       "3178  -31.721272   2.493158   1.657660 -24.926093  18.835256  28.757961   \n",
       "18290  -5.115132   3.080932   2.942164 -10.024862   3.715282  11.249158   \n",
       "23671   1.828256   6.113501   0.804816  -2.037244   0.229026  -0.606844   \n",
       "18351  -5.129800  -4.810049   5.550137  -6.397499  -5.981508  17.487907   \n",
       "20279  -1.395811  -1.875431   3.071067  -4.703925   1.530614   7.599783   \n",
       "12220 -11.821404  -1.330741   7.948573 -19.581229  15.206840  37.106793   \n",
       "18201  -7.279271  -1.513148   0.303210  -8.544380   2.780081  19.919154   \n",
       "18065 -12.342314  -9.987113   9.357621 -15.737674   8.287498   6.715864   \n",
       "10481 -18.176127   4.505814   7.759429 -18.703472   9.871421  18.407522   \n",
       "17614  -6.344917   2.663526   0.133428  -1.087571   1.088142   5.407431   \n",
       "11689  -0.666492   9.380249  10.259265 -16.682173  -0.688762  23.445280   \n",
       "11574  -9.513247   0.096570  -1.016378 -13.176734   0.861501   3.631342   \n",
       "24739  -8.386566   0.735110   2.415468   1.463672   5.507385   5.520896   \n",
       "20919  -9.322118  -3.197781   4.942729  -3.339224   5.908590  14.735142   \n",
       "5393   -0.463657   3.626920   1.404716  -4.813219   9.839500   6.833734   \n",
       "\n",
       "               6          7          8           9    ...            290  \\\n",
       "23466 -17.161828  12.029130  -0.591008 -158.001536    ...      -1.915202   \n",
       "4830   -5.771536   3.221016   3.105647 -124.297551    ...       6.918535   \n",
       "24948 -12.014888  18.564904  -1.132379 -108.502450    ...      -5.656457   \n",
       "23736 -11.455393  16.721467   5.193775 -109.108859    ...      -4.850501   \n",
       "2379  -24.734865  21.170251  -3.272141 -161.999388    ...      -9.767677   \n",
       "23526 -58.378764  53.419397   1.213361 -376.578505    ...     -15.783971   \n",
       "15168 -13.505236  17.294490  -0.605547  -94.353434    ...      -3.507356   \n",
       "5539  -45.798463  55.679137   5.525473 -632.572143    ...     -40.299987   \n",
       "7575  -16.857491  20.017110   1.982764 -128.073439    ...     -10.488233   \n",
       "14242   0.092088  -0.482050   9.402231  -99.443276    ...       0.755983   \n",
       "3012  -11.946574   9.848292   4.410375 -142.011628    ...      -6.965625   \n",
       "4106  -13.431412   7.913226  -0.904062  -74.198070    ...      -0.241437   \n",
       "22407  -9.180261  15.639782   0.464764  -98.869372    ...      -4.493997   \n",
       "19119 -31.763460  28.586823  -4.600947 -222.282175    ...      -5.529511   \n",
       "8645   -6.118245   3.854455   6.916669  -98.319867    ...       4.205456   \n",
       "13306  -9.246573   8.626601  12.275572 -122.976076    ...      -4.272587   \n",
       "8373   -7.609906  -0.146840  -2.651701  -45.773696    ...     -10.766511   \n",
       "22030 -54.323985  89.823002 -11.531563 -509.971428    ...     -52.849037   \n",
       "10886 -12.208080   5.959574   9.980828 -102.652060    ...      -3.353372   \n",
       "21010 -10.217367   0.765258   2.118605  -71.417318    ...      -3.617392   \n",
       "12980  -6.621835   3.335962  11.374757 -100.504739    ...       2.326234   \n",
       "21975 -19.177499  28.904262  -0.884821 -161.991833    ...     -14.555810   \n",
       "9104  -49.840546  65.272382 -28.027808 -366.934458    ...     -17.182918   \n",
       "24338  -6.628287   5.384666   2.506913  -56.495236    ...       0.757246   \n",
       "19647  -6.379341  18.084499   5.478194 -128.406414    ...       2.910437   \n",
       "20702  -4.160598  13.425172  16.824955 -155.809686    ...       2.850884   \n",
       "19232 -36.740112  18.508811 -10.020760 -344.775355    ...     -17.040572   \n",
       "6885  -39.389114  25.467694  -0.150059 -273.738859    ...     -14.746224   \n",
       "16015  -4.471591  13.152434  -4.098528  -45.815503    ...      -5.503885   \n",
       "14477 -89.036082  51.300486  -6.721999 -682.414391    ...     -20.049204   \n",
       "...          ...        ...        ...         ...    ...            ...   \n",
       "3691  -11.474327  25.848899 -11.106596  -89.521008    ...     -16.087630   \n",
       "20273 -13.485825  23.172090  -4.348411 -103.051452    ...     -14.249912   \n",
       "24169 -16.494847  19.164431   0.164877 -163.247252    ...      -5.768125   \n",
       "1083  -16.583325   3.460597   1.656298 -207.608604    ...      -3.454272   \n",
       "2956  -12.139471   9.671207   0.415015 -109.308416    ...     -12.924787   \n",
       "12449 -47.528697  64.981143   2.448383 -514.758425    ...       0.798693   \n",
       "9761   -8.242550   7.567896   7.481587 -126.254677    ...     -13.819538   \n",
       "8776  -29.430465  43.994741  -6.599786 -230.334011    ...     -17.438599   \n",
       "15846 -17.070490  12.034405  -4.832986  -96.848990    ...      -5.464216   \n",
       "3370   -6.045477   3.544666   3.380720 -107.889926    ...       4.002229   \n",
       "23071  -6.530899   3.688398   2.494096 -120.342170    ...      -0.446258   \n",
       "5320  -19.417351  33.365423  -8.300340 -158.569343    ...     -15.687331   \n",
       "6678   -4.698054   9.237486   2.368931  -59.898915    ...       1.048051   \n",
       "15344 -54.782512  26.390859  16.386493 -473.174670    ...     -16.761137   \n",
       "1244  -13.181029   0.863070   2.193788  -93.784390    ...       0.274008   \n",
       "3178  -47.106519   4.165757  -1.784678 -369.149379    ...      -7.235371   \n",
       "18290 -23.958547  19.821458  -9.778334 -143.940945    ...      -5.417139   \n",
       "23671  -8.315474   6.690385  -0.782588  -49.745776    ...      -1.960647   \n",
       "18351 -17.566117  30.192076  -7.875016  -83.183763    ...      -9.866989   \n",
       "20279  -7.331894  17.412843  -0.931424  -88.109725    ...      -5.706028   \n",
       "12220 -25.279644  56.277808  -5.253833 -282.360372    ...     -20.585236   \n",
       "18201 -17.635160  21.094376   2.945053 -217.173449    ...      -7.531290   \n",
       "18065 -37.732193  15.442818  10.818756 -283.076171    ...      -5.813408   \n",
       "10481 -22.337374  39.453819  -4.948215 -336.097801    ...     -11.326902   \n",
       "17614  -7.047140  -0.810899   5.796720  -69.809979    ...       2.372293   \n",
       "11689 -21.574286  25.920608  -3.317183 -214.294439    ...     -12.027970   \n",
       "11574  -5.413134   5.490658   5.739015 -106.426476    ...       1.517371   \n",
       "24739  -4.710844   1.252946   4.132724  -90.607181    ...       0.382904   \n",
       "20919 -11.302921  13.459735   3.056048 -176.612398    ...       2.650568   \n",
       "5393   -9.003224   1.399248   1.484418 -102.289520    ...       2.767877   \n",
       "\n",
       "             291        292        293        294        295        296  \\\n",
       "23466 -14.362643 -10.617706   3.466634  -4.418027  18.719916  -2.894185   \n",
       "4830  -12.327835   2.293610   1.302110   5.220683 -19.774624   2.891981   \n",
       "24948  -2.219772  -5.192971  -1.484033  11.546587 -13.245098   9.493588   \n",
       "23736  -5.386972  -3.186071  -0.280276   0.734114  -8.987849   7.224415   \n",
       "2379   -7.178676   8.009605  -7.917645   2.532413  -3.049287   8.429122   \n",
       "23526   2.203979 -22.033561 -14.086114  11.929373 -13.709570  26.395440   \n",
       "15168  -4.052887  -6.261077  -1.646614  -1.030465  -3.727235   3.517439   \n",
       "5539  -29.439347 -50.505792  13.707982  18.225685   6.371740 -11.089697   \n",
       "7575    0.155041  -8.392291  -9.963056  12.395026  -5.209892   0.391239   \n",
       "14242  -4.640630  -1.060889   7.147705   0.146469  -7.309369   3.796514   \n",
       "3012  -15.916383  -2.618227   2.235742   8.116533 -10.085681   3.528779   \n",
       "4106   -3.160958   0.882059  -2.047996   3.106531  -8.482807   3.680201   \n",
       "22407   2.315805  -1.809286  -2.076633   8.181276 -11.883284   3.298349   \n",
       "19119 -12.561794 -13.462274   0.313848   8.381645 -20.502949  18.158500   \n",
       "8645   -2.703624 -13.373855  13.167686  -0.799346  -7.678902   3.269016   \n",
       "13306  -8.963465  -6.821433   6.279054   5.900607   0.540821  -2.708883   \n",
       "8373   -1.009266  -4.396433   3.415302  -3.536441   2.912895  -4.924084   \n",
       "22030 -30.760297 -28.494524 -22.233624  33.285767 -38.965658  21.433073   \n",
       "10886  -8.672554  -6.181036   1.191237   0.318106  -7.293444  -2.095083   \n",
       "21010   2.211529  -6.714586   2.965684  -7.075214   9.231201  -2.176942   \n",
       "12980  -7.663560   1.846511   8.339030   7.506838   0.315843   4.434928   \n",
       "21975  -3.948519  -6.033382  -2.806161   3.020891 -12.469153   7.073688   \n",
       "9104  -24.589132 -21.684868   6.486776  11.376559  29.340348  18.350683   \n",
       "24338  -3.366980  -4.395236  -0.111896   2.967986  -2.648702  -0.494547   \n",
       "19647 -11.005637  -6.893380   4.272665  -0.255134  -3.068025   4.884244   \n",
       "20702  -7.024557  -6.415440   4.199648   8.099173   3.230869  -4.172433   \n",
       "19232  -9.460420   5.789862  -0.618516  12.788148 -23.100120  10.157858   \n",
       "6885   -3.012283 -12.152158  -3.566240   9.784496   4.542156   6.175564   \n",
       "16015   1.400501  -5.122602  -1.656916   3.998982 -11.292749   3.371133   \n",
       "14477 -39.344662  -5.305472  21.819538  30.174680  -5.725104  30.880483   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "3691    2.250200  -4.914206  -5.113750   9.255205  -9.328271   7.577898   \n",
       "20273   2.644893  -5.863980 -10.325793   6.971148 -14.710165   5.284609   \n",
       "24169  -7.860130  -7.418351   8.533393   8.632243   1.370027   1.324204   \n",
       "1083   -4.694391  -2.185712   5.117630  13.293782   6.781688  -4.249224   \n",
       "2956   -1.195256  -8.928538  -2.260764  -9.292087  -3.871352   4.938271   \n",
       "12449 -18.960250 -34.247080  24.671551  15.406835  -0.063786   3.607893   \n",
       "9761   -8.080878  -0.005063  10.137972   9.684218 -11.427275  -2.986698   \n",
       "8776    1.801730 -10.042653   0.723458  -4.317535  -7.923604  -2.473023   \n",
       "15846  -0.973825  -7.130933  -5.594340  -5.955897 -10.028537   0.988557   \n",
       "3370   -6.756899  -6.498502   2.813978  -2.386410  -0.617349  -2.734974   \n",
       "23071  -7.888408  -4.652338   5.086521   6.424604  14.565184  -0.445434   \n",
       "5320   -4.402992 -19.383372   2.097127  18.016863  -0.775409   7.353597   \n",
       "6678    1.976198  -4.706003  -0.402661   2.811744   4.698605  -2.704089   \n",
       "15344  -5.375424 -19.493218   8.167412   5.877929 -39.082685   0.015964   \n",
       "1244   -5.097932  -6.165560  -0.189601   2.751795   1.482555  -2.175515   \n",
       "3178    4.203783  11.441400  19.157073  14.072711 -20.563410   3.287357   \n",
       "18290  -1.355518  -6.147459  -5.334230   2.155662  -5.405018   0.118511   \n",
       "23671  -5.610909  -3.836043  -1.456762  -1.045058   2.223278  -2.118481   \n",
       "18351   5.072011  -4.120923  -6.048627   8.511630   0.822876   9.328799   \n",
       "20279  -0.895109  -4.725322  -1.329977   7.461731  -6.091389   1.631484   \n",
       "12220 -11.230902 -17.788731  -2.498181   6.265777 -10.279454   3.091951   \n",
       "18201 -12.617691  -3.132872  -5.285153   8.263999 -27.444443   5.592849   \n",
       "18065 -17.497632 -15.091844   5.633425  13.233349  13.928752   5.323454   \n",
       "10481  -5.882550 -10.024254  -1.944443   2.578634 -19.243147  -9.914469   \n",
       "17614  -0.523255   0.030383   4.173936   1.963436  -3.349098  -6.793965   \n",
       "11689  -8.163681  -7.784205   6.155689  12.633283   4.712896   7.181534   \n",
       "11574  -9.243353  -0.846005  14.185759  -2.533210  -4.865922  -3.647384   \n",
       "24739  -1.428945  -0.187184   6.035601   6.606554   2.857801  -3.572452   \n",
       "20919 -11.255787  -8.430085  13.520830   3.433002  12.394903  -7.694484   \n",
       "5393   -9.765071  -4.011418  -0.681792  -1.224861 -10.958973   6.233689   \n",
       "\n",
       "             297        298        299  \n",
       "23466  -2.608892  -7.436052   8.361118  \n",
       "4830  -15.062416  -2.457781  15.464681  \n",
       "24948 -17.866740  -3.454605  17.845281  \n",
       "23736  -9.832436  -1.555770  12.914127  \n",
       "2379  -22.099984   1.411121  31.630693  \n",
       "23526 -25.057489 -13.350622  33.441044  \n",
       "15168 -12.853355   2.797053   7.115327  \n",
       "5539  -36.399549 -11.360035  26.103479  \n",
       "7575  -14.379635  -1.777981  28.843793  \n",
       "14242  -8.246084   1.834856  19.446099  \n",
       "3012  -20.083711  -1.709733   3.075917  \n",
       "4106  -11.099233   0.431697  14.013174  \n",
       "22407 -23.153842  -0.285598  13.349677  \n",
       "19119 -16.914925 -10.841430  35.497994  \n",
       "8645   -8.293049   3.289054  16.729202  \n",
       "13306 -17.232246  -1.559909  13.246264  \n",
       "8373   -5.794489  -0.542816   6.253063  \n",
       "22030 -89.992800   2.317492  84.355632  \n",
       "10886  -9.531925  -5.926222   6.964731  \n",
       "21010  -2.062894  -6.245191  12.928256  \n",
       "12980 -13.483728  -8.313598   9.126546  \n",
       "21975 -27.512704   1.689985  23.879937  \n",
       "9104  -77.868315 -38.073706  60.898119  \n",
       "24338  -4.051354  -0.731598   4.311168  \n",
       "19647 -15.787470  -0.519110  23.777695  \n",
       "20702 -16.184178  -3.420766   4.595183  \n",
       "19232 -34.759542 -12.670676  61.162983  \n",
       "6885  -19.289295 -20.192969  22.686837  \n",
       "16015 -12.216825   2.857740   7.545529  \n",
       "14477 -81.438885 -52.801822  99.929113  \n",
       "...          ...        ...        ...  \n",
       "3691  -23.522743  -0.869034  10.783981  \n",
       "20273 -26.296990   9.578152  15.921844  \n",
       "24169 -19.087657  -4.347405   9.538978  \n",
       "1083  -17.030843 -11.562210  17.677011  \n",
       "2956   -0.003243  -5.057008   7.558971  \n",
       "12449 -45.440681 -28.085283  44.416185  \n",
       "9761   -2.049864  -5.626375  16.481657  \n",
       "8776  -22.690875  -2.631586  27.632942  \n",
       "15846 -12.527120  -1.873470  15.226022  \n",
       "3370   -6.136994  -1.637426   2.392558  \n",
       "23071 -15.801587  -9.528174  16.307462  \n",
       "5320  -22.191708   0.369790  11.033066  \n",
       "6678   -9.996537   1.702047  -4.528905  \n",
       "15344 -13.892169 -11.862948  41.642323  \n",
       "1244   -9.796681  -1.473156   6.462346  \n",
       "3178  -51.070610  -8.409236  45.097710  \n",
       "18290 -21.846290   3.424279   9.629625  \n",
       "23671   0.794092  -6.652835   2.338544  \n",
       "18351 -17.095631   8.346865   1.106841  \n",
       "20279 -19.605967  -2.866596  10.358732  \n",
       "12220 -31.839357  -6.320727  18.275239  \n",
       "18201 -22.815487  -4.444835  30.603269  \n",
       "18065 -13.184178 -16.859804  29.618361  \n",
       "10481 -39.466771  -0.892930  13.044264  \n",
       "17614  -3.021869  -1.892759   9.181081  \n",
       "11689 -30.792453 -11.115623  14.599836  \n",
       "11574  -6.375063  -5.304667  14.495459  \n",
       "24739  -8.561798   2.100764  -0.039102  \n",
       "20919 -14.119385  -6.469682  14.257515  \n",
       "5393   -7.659970  -1.015630   8.358848  \n",
       "\n",
       "[14062 rows x 300 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jared\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LeakyReLU, Dropout\n",
    "from keras import optimizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=0.01, nesterov=True)\n",
    "rms = optimizers.RMSprop()\n",
    "adam = optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(300, input_dim=300))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1000))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(500))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(250))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(100))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(50))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "After trying different models between sgd, rms and adam, adam and rms are the fastest to get to 80% so far.\n",
    "\n",
    "They max out around val_acc = 83%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14062 samples, validate on 4688 samples\n",
      "Epoch 1/10000\n",
      "14062/14062 [==============================] - 13s 936us/step - loss: 0.5913 - acc: 0.7096 - val_loss: 0.4994 - val_acc: 0.7993\n",
      "Epoch 2/10000\n",
      "14062/14062 [==============================] - 8s 548us/step - loss: 0.4544 - acc: 0.7971 - val_loss: 0.5060 - val_acc: 0.8006\n",
      "Epoch 3/10000\n",
      "14062/14062 [==============================] - 8s 573us/step - loss: 0.4356 - acc: 0.8102 - val_loss: 0.4191 - val_acc: 0.8172\n",
      "Epoch 4/10000\n",
      "14062/14062 [==============================] - 8s 562us/step - loss: 0.4189 - acc: 0.8165 - val_loss: 0.4401 - val_acc: 0.8142\n",
      "Epoch 5/10000\n",
      "14062/14062 [==============================] - 9s 627us/step - loss: 0.4165 - acc: 0.8181 - val_loss: 0.3855 - val_acc: 0.8351\n",
      "Epoch 6/10000\n",
      "14062/14062 [==============================] - 9s 645us/step - loss: 0.4093 - acc: 0.8195 - val_loss: 0.3946 - val_acc: 0.8319\n",
      "Epoch 7/10000\n",
      "14062/14062 [==============================] - 8s 600us/step - loss: 0.4041 - acc: 0.8187 - val_loss: 0.3888 - val_acc: 0.8317\n",
      "Epoch 8/10000\n",
      "14062/14062 [==============================] - 8s 561us/step - loss: 0.4057 - acc: 0.8202 - val_loss: 0.3869 - val_acc: 0.8294\n",
      "Epoch 9/10000\n",
      "14062/14062 [==============================] - 8s 555us/step - loss: 0.4031 - acc: 0.8229 - val_loss: 0.3917 - val_acc: 0.8304\n",
      "Epoch 10/10000\n",
      "14062/14062 [==============================] - 8s 565us/step - loss: 0.4014 - acc: 0.8223 - val_loss: 0.4049 - val_acc: 0.8276\n",
      "Epoch 11/10000\n",
      "14062/14062 [==============================] - 8s 588us/step - loss: 0.3996 - acc: 0.8233 - val_loss: 0.3867 - val_acc: 0.8338\n",
      "Epoch 12/10000\n",
      "14062/14062 [==============================] - 9s 624us/step - loss: 0.3957 - acc: 0.8275 - val_loss: 0.3854 - val_acc: 0.8334\n",
      "Epoch 13/10000\n",
      "14062/14062 [==============================] - 9s 618us/step - loss: 0.3933 - acc: 0.8285 - val_loss: 0.3922 - val_acc: 0.8300\n",
      "Epoch 14/10000\n",
      "14062/14062 [==============================] - 8s 586us/step - loss: 0.3984 - acc: 0.8258 - val_loss: 0.3862 - val_acc: 0.8340\n",
      "Epoch 15/10000\n",
      "14062/14062 [==============================] - 8s 589us/step - loss: 0.3935 - acc: 0.8246 - val_loss: 0.4207 - val_acc: 0.8046\n",
      "Epoch 16/10000\n",
      "14062/14062 [==============================] - 8s 577us/step - loss: 0.3929 - acc: 0.8265 - val_loss: 0.4111 - val_acc: 0.8151\n",
      "Epoch 17/10000\n",
      "14062/14062 [==============================] - 8s 583us/step - loss: 0.3918 - acc: 0.8300 - val_loss: 0.4399 - val_acc: 0.8082\n",
      "Epoch 18/10000\n",
      "14062/14062 [==============================] - 8s 578us/step - loss: 0.3902 - acc: 0.8313 - val_loss: 0.4285 - val_acc: 0.8044\n",
      "Epoch 19/10000\n",
      "14062/14062 [==============================] - 8s 581us/step - loss: 0.3927 - acc: 0.8282 - val_loss: 0.3861 - val_acc: 0.8358\n",
      "Epoch 20/10000\n",
      "14062/14062 [==============================] - 8s 595us/step - loss: 0.3883 - acc: 0.8289 - val_loss: 0.4255 - val_acc: 0.8065\n",
      "Epoch 21/10000\n",
      "14062/14062 [==============================] - 9s 620us/step - loss: 0.3877 - acc: 0.8307 - val_loss: 0.3890 - val_acc: 0.8264\n",
      "Epoch 22/10000\n",
      "14062/14062 [==============================] - 9s 625us/step - loss: 0.3846 - acc: 0.8347 - val_loss: 0.3824 - val_acc: 0.8332\n",
      "Epoch 23/10000\n",
      "14062/14062 [==============================] - 8s 587us/step - loss: 0.3906 - acc: 0.8307 - val_loss: 0.3736 - val_acc: 0.8379\n",
      "Epoch 24/10000\n",
      "14062/14062 [==============================] - 8s 576us/step - loss: 0.3842 - acc: 0.8307 - val_loss: 0.3922 - val_acc: 0.8298\n",
      "Epoch 25/10000\n",
      "14062/14062 [==============================] - 8s 586us/step - loss: 0.3832 - acc: 0.8364 - val_loss: 0.3872 - val_acc: 0.8300\n",
      "Epoch 26/10000\n",
      "14062/14062 [==============================] - 8s 591us/step - loss: 0.3876 - acc: 0.8291 - val_loss: 0.3816 - val_acc: 0.8383\n",
      "Epoch 27/10000\n",
      "14062/14062 [==============================] - 8s 590us/step - loss: 0.3897 - acc: 0.8303 - val_loss: 0.3777 - val_acc: 0.8364\n",
      "Epoch 28/10000\n",
      "14062/14062 [==============================] - 8s 583us/step - loss: 0.3783 - acc: 0.8347 - val_loss: 0.3833 - val_acc: 0.8396\n",
      "Epoch 29/10000\n",
      "14062/14062 [==============================] - 8s 588us/step - loss: 0.3862 - acc: 0.8338 - val_loss: 0.3837 - val_acc: 0.8347\n",
      "Epoch 30/10000\n",
      "14062/14062 [==============================] - 9s 644us/step - loss: 0.3807 - acc: 0.8340 - val_loss: 0.3917 - val_acc: 0.8279\n",
      "Epoch 31/10000\n",
      "14062/14062 [==============================] - 9s 627us/step - loss: 0.3810 - acc: 0.8348 - val_loss: 0.3862 - val_acc: 0.8375\n",
      "Epoch 32/10000\n",
      "14062/14062 [==============================] - 8s 579us/step - loss: 0.3799 - acc: 0.8378 - val_loss: 0.3841 - val_acc: 0.8343\n",
      "Epoch 33/10000\n",
      "14062/14062 [==============================] - 8s 590us/step - loss: 0.3823 - acc: 0.8350 - val_loss: 0.3930 - val_acc: 0.8276\n",
      "Epoch 34/10000\n",
      "14062/14062 [==============================] - 8s 584us/step - loss: 0.3796 - acc: 0.8335 - val_loss: 0.3777 - val_acc: 0.8396\n",
      "Epoch 35/10000\n",
      "14062/14062 [==============================] - 9s 619us/step - loss: 0.3810 - acc: 0.8339 - val_loss: 0.3813 - val_acc: 0.8366\n",
      "Epoch 36/10000\n",
      "14062/14062 [==============================] - 8s 589us/step - loss: 0.3769 - acc: 0.8361 - val_loss: 0.3873 - val_acc: 0.8355\n",
      "Epoch 37/10000\n",
      "14062/14062 [==============================] - 8s 597us/step - loss: 0.3740 - acc: 0.8365 - val_loss: 0.3809 - val_acc: 0.8362\n",
      "Epoch 38/10000\n",
      " 7936/14062 [===============>..............] - ETA: 3s - loss: 0.3744 - acc: 0.8368"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-d732e575c9c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhist2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1002\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1003\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1236\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1237\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2482\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2483\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=rms, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "hist2 = model.fit(X_train, y_train, epochs=10000, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = np.reshape(model.predict(X_val), (-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jared\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:52: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  return getattr(obj, method)(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "actual = np.reshape(y_val, (-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diff = actual - np.round(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "false_neg = reviews_df.iloc[X_val[diff==1].index]\n",
    "false_pos = reviews_df.iloc[X_val[diff==-1].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#looking at random entries in false negative\n",
    "rand_int = np.random.randint(0, len(false_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "very\n",
      "strong\n",
      "movie\n",
      ".\n",
      "bruce\n",
      "is\n",
      "good\n",
      "and\n",
      "brad\n",
      "also\n",
      ".\n",
      "<\n",
      "br\n",
      "/\n",
      ">\n",
      "<\n",
      "br\n",
      "/\n",
      ">\n",
      "as\n",
      "i\n",
      "think\n",
      "there\n",
      "are\n",
      "two\n",
      "cities\n",
      "missed\n",
      "in\n",
      "the\n",
      "receptionist\n",
      "list\n",
      "from\n",
      "the\n",
      "list\n",
      "bruce\n",
      "remembered\n",
      ".\n",
      "<\n",
      "br\n",
      "/\n",
      ">\n",
      "<\n",
      "br\n",
      "/\n",
      ">\n",
      "that\n",
      "means\n",
      "the\n",
      "woman\n",
      "was\n",
      "a\n",
      "real\n",
      "insurance\n",
      "and\n",
      "she\n",
      "did\n",
      "her\n",
      "job\n",
      ".\n",
      "<\n",
      "br\n",
      "/\n",
      ">\n",
      "<\n",
      "br\n",
      "/\n",
      ">\n",
      "well\n",
      ",\n",
      "novikov\n",
      "property\n",
      "seems\n",
      "to\n",
      "me\n",
      "work\n",
      "in\n",
      "this\n",
      "movie\n",
      ".\n",
      "however\n",
      ",\n",
      "i\n",
      "do\n",
      "believe\n",
      "in\n",
      "back\n",
      "to\n",
      "the\n",
      "future\n",
      "theory\n",
      "of\n",
      "worlds\n",
      "'\n",
      "multiplicity\n",
      ".\n",
      "<\n",
      "br\n",
      "/\n",
      ">\n",
      "<\n",
      "br\n",
      "/\n",
      ">\n",
      "so\n",
      "bruce\n",
      "could\n",
      "save\n",
      "the\n",
      "world\n",
      ",\n",
      "but\n",
      "not\n",
      "his\n",
      "world\n",
      ".\n",
      "<\n",
      "br\n",
      "/\n",
      ">\n",
      "<\n",
      "br\n",
      "/\n",
      ">\n",
      "in\n",
      "the\n",
      "theory\n",
      "of\n",
      "parallel\n",
      "worlds\n",
      "the\n",
      "man\n",
      "can\n",
      "meet\n",
      "himself\n",
      ".\n",
      "<\n",
      "br\n",
      "/\n",
      ">\n",
      "<\n",
      "br\n",
      "/\n",
      ">\n",
      "and\n",
      "i\n",
      "do\n",
      "believe\n",
      "there\n",
      "is\n",
      "no\n",
      "problem\n",
      "in\n",
      "that\n",
      ".\n",
      "here\n",
      "i\n",
      "disagree\n",
      "with\n",
      "dr\n",
      ".\n",
      "brown\n",
      "from\n",
      "back\n",
      "...\n",
      "<\n",
      "br\n",
      "/\n",
      ">\n",
      "<\n",
      "br\n",
      "/\n",
      ">\n",
      "but\n",
      "the\n",
      "story\n",
      "pf\n",
      "12\n",
      "monkeys\n",
      "has\n",
      "its\n",
      "own\n",
      "beauty\n",
      ".\n",
      "inspite\n",
      "of\n",
      "all\n",
      "these\n",
      "theories\n",
      "of\n",
      "one\n",
      "world\n",
      "or\n",
      "many\n",
      "or\n",
      "continuum\n",
      "one\n",
      "can\n",
      "believe\n",
      "that\n",
      "he\n",
      "is\n",
      "really\n",
      "insane\n",
      "and\n",
      "the\n",
      "doctor\n",
      "-\n",
      "his\n",
      "girlfriend\n",
      "was\n",
      "just\n",
      "lost\n",
      ".\n",
      "<\n",
      "br\n",
      "/\n",
      ">\n",
      "<\n",
      "br\n",
      "/\n",
      ">\n",
      "a\n",
      "sequence\n",
      "of\n",
      "events\n",
      "which\n",
      "may\n",
      "lead\n",
      "her\n",
      "to\n",
      "believe\n",
      "that\n",
      "he\n",
      "is\n",
      "from\n",
      "the\n",
      "future\n",
      ".\n",
      "the\n",
      "bullet\n",
      "-\n",
      "well\n",
      "it\n",
      "might\n",
      "be\n",
      "some\n",
      "mistake\n",
      ",\n",
      "some\n",
      "falsification\n",
      ".\n",
      "<\n",
      "br\n",
      "/\n",
      ">\n",
      "<\n",
      "br\n",
      "/\n",
      ">\n",
      "well\n",
      "i\n",
      "like\n",
      "this\n",
      "movie\n",
      "-\n",
      "has\n",
      "to\n",
      "buy\n",
      "a\n",
      "dvd\n",
      ".\n",
      "<\n",
      "br\n",
      "/\n",
      ">\n",
      "<\n",
      "br\n",
      "/\n",
      ">\n",
      "best\n",
      ".\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(rand_int, '\\n\\n')\n",
    "[print(i) for i in false_neg.iloc[rand_int,0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that a lot of false negatives are made because of summaries in movies. The summary contains negative words that the network picks up and interprets the review as negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#looking at random entries in false positive\n",
    "rand_int = np.random.randint(0, len(false_pos))\n",
    "# Obtained values 223, 320, 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 \n",
      "\n",
      "\n",
      "stefan\n",
      "is\n",
      "an\n",
      "x-con\n",
      "that\n",
      "five\n",
      "years\n",
      "ago\n",
      "got\n",
      "married\n",
      "to\n",
      "marie\n",
      ".\n",
      "their\n",
      "marriage\n",
      "has\n",
      "been\n",
      "stable\n",
      "until\n",
      "stefan\n",
      "past\n",
      "catch\n",
      "up\n",
      "with\n",
      "them\n",
      "and\n",
      "he's\n",
      "offered\n",
      "to\n",
      "do\n",
      "a\n",
      "courier\n",
      "job\n",
      ".\n",
      "stefan's\n",
      "job\n",
      "is\n",
      "a\n",
      "heroin\n",
      "delivery\n",
      "from\n",
      "germany\n",
      "to\n",
      "sweden\n",
      "which\n",
      "should\n",
      "go\n",
      "easily\n",
      ".\n",
      "<\n",
      "br\n",
      "/\n",
      ">\n",
      "<\n",
      "br\n",
      "/\n",
      ">\n",
      "in\n",
      "germany\n",
      "stefan\n",
      "meet\n",
      "elli\n",
      ",\n",
      "a\n",
      "girl\n",
      "from\n",
      "bosnia\n",
      "that\n",
      "has\n",
      "been\n",
      "sold\n",
      "to\n",
      "a\n",
      "stripclub\n",
      "owner\n",
      ".\n",
      "stefan\n",
      "dislikes\n",
      "what\n",
      "he\n",
      "sees\n",
      "and\n",
      "decide\n",
      "to\n",
      "help\n",
      "elli\n",
      "out\n",
      "of\n",
      "her\n",
      "misery\n",
      ".\n",
      "due\n",
      "to\n",
      "the\n",
      "fact\n",
      "that\n",
      "elli's\n",
      "father\n",
      "during\n",
      "the\n",
      "war\n",
      "fleed\n",
      "to\n",
      "sweden\n",
      "elli\n",
      "now\n",
      "goes\n",
      "with\n",
      "stefan\n",
      "to\n",
      "sweden\n",
      ".\n",
      "to\n",
      "make\n",
      "up\n",
      "with\n",
      "the\n",
      "past\n",
      "stefan\n",
      "promises\n",
      "elli\n",
      "to\n",
      "help\n",
      "her\n",
      "find\n",
      "her\n",
      "father\n",
      ",\n",
      "no\n",
      "matter\n",
      "what\n",
      "it\n",
      "takes\n",
      ".\n",
      "finally\n",
      "back\n",
      "in\n",
      "sweden\n",
      "the\n",
      "whole\n",
      "situation\n",
      "seems\n",
      "to\n",
      "be\n",
      "more\n",
      "complicated\n",
      "than\n",
      "stefan\n",
      "ever\n",
      "thought\n",
      "of\n",
      "..\n",
      "<\n",
      "br\n",
      "/\n",
      ">\n",
      "<\n",
      "br\n",
      "/\n",
      ">\n",
      "this\n",
      "movie\n",
      "doesn't\n",
      "seem\n",
      "to\n",
      "fit\n",
      "in\n",
      "the\n",
      "ordinary\n",
      "class\n",
      "of\n",
      "swedish\n",
      "movies\n",
      "due\n",
      "to\n",
      "the\n",
      "fact\n",
      "that\n",
      "it's\n",
      "been\n",
      "americanized\n",
      "alot\n",
      ".\n",
      "regina\n",
      "lund\n",
      "and\n",
      "cecilia\n",
      "bergqvist\n",
      "makes\n",
      "it\n",
      "all\n",
      "average\n",
      ",\n",
      "the\n",
      "effects\n",
      "makes\n",
      "the\n",
      "movie\n",
      "a\n",
      "little\n",
      "too\n",
      "much\n",
      "though\n",
      ".\n",
      "see\n",
      "it\n",
      "and\n",
      "jugde\n",
      "for\n",
      "yourself\n",
      ".\n",
      "<\n",
      "br\n",
      "/\n",
      ">\n",
      "<\n",
      "br\n",
      "/\n",
      ">\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(rand_int, '\\n\\n')\n",
    "[print(i) for i in false_pos.iloc[rand_int,0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "223 is interesting because it is a great example of how position drastically affects sentiment. \n",
    "\n",
    "Here's a quote: \"Its a good family movie; however if michael landon jr.'s editing team did a better job of editing, the movie would be much better.\" Sounds negative clearly.\n",
    "Now rotating on the \"however\", we'd have:\n",
    "\"If michael landon jr.'s editing team did a better job of editing, the movie would be much better; however its a good family movie\". Here, it sounds more positive! A model change might be in order. LTSM or CNN would be great at detecting these.\n",
    "\n",
    "\n",
    "320 is filled with jokes that go over the neural network's head. Also filled with \"positive words\" but not saying somehting positive. Example: \"it's\n",
    "as if this movie was dreamt up at a shortland street cast christmas party , the result of too many gins , and possibly a bit of salmonella\". This sentence is clearly negative to the human eye. But with data that simply aggregates through words, this sentence is filled with positive words and decoded in jokes.\n",
    "\n",
    "\n",
    "55 is another example of the summary taking up a huge portion of the review, and then the actual opinionated piece is left at the end. Perhaps a solution to this is to find patterns in summaries so as to be able to separate summary from opinion like opinion statements usually have sentences like \"this movie ____\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11438</th>\n",
       "      <td>[there, is, a, bit, of, trivia, which, should,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11278</th>\n",
       "      <td>[\", the, blob, \", qualifies, as, a, cult, sci-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>[\", cut, \", is, a, full-tilt, spoof, of, the, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5336</th>\n",
       "      <td>[guys, and, dolls, is, a, movie, itching, for,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2741</th>\n",
       "      <td>[ok, ,, this, movie, starts, out, like, a, che...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4778</th>\n",
       "      <td>[this, is, simply, the, funniest, movie, i've,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8854</th>\n",
       "      <td>[i, remember, seeing, this, when, it, was, rel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4914</th>\n",
       "      <td>[an, imagination, is, a, terrible, thing, to, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2475</th>\n",
       "      <td>[i, must, admit, i'm, a, little, surprised, an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>[to, all, the, miserable, people, who, have, d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10191</th>\n",
       "      <td>[while, visiting, romania, with, his, cia, dad...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11937</th>\n",
       "      <td>[superbly, trashy, and, wondrously, unpretenti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10273</th>\n",
       "      <td>[i, loved, this, movie, ., i'm, a, mexican, an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3840</th>\n",
       "      <td>[don't, believe, all, of, the, negative, revie...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835</th>\n",
       "      <td>[i, came, across, this, film, by, accident, wh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>[wracked, with, guilt, after, a, lot, of, thin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9210</th>\n",
       "      <td>[i, rated, this, a, ten, just, because, i, fin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9481</th>\n",
       "      <td>[when, i, was, young, i, had, seen, very, few,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10203</th>\n",
       "      <td>[according, to, this, board, ,, i, guess, eith...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9500</th>\n",
       "      <td>[like, the, gentle, giants, that, make, up, th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>[difficult, film, to, comment, on, ,, how, do,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>[the, odd, couple, is, a, comic, gem, ., one, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3714</th>\n",
       "      <td>[this, is, a, bit, long, (, 2, hours, ,, 20, m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1639</th>\n",
       "      <td>[zero, day, is, based, of, columbine, high, sc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6438</th>\n",
       "      <td>[i, saw, this, movie, recently, ., 2, hours, l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9763</th>\n",
       "      <td>[nothing, new, is, this, tired, serio-comedy, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2294</th>\n",
       "      <td>[\", insignificance, \", is, a, far, from, great...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5467</th>\n",
       "      <td>[\", hollywood, north, \", is, an, euphemism, fr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10053</th>\n",
       "      <td>[in, northeastern, of, brazil, ,, the, father,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>[loved, the, movie, ., i, even, liked, most, o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4018</th>\n",
       "      <td>[i, could, not, take, my, eyes, off, this, mov...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6523</th>\n",
       "      <td>[considering, all, the, teen, films, like, \", ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10376</th>\n",
       "      <td>[i, only, wish, that, return, of, the, jedi, ,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11034</th>\n",
       "      <td>[i, am, sad, to, say, that, i, disagree, with,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7291</th>\n",
       "      <td>[edward, furlong, and, christina, ricci, are, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9798</th>\n",
       "      <td>[i, have, to, say, although, i, despise, these...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5283</th>\n",
       "      <td>[i, have, no, idea, how, imdb, sorts, reviews,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9157</th>\n",
       "      <td>[this, is, not, bela, lagosi's, best, movie, ,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5294</th>\n",
       "      <td>[pretty, good, picture, about, a, man, being, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>[john, (, ben, chaplin, ), is, a, lonely, bank...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8079</th>\n",
       "      <td>[though, this, may, not, necessarily, be, a, s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2257</th>\n",
       "      <td>[(, warning, :, minor, spoilers, ), &lt;, br, /, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12461</th>\n",
       "      <td>[there, are, some, things, i, will, never, und...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9911</th>\n",
       "      <td>[dr, ., mordrid, ,, what, can, i, say, ?, jeff...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8283</th>\n",
       "      <td>[convoluted, ,, infuriating, and, implausible,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8875</th>\n",
       "      <td>[\", film, noir, \", is, an, overused, expressio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3681</th>\n",
       "      <td>[verry, classic, plot, but, a, verry, fun, hor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8489</th>\n",
       "      <td>[caught, this, film, at, the, arizona, interna...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>[all, i, have, to, say, is, if, you, don't, li...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4432</th>\n",
       "      <td>[it's, hard, for, me, to, criticize, anything,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>[this, is, one, of, the, most, guilty, pleasur...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>[like, a, lot, of, series, pilots, ,, dark, an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>[being, a, giant, monster, fan, ,, me, seeing,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11729</th>\n",
       "      <td>[not, a, movie, for, everyone, ,, but, this, m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10961</th>\n",
       "      <td>[what, if, a, platoon, of, g, ., i, ., ', s, f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>[there, are, a, few, spoilers, in, this, comme...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11162</th>\n",
       "      <td>[first, than, anything, ,, i'm, not, going, to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7722</th>\n",
       "      <td>[i've, read, many, negative, reviews, of, this...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9250</th>\n",
       "      <td>[a, very, strong, movie, ., bruce, is, good, a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11252</th>\n",
       "      <td>[*, *, *, spoilers, *, *, *, &lt;, br, /, &gt;, &lt;, b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>292 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  pos\n",
       "11438  [there, is, a, bit, of, trivia, which, should,...    1\n",
       "11278  [\", the, blob, \", qualifies, as, a, cult, sci-...    1\n",
       "978    [\", cut, \", is, a, full-tilt, spoof, of, the, ...    1\n",
       "5336   [guys, and, dolls, is, a, movie, itching, for,...    1\n",
       "2741   [ok, ,, this, movie, starts, out, like, a, che...    1\n",
       "4778   [this, is, simply, the, funniest, movie, i've,...    1\n",
       "8854   [i, remember, seeing, this, when, it, was, rel...    1\n",
       "4914   [an, imagination, is, a, terrible, thing, to, ...    1\n",
       "2475   [i, must, admit, i'm, a, little, surprised, an...    1\n",
       "52     [to, all, the, miserable, people, who, have, d...    1\n",
       "10191  [while, visiting, romania, with, his, cia, dad...    1\n",
       "11937  [superbly, trashy, and, wondrously, unpretenti...    1\n",
       "10273  [i, loved, this, movie, ., i'm, a, mexican, an...    1\n",
       "3840   [don't, believe, all, of, the, negative, revie...    1\n",
       "1835   [i, came, across, this, film, by, accident, wh...    1\n",
       "1147   [wracked, with, guilt, after, a, lot, of, thin...    1\n",
       "9210   [i, rated, this, a, ten, just, because, i, fin...    1\n",
       "9481   [when, i, was, young, i, had, seen, very, few,...    1\n",
       "10203  [according, to, this, board, ,, i, guess, eith...    1\n",
       "9500   [like, the, gentle, giants, that, make, up, th...    1\n",
       "824    [difficult, film, to, comment, on, ,, how, do,...    1\n",
       "1731   [the, odd, couple, is, a, comic, gem, ., one, ...    1\n",
       "3714   [this, is, a, bit, long, (, 2, hours, ,, 20, m...    1\n",
       "1639   [zero, day, is, based, of, columbine, high, sc...    1\n",
       "6438   [i, saw, this, movie, recently, ., 2, hours, l...    1\n",
       "9763   [nothing, new, is, this, tired, serio-comedy, ...    1\n",
       "2294   [\", insignificance, \", is, a, far, from, great...    1\n",
       "5467   [\", hollywood, north, \", is, an, euphemism, fr...    1\n",
       "10053  [in, northeastern, of, brazil, ,, the, father,...    1\n",
       "432    [loved, the, movie, ., i, even, liked, most, o...    1\n",
       "...                                                  ...  ...\n",
       "4018   [i, could, not, take, my, eyes, off, this, mov...    1\n",
       "6523   [considering, all, the, teen, films, like, \", ...    1\n",
       "10376  [i, only, wish, that, return, of, the, jedi, ,...    1\n",
       "11034  [i, am, sad, to, say, that, i, disagree, with,...    1\n",
       "7291   [edward, furlong, and, christina, ricci, are, ...    1\n",
       "9798   [i, have, to, say, although, i, despise, these...    1\n",
       "5283   [i, have, no, idea, how, imdb, sorts, reviews,...    1\n",
       "9157   [this, is, not, bela, lagosi's, best, movie, ,...    1\n",
       "5294   [pretty, good, picture, about, a, man, being, ...    1\n",
       "786    [john, (, ben, chaplin, ), is, a, lonely, bank...    1\n",
       "8079   [though, this, may, not, necessarily, be, a, s...    1\n",
       "2257   [(, warning, :, minor, spoilers, ), <, br, /, ...    1\n",
       "12461  [there, are, some, things, i, will, never, und...    1\n",
       "9911   [dr, ., mordrid, ,, what, can, i, say, ?, jeff...    1\n",
       "8283   [convoluted, ,, infuriating, and, implausible,...    1\n",
       "8875   [\", film, noir, \", is, an, overused, expressio...    1\n",
       "3681   [verry, classic, plot, but, a, verry, fun, hor...    1\n",
       "8489   [caught, this, film, at, the, arizona, interna...    1\n",
       "621    [all, i, have, to, say, is, if, you, don't, li...    1\n",
       "4432   [it's, hard, for, me, to, criticize, anything,...    1\n",
       "801    [this, is, one, of, the, most, guilty, pleasur...    1\n",
       "754    [like, a, lot, of, series, pilots, ,, dark, an...    1\n",
       "253    [being, a, giant, monster, fan, ,, me, seeing,...    1\n",
       "11729  [not, a, movie, for, everyone, ,, but, this, m...    1\n",
       "10961  [what, if, a, platoon, of, g, ., i, ., ', s, f...    1\n",
       "880    [there, are, a, few, spoilers, in, this, comme...    1\n",
       "11162  [first, than, anything, ,, i'm, not, going, to...    1\n",
       "7722   [i've, read, many, negative, reviews, of, this...    1\n",
       "9250   [a, very, strong, movie, ., bruce, is, good, a...    1\n",
       "11252  [*, *, *, spoilers, *, *, *, <, br, /, >, <, b...    1\n",
       "\n",
       "[292 rows x 2 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.726448  , 0.4589625 , 0.07255747, ..., 0.46413127, 0.20929205,\n",
       "       0.01794988], dtype=float32)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(model.predict(X_val), (-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8378839590443686"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_val, model.predict(X_val).round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>-2.709607</td>\n",
       "      <td>0.823322</td>\n",
       "      <td>-0.976731</td>\n",
       "      <td>-3.942001</td>\n",
       "      <td>2.466419</td>\n",
       "      <td>4.572990</td>\n",
       "      <td>-5.011948</td>\n",
       "      <td>-0.157489</td>\n",
       "      <td>0.726081</td>\n",
       "      <td>-54.957779</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.171843</td>\n",
       "      <td>-8.035381</td>\n",
       "      <td>1.083654</td>\n",
       "      <td>5.119356</td>\n",
       "      <td>1.752086</td>\n",
       "      <td>6.575515</td>\n",
       "      <td>2.586730</td>\n",
       "      <td>-8.790066</td>\n",
       "      <td>-1.765028</td>\n",
       "      <td>2.371102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7410</th>\n",
       "      <td>-23.788113</td>\n",
       "      <td>18.849995</td>\n",
       "      <td>-5.478467</td>\n",
       "      <td>-28.850559</td>\n",
       "      <td>-2.977987</td>\n",
       "      <td>0.345442</td>\n",
       "      <td>-12.294386</td>\n",
       "      <td>16.698851</td>\n",
       "      <td>10.019337</td>\n",
       "      <td>-342.338497</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.129589</td>\n",
       "      <td>-38.117047</td>\n",
       "      <td>-6.299583</td>\n",
       "      <td>23.221125</td>\n",
       "      <td>23.576995</td>\n",
       "      <td>22.822184</td>\n",
       "      <td>5.582503</td>\n",
       "      <td>-37.512159</td>\n",
       "      <td>-21.140500</td>\n",
       "      <td>13.774973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23895</th>\n",
       "      <td>-35.850669</td>\n",
       "      <td>15.220555</td>\n",
       "      <td>1.413695</td>\n",
       "      <td>-44.379361</td>\n",
       "      <td>-7.873908</td>\n",
       "      <td>31.699543</td>\n",
       "      <td>-34.713713</td>\n",
       "      <td>37.065645</td>\n",
       "      <td>9.976540</td>\n",
       "      <td>-458.033963</td>\n",
       "      <td>...</td>\n",
       "      <td>-20.993391</td>\n",
       "      <td>-43.211417</td>\n",
       "      <td>-20.704074</td>\n",
       "      <td>5.212338</td>\n",
       "      <td>36.088696</td>\n",
       "      <td>21.677928</td>\n",
       "      <td>11.571817</td>\n",
       "      <td>-69.118344</td>\n",
       "      <td>-21.848598</td>\n",
       "      <td>42.669947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10769</th>\n",
       "      <td>-10.986825</td>\n",
       "      <td>10.719739</td>\n",
       "      <td>3.078759</td>\n",
       "      <td>-13.467352</td>\n",
       "      <td>-1.792570</td>\n",
       "      <td>5.015190</td>\n",
       "      <td>-12.948430</td>\n",
       "      <td>4.535959</td>\n",
       "      <td>2.971679</td>\n",
       "      <td>-132.727480</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.892459</td>\n",
       "      <td>-17.914209</td>\n",
       "      <td>-9.408483</td>\n",
       "      <td>6.720133</td>\n",
       "      <td>9.273998</td>\n",
       "      <td>5.329989</td>\n",
       "      <td>4.608194</td>\n",
       "      <td>-9.265466</td>\n",
       "      <td>-12.741729</td>\n",
       "      <td>11.088184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11934</th>\n",
       "      <td>-13.702834</td>\n",
       "      <td>19.802053</td>\n",
       "      <td>-4.625199</td>\n",
       "      <td>-19.982145</td>\n",
       "      <td>-6.132876</td>\n",
       "      <td>4.284878</td>\n",
       "      <td>-10.975221</td>\n",
       "      <td>5.712880</td>\n",
       "      <td>11.948668</td>\n",
       "      <td>-238.715968</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.195186</td>\n",
       "      <td>-27.646590</td>\n",
       "      <td>0.126074</td>\n",
       "      <td>14.156489</td>\n",
       "      <td>7.526001</td>\n",
       "      <td>19.592931</td>\n",
       "      <td>4.628328</td>\n",
       "      <td>-28.717259</td>\n",
       "      <td>-19.414686</td>\n",
       "      <td>0.740996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20759</th>\n",
       "      <td>-22.053144</td>\n",
       "      <td>17.074883</td>\n",
       "      <td>-8.434952</td>\n",
       "      <td>-23.331036</td>\n",
       "      <td>-2.651745</td>\n",
       "      <td>11.588363</td>\n",
       "      <td>-14.444987</td>\n",
       "      <td>11.108250</td>\n",
       "      <td>15.252464</td>\n",
       "      <td>-275.128910</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.502355</td>\n",
       "      <td>-34.103729</td>\n",
       "      <td>3.371719</td>\n",
       "      <td>10.743932</td>\n",
       "      <td>7.673473</td>\n",
       "      <td>-0.151231</td>\n",
       "      <td>-3.961266</td>\n",
       "      <td>-26.844456</td>\n",
       "      <td>-8.766967</td>\n",
       "      <td>3.362617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21388</th>\n",
       "      <td>-42.377950</td>\n",
       "      <td>32.686152</td>\n",
       "      <td>-2.381558</td>\n",
       "      <td>-64.923632</td>\n",
       "      <td>-20.516477</td>\n",
       "      <td>30.028512</td>\n",
       "      <td>-54.464985</td>\n",
       "      <td>39.650377</td>\n",
       "      <td>30.458246</td>\n",
       "      <td>-720.329316</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.827593</td>\n",
       "      <td>-77.953762</td>\n",
       "      <td>-18.054393</td>\n",
       "      <td>29.590047</td>\n",
       "      <td>46.779906</td>\n",
       "      <td>5.194965</td>\n",
       "      <td>16.711095</td>\n",
       "      <td>-75.315097</td>\n",
       "      <td>-41.500587</td>\n",
       "      <td>44.811492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13678</th>\n",
       "      <td>-7.541792</td>\n",
       "      <td>6.129093</td>\n",
       "      <td>-0.594800</td>\n",
       "      <td>-6.274873</td>\n",
       "      <td>1.609949</td>\n",
       "      <td>-0.213843</td>\n",
       "      <td>-6.886704</td>\n",
       "      <td>6.109926</td>\n",
       "      <td>7.357906</td>\n",
       "      <td>-106.628828</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.370001</td>\n",
       "      <td>-13.300510</td>\n",
       "      <td>1.454698</td>\n",
       "      <td>3.520301</td>\n",
       "      <td>8.904697</td>\n",
       "      <td>8.872277</td>\n",
       "      <td>-0.344592</td>\n",
       "      <td>-13.677129</td>\n",
       "      <td>-7.637608</td>\n",
       "      <td>4.256431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24133</th>\n",
       "      <td>-14.680323</td>\n",
       "      <td>5.433444</td>\n",
       "      <td>-7.738582</td>\n",
       "      <td>-14.968245</td>\n",
       "      <td>-1.674337</td>\n",
       "      <td>6.351103</td>\n",
       "      <td>-15.944126</td>\n",
       "      <td>6.924268</td>\n",
       "      <td>0.089956</td>\n",
       "      <td>-159.389650</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.140383</td>\n",
       "      <td>-15.928990</td>\n",
       "      <td>-3.467835</td>\n",
       "      <td>2.890319</td>\n",
       "      <td>12.100413</td>\n",
       "      <td>0.381100</td>\n",
       "      <td>8.298843</td>\n",
       "      <td>-25.018817</td>\n",
       "      <td>-7.148896</td>\n",
       "      <td>18.951194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4114</th>\n",
       "      <td>-5.544905</td>\n",
       "      <td>6.907739</td>\n",
       "      <td>15.471430</td>\n",
       "      <td>-13.956575</td>\n",
       "      <td>2.924074</td>\n",
       "      <td>3.308911</td>\n",
       "      <td>-14.841988</td>\n",
       "      <td>29.147561</td>\n",
       "      <td>9.979552</td>\n",
       "      <td>-239.164830</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.826192</td>\n",
       "      <td>-31.208858</td>\n",
       "      <td>1.823677</td>\n",
       "      <td>5.789113</td>\n",
       "      <td>5.687960</td>\n",
       "      <td>35.277971</td>\n",
       "      <td>-2.175263</td>\n",
       "      <td>-33.342825</td>\n",
       "      <td>-22.623583</td>\n",
       "      <td>9.120666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17178</th>\n",
       "      <td>-17.635152</td>\n",
       "      <td>15.024632</td>\n",
       "      <td>-5.425530</td>\n",
       "      <td>-11.174331</td>\n",
       "      <td>-3.894342</td>\n",
       "      <td>8.941169</td>\n",
       "      <td>-10.579772</td>\n",
       "      <td>2.531203</td>\n",
       "      <td>11.211590</td>\n",
       "      <td>-188.086081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.350070</td>\n",
       "      <td>-27.712951</td>\n",
       "      <td>1.824640</td>\n",
       "      <td>7.358192</td>\n",
       "      <td>12.442729</td>\n",
       "      <td>-0.038419</td>\n",
       "      <td>6.188585</td>\n",
       "      <td>-27.028565</td>\n",
       "      <td>-11.562124</td>\n",
       "      <td>5.805018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24040</th>\n",
       "      <td>-34.137248</td>\n",
       "      <td>11.821856</td>\n",
       "      <td>-10.746266</td>\n",
       "      <td>-39.348166</td>\n",
       "      <td>-11.998774</td>\n",
       "      <td>20.910849</td>\n",
       "      <td>-25.526554</td>\n",
       "      <td>12.494822</td>\n",
       "      <td>9.639688</td>\n",
       "      <td>-431.715341</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.089056</td>\n",
       "      <td>-45.386066</td>\n",
       "      <td>-11.704773</td>\n",
       "      <td>6.196250</td>\n",
       "      <td>35.751139</td>\n",
       "      <td>11.901527</td>\n",
       "      <td>25.515976</td>\n",
       "      <td>-49.792218</td>\n",
       "      <td>-30.145137</td>\n",
       "      <td>32.132744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6956</th>\n",
       "      <td>-91.516994</td>\n",
       "      <td>52.274754</td>\n",
       "      <td>-52.616213</td>\n",
       "      <td>-114.367492</td>\n",
       "      <td>-61.598924</td>\n",
       "      <td>34.118924</td>\n",
       "      <td>-110.584593</td>\n",
       "      <td>32.890318</td>\n",
       "      <td>-14.976800</td>\n",
       "      <td>-1580.101400</td>\n",
       "      <td>...</td>\n",
       "      <td>-58.295342</td>\n",
       "      <td>-162.102638</td>\n",
       "      <td>-14.835520</td>\n",
       "      <td>34.333048</td>\n",
       "      <td>78.321964</td>\n",
       "      <td>43.528371</td>\n",
       "      <td>89.786181</td>\n",
       "      <td>-183.885912</td>\n",
       "      <td>-103.693346</td>\n",
       "      <td>119.396108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20657</th>\n",
       "      <td>-61.914712</td>\n",
       "      <td>42.696366</td>\n",
       "      <td>0.541368</td>\n",
       "      <td>-50.407364</td>\n",
       "      <td>-23.787768</td>\n",
       "      <td>35.639725</td>\n",
       "      <td>-40.469919</td>\n",
       "      <td>35.103623</td>\n",
       "      <td>16.111696</td>\n",
       "      <td>-685.626899</td>\n",
       "      <td>...</td>\n",
       "      <td>-20.008726</td>\n",
       "      <td>-75.858099</td>\n",
       "      <td>-9.509220</td>\n",
       "      <td>17.381242</td>\n",
       "      <td>68.933670</td>\n",
       "      <td>6.970753</td>\n",
       "      <td>30.325388</td>\n",
       "      <td>-88.020388</td>\n",
       "      <td>-45.870046</td>\n",
       "      <td>37.578448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>-62.818643</td>\n",
       "      <td>38.271435</td>\n",
       "      <td>4.865248</td>\n",
       "      <td>-116.327583</td>\n",
       "      <td>-25.147169</td>\n",
       "      <td>19.254611</td>\n",
       "      <td>-68.374034</td>\n",
       "      <td>75.122895</td>\n",
       "      <td>17.877368</td>\n",
       "      <td>-949.093840</td>\n",
       "      <td>...</td>\n",
       "      <td>-45.790695</td>\n",
       "      <td>-88.607616</td>\n",
       "      <td>9.718939</td>\n",
       "      <td>6.280769</td>\n",
       "      <td>72.204049</td>\n",
       "      <td>109.209820</td>\n",
       "      <td>7.305168</td>\n",
       "      <td>-137.083864</td>\n",
       "      <td>-61.806656</td>\n",
       "      <td>39.806637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9722</th>\n",
       "      <td>-10.554987</td>\n",
       "      <td>18.293404</td>\n",
       "      <td>3.678934</td>\n",
       "      <td>-11.550363</td>\n",
       "      <td>-1.442203</td>\n",
       "      <td>10.294282</td>\n",
       "      <td>-5.235103</td>\n",
       "      <td>8.549960</td>\n",
       "      <td>10.290837</td>\n",
       "      <td>-170.107648</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.824146</td>\n",
       "      <td>-19.358070</td>\n",
       "      <td>-0.425231</td>\n",
       "      <td>10.595602</td>\n",
       "      <td>4.428870</td>\n",
       "      <td>0.338749</td>\n",
       "      <td>0.788579</td>\n",
       "      <td>-18.276870</td>\n",
       "      <td>-16.499237</td>\n",
       "      <td>-1.503728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8550</th>\n",
       "      <td>-23.916559</td>\n",
       "      <td>4.901791</td>\n",
       "      <td>-12.783871</td>\n",
       "      <td>-25.181798</td>\n",
       "      <td>-11.468971</td>\n",
       "      <td>6.784252</td>\n",
       "      <td>-24.808197</td>\n",
       "      <td>5.720500</td>\n",
       "      <td>13.941881</td>\n",
       "      <td>-296.114011</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.341810</td>\n",
       "      <td>-32.060028</td>\n",
       "      <td>-2.630740</td>\n",
       "      <td>1.894903</td>\n",
       "      <td>21.141129</td>\n",
       "      <td>15.967242</td>\n",
       "      <td>15.508029</td>\n",
       "      <td>-29.628169</td>\n",
       "      <td>-16.566734</td>\n",
       "      <td>16.177913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6272</th>\n",
       "      <td>-10.524945</td>\n",
       "      <td>2.056857</td>\n",
       "      <td>-1.750165</td>\n",
       "      <td>-16.521204</td>\n",
       "      <td>-2.468580</td>\n",
       "      <td>13.028403</td>\n",
       "      <td>-15.045720</td>\n",
       "      <td>26.526282</td>\n",
       "      <td>-6.916416</td>\n",
       "      <td>-211.754427</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.750727</td>\n",
       "      <td>-12.531376</td>\n",
       "      <td>2.316930</td>\n",
       "      <td>-2.453000</td>\n",
       "      <td>13.557028</td>\n",
       "      <td>-1.573377</td>\n",
       "      <td>11.605980</td>\n",
       "      <td>-36.639773</td>\n",
       "      <td>-3.663009</td>\n",
       "      <td>13.151600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19673</th>\n",
       "      <td>-20.279389</td>\n",
       "      <td>16.537558</td>\n",
       "      <td>-7.966284</td>\n",
       "      <td>-46.427341</td>\n",
       "      <td>-7.548783</td>\n",
       "      <td>22.509497</td>\n",
       "      <td>-21.977801</td>\n",
       "      <td>31.516395</td>\n",
       "      <td>0.531357</td>\n",
       "      <td>-332.587640</td>\n",
       "      <td>...</td>\n",
       "      <td>-18.127918</td>\n",
       "      <td>-33.764294</td>\n",
       "      <td>0.834980</td>\n",
       "      <td>-0.211265</td>\n",
       "      <td>35.433119</td>\n",
       "      <td>7.023923</td>\n",
       "      <td>17.636970</td>\n",
       "      <td>-41.483486</td>\n",
       "      <td>-14.070973</td>\n",
       "      <td>18.460387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7453</th>\n",
       "      <td>-12.497348</td>\n",
       "      <td>-2.265612</td>\n",
       "      <td>8.483896</td>\n",
       "      <td>-33.964158</td>\n",
       "      <td>-19.962167</td>\n",
       "      <td>39.119092</td>\n",
       "      <td>-36.806663</td>\n",
       "      <td>73.536870</td>\n",
       "      <td>-26.690043</td>\n",
       "      <td>-318.526219</td>\n",
       "      <td>...</td>\n",
       "      <td>-43.859190</td>\n",
       "      <td>-10.297078</td>\n",
       "      <td>0.046708</td>\n",
       "      <td>-25.237965</td>\n",
       "      <td>41.006575</td>\n",
       "      <td>-26.674120</td>\n",
       "      <td>33.880869</td>\n",
       "      <td>-74.359070</td>\n",
       "      <td>2.091837</td>\n",
       "      <td>31.534452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>-8.694349</td>\n",
       "      <td>7.204436</td>\n",
       "      <td>-1.723532</td>\n",
       "      <td>-15.276075</td>\n",
       "      <td>-3.207695</td>\n",
       "      <td>8.371777</td>\n",
       "      <td>-11.845626</td>\n",
       "      <td>7.413905</td>\n",
       "      <td>0.645721</td>\n",
       "      <td>-155.346151</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.485414</td>\n",
       "      <td>-15.926581</td>\n",
       "      <td>-8.888863</td>\n",
       "      <td>9.101328</td>\n",
       "      <td>11.363404</td>\n",
       "      <td>-8.706879</td>\n",
       "      <td>1.195394</td>\n",
       "      <td>-19.194101</td>\n",
       "      <td>-3.148790</td>\n",
       "      <td>15.858376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21798</th>\n",
       "      <td>-26.486942</td>\n",
       "      <td>12.155884</td>\n",
       "      <td>-9.232964</td>\n",
       "      <td>-22.039451</td>\n",
       "      <td>-1.888616</td>\n",
       "      <td>4.156054</td>\n",
       "      <td>-17.112060</td>\n",
       "      <td>9.139269</td>\n",
       "      <td>4.758725</td>\n",
       "      <td>-256.882626</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.007603</td>\n",
       "      <td>-20.742316</td>\n",
       "      <td>7.513851</td>\n",
       "      <td>5.681077</td>\n",
       "      <td>17.617936</td>\n",
       "      <td>7.925859</td>\n",
       "      <td>11.999182</td>\n",
       "      <td>-31.533458</td>\n",
       "      <td>-8.043964</td>\n",
       "      <td>13.173580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15278</th>\n",
       "      <td>-69.856524</td>\n",
       "      <td>23.712133</td>\n",
       "      <td>-3.634923</td>\n",
       "      <td>-108.757975</td>\n",
       "      <td>-31.141270</td>\n",
       "      <td>54.226308</td>\n",
       "      <td>-81.540717</td>\n",
       "      <td>81.580393</td>\n",
       "      <td>16.293521</td>\n",
       "      <td>-1103.865205</td>\n",
       "      <td>...</td>\n",
       "      <td>-20.284524</td>\n",
       "      <td>-104.421857</td>\n",
       "      <td>3.525290</td>\n",
       "      <td>50.042522</td>\n",
       "      <td>73.953080</td>\n",
       "      <td>51.959726</td>\n",
       "      <td>27.216909</td>\n",
       "      <td>-145.854586</td>\n",
       "      <td>-61.183026</td>\n",
       "      <td>72.546931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19037</th>\n",
       "      <td>-23.636054</td>\n",
       "      <td>11.513394</td>\n",
       "      <td>-4.329027</td>\n",
       "      <td>-28.124834</td>\n",
       "      <td>-6.147302</td>\n",
       "      <td>11.882611</td>\n",
       "      <td>-16.675913</td>\n",
       "      <td>7.222268</td>\n",
       "      <td>14.560933</td>\n",
       "      <td>-337.465830</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.643878</td>\n",
       "      <td>-38.617855</td>\n",
       "      <td>3.077293</td>\n",
       "      <td>9.424583</td>\n",
       "      <td>16.966189</td>\n",
       "      <td>-0.971775</td>\n",
       "      <td>-2.018547</td>\n",
       "      <td>-29.752288</td>\n",
       "      <td>-20.689244</td>\n",
       "      <td>22.809218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3441</th>\n",
       "      <td>-56.276520</td>\n",
       "      <td>24.709320</td>\n",
       "      <td>16.943392</td>\n",
       "      <td>-94.586030</td>\n",
       "      <td>-40.146525</td>\n",
       "      <td>47.382002</td>\n",
       "      <td>-79.260050</td>\n",
       "      <td>72.378042</td>\n",
       "      <td>2.135351</td>\n",
       "      <td>-801.801134</td>\n",
       "      <td>...</td>\n",
       "      <td>-63.209418</td>\n",
       "      <td>-65.137210</td>\n",
       "      <td>-5.831389</td>\n",
       "      <td>-15.684218</td>\n",
       "      <td>46.809611</td>\n",
       "      <td>76.860060</td>\n",
       "      <td>49.717672</td>\n",
       "      <td>-100.850171</td>\n",
       "      <td>-65.155665</td>\n",
       "      <td>46.828688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22749</th>\n",
       "      <td>-29.855294</td>\n",
       "      <td>7.149188</td>\n",
       "      <td>-4.481483</td>\n",
       "      <td>-51.091488</td>\n",
       "      <td>-16.732281</td>\n",
       "      <td>14.174086</td>\n",
       "      <td>-44.271995</td>\n",
       "      <td>28.421128</td>\n",
       "      <td>-5.544694</td>\n",
       "      <td>-371.868273</td>\n",
       "      <td>...</td>\n",
       "      <td>-30.120596</td>\n",
       "      <td>-29.834163</td>\n",
       "      <td>4.205755</td>\n",
       "      <td>4.181478</td>\n",
       "      <td>38.896434</td>\n",
       "      <td>6.472579</td>\n",
       "      <td>15.531713</td>\n",
       "      <td>-34.791097</td>\n",
       "      <td>-21.409873</td>\n",
       "      <td>24.415801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24147</th>\n",
       "      <td>-3.989927</td>\n",
       "      <td>4.686236</td>\n",
       "      <td>-0.501458</td>\n",
       "      <td>-5.200674</td>\n",
       "      <td>-2.823350</td>\n",
       "      <td>-0.130515</td>\n",
       "      <td>-2.690244</td>\n",
       "      <td>2.764236</td>\n",
       "      <td>2.858595</td>\n",
       "      <td>-81.611090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.556223</td>\n",
       "      <td>-7.419544</td>\n",
       "      <td>-2.115418</td>\n",
       "      <td>8.344134</td>\n",
       "      <td>0.377605</td>\n",
       "      <td>2.453584</td>\n",
       "      <td>3.366217</td>\n",
       "      <td>-5.990797</td>\n",
       "      <td>-4.235276</td>\n",
       "      <td>5.533039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3921</th>\n",
       "      <td>-11.576901</td>\n",
       "      <td>6.835486</td>\n",
       "      <td>0.667105</td>\n",
       "      <td>-13.734924</td>\n",
       "      <td>-2.997944</td>\n",
       "      <td>-1.417964</td>\n",
       "      <td>-15.989688</td>\n",
       "      <td>3.036850</td>\n",
       "      <td>5.678731</td>\n",
       "      <td>-152.090173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.677100</td>\n",
       "      <td>-20.877940</td>\n",
       "      <td>-1.113330</td>\n",
       "      <td>4.978338</td>\n",
       "      <td>0.390237</td>\n",
       "      <td>18.225844</td>\n",
       "      <td>3.766373</td>\n",
       "      <td>-11.577898</td>\n",
       "      <td>-15.167913</td>\n",
       "      <td>8.656410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13256</th>\n",
       "      <td>-13.422250</td>\n",
       "      <td>15.967827</td>\n",
       "      <td>0.541279</td>\n",
       "      <td>-20.880829</td>\n",
       "      <td>-9.052532</td>\n",
       "      <td>7.523625</td>\n",
       "      <td>-26.246926</td>\n",
       "      <td>14.467357</td>\n",
       "      <td>6.543092</td>\n",
       "      <td>-275.658067</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.721702</td>\n",
       "      <td>-33.711442</td>\n",
       "      <td>-5.231274</td>\n",
       "      <td>10.041005</td>\n",
       "      <td>6.037990</td>\n",
       "      <td>22.913020</td>\n",
       "      <td>10.326905</td>\n",
       "      <td>-35.332736</td>\n",
       "      <td>-24.098249</td>\n",
       "      <td>22.227937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16435</th>\n",
       "      <td>-29.162212</td>\n",
       "      <td>25.702320</td>\n",
       "      <td>9.399868</td>\n",
       "      <td>-28.383447</td>\n",
       "      <td>-18.962298</td>\n",
       "      <td>42.109186</td>\n",
       "      <td>-41.805056</td>\n",
       "      <td>45.812356</td>\n",
       "      <td>-1.561793</td>\n",
       "      <td>-407.691049</td>\n",
       "      <td>...</td>\n",
       "      <td>-28.159647</td>\n",
       "      <td>-32.248900</td>\n",
       "      <td>-10.401135</td>\n",
       "      <td>6.772282</td>\n",
       "      <td>28.876440</td>\n",
       "      <td>-6.342793</td>\n",
       "      <td>19.486085</td>\n",
       "      <td>-77.279025</td>\n",
       "      <td>6.872116</td>\n",
       "      <td>35.621059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18044</th>\n",
       "      <td>-17.299292</td>\n",
       "      <td>13.569815</td>\n",
       "      <td>-12.008714</td>\n",
       "      <td>-28.618839</td>\n",
       "      <td>-17.119542</td>\n",
       "      <td>12.354782</td>\n",
       "      <td>-25.226864</td>\n",
       "      <td>31.875416</td>\n",
       "      <td>8.878036</td>\n",
       "      <td>-370.907944</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.961557</td>\n",
       "      <td>-48.946702</td>\n",
       "      <td>5.751652</td>\n",
       "      <td>14.417736</td>\n",
       "      <td>28.186744</td>\n",
       "      <td>25.170991</td>\n",
       "      <td>15.249298</td>\n",
       "      <td>-45.859094</td>\n",
       "      <td>-19.161795</td>\n",
       "      <td>26.066287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19063</th>\n",
       "      <td>-50.629381</td>\n",
       "      <td>25.236919</td>\n",
       "      <td>0.713171</td>\n",
       "      <td>-31.733271</td>\n",
       "      <td>-5.357775</td>\n",
       "      <td>-1.075426</td>\n",
       "      <td>-32.619189</td>\n",
       "      <td>0.096529</td>\n",
       "      <td>14.763721</td>\n",
       "      <td>-439.019189</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.774474</td>\n",
       "      <td>-38.748451</td>\n",
       "      <td>-17.961948</td>\n",
       "      <td>13.771792</td>\n",
       "      <td>33.810693</td>\n",
       "      <td>41.866615</td>\n",
       "      <td>14.847798</td>\n",
       "      <td>-42.521002</td>\n",
       "      <td>-37.862705</td>\n",
       "      <td>29.554397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14135</th>\n",
       "      <td>-5.936827</td>\n",
       "      <td>2.513216</td>\n",
       "      <td>-6.172071</td>\n",
       "      <td>-14.840147</td>\n",
       "      <td>-5.526911</td>\n",
       "      <td>3.915300</td>\n",
       "      <td>-9.372801</td>\n",
       "      <td>10.879915</td>\n",
       "      <td>2.238434</td>\n",
       "      <td>-131.630420</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.644825</td>\n",
       "      <td>-15.986249</td>\n",
       "      <td>-1.828669</td>\n",
       "      <td>-2.962533</td>\n",
       "      <td>12.704037</td>\n",
       "      <td>-0.981785</td>\n",
       "      <td>0.622333</td>\n",
       "      <td>-17.848688</td>\n",
       "      <td>-2.275290</td>\n",
       "      <td>5.411064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9001</th>\n",
       "      <td>-76.292290</td>\n",
       "      <td>37.614296</td>\n",
       "      <td>9.673225</td>\n",
       "      <td>-93.375728</td>\n",
       "      <td>-4.186533</td>\n",
       "      <td>25.551793</td>\n",
       "      <td>-89.199038</td>\n",
       "      <td>43.561187</td>\n",
       "      <td>41.967603</td>\n",
       "      <td>-1007.815200</td>\n",
       "      <td>...</td>\n",
       "      <td>-40.339309</td>\n",
       "      <td>-96.080122</td>\n",
       "      <td>-24.307046</td>\n",
       "      <td>50.248572</td>\n",
       "      <td>102.028192</td>\n",
       "      <td>115.806666</td>\n",
       "      <td>26.248288</td>\n",
       "      <td>-102.357000</td>\n",
       "      <td>-76.521082</td>\n",
       "      <td>40.759889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3732</th>\n",
       "      <td>-18.060386</td>\n",
       "      <td>4.617568</td>\n",
       "      <td>0.126421</td>\n",
       "      <td>-6.254873</td>\n",
       "      <td>2.915992</td>\n",
       "      <td>7.915182</td>\n",
       "      <td>-5.539584</td>\n",
       "      <td>0.132867</td>\n",
       "      <td>6.709639</td>\n",
       "      <td>-191.267771</td>\n",
       "      <td>...</td>\n",
       "      <td>3.333621</td>\n",
       "      <td>-13.579564</td>\n",
       "      <td>3.448079</td>\n",
       "      <td>5.873173</td>\n",
       "      <td>10.429939</td>\n",
       "      <td>-2.004042</td>\n",
       "      <td>-2.281200</td>\n",
       "      <td>-25.085898</td>\n",
       "      <td>-9.946902</td>\n",
       "      <td>15.421309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22233</th>\n",
       "      <td>-20.753592</td>\n",
       "      <td>9.187054</td>\n",
       "      <td>3.264305</td>\n",
       "      <td>-21.347526</td>\n",
       "      <td>-14.310382</td>\n",
       "      <td>12.807159</td>\n",
       "      <td>-30.323598</td>\n",
       "      <td>24.669629</td>\n",
       "      <td>17.720187</td>\n",
       "      <td>-369.979979</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.267588</td>\n",
       "      <td>-24.848232</td>\n",
       "      <td>0.048142</td>\n",
       "      <td>17.914561</td>\n",
       "      <td>25.488767</td>\n",
       "      <td>16.688674</td>\n",
       "      <td>8.901095</td>\n",
       "      <td>-54.772730</td>\n",
       "      <td>-18.370131</td>\n",
       "      <td>18.016334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3396</th>\n",
       "      <td>-18.642317</td>\n",
       "      <td>9.877814</td>\n",
       "      <td>-1.224489</td>\n",
       "      <td>-19.441003</td>\n",
       "      <td>-3.979040</td>\n",
       "      <td>10.278570</td>\n",
       "      <td>-18.606195</td>\n",
       "      <td>12.696260</td>\n",
       "      <td>12.786393</td>\n",
       "      <td>-251.493070</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.385506</td>\n",
       "      <td>-26.196744</td>\n",
       "      <td>-6.033366</td>\n",
       "      <td>10.388296</td>\n",
       "      <td>24.887487</td>\n",
       "      <td>1.054616</td>\n",
       "      <td>8.776082</td>\n",
       "      <td>-29.509978</td>\n",
       "      <td>-5.587186</td>\n",
       "      <td>19.945198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21321</th>\n",
       "      <td>-20.718045</td>\n",
       "      <td>19.945724</td>\n",
       "      <td>-11.095311</td>\n",
       "      <td>-22.968617</td>\n",
       "      <td>-10.075198</td>\n",
       "      <td>12.838906</td>\n",
       "      <td>-13.025911</td>\n",
       "      <td>5.571900</td>\n",
       "      <td>10.330121</td>\n",
       "      <td>-287.525323</td>\n",
       "      <td>...</td>\n",
       "      <td>5.803207</td>\n",
       "      <td>-36.551702</td>\n",
       "      <td>-0.903895</td>\n",
       "      <td>7.905721</td>\n",
       "      <td>10.756023</td>\n",
       "      <td>-0.496114</td>\n",
       "      <td>0.938126</td>\n",
       "      <td>-36.570446</td>\n",
       "      <td>-11.452912</td>\n",
       "      <td>11.695590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7871</th>\n",
       "      <td>-16.732786</td>\n",
       "      <td>7.242041</td>\n",
       "      <td>6.477521</td>\n",
       "      <td>-32.065223</td>\n",
       "      <td>-4.657519</td>\n",
       "      <td>30.616734</td>\n",
       "      <td>-25.281264</td>\n",
       "      <td>36.540971</td>\n",
       "      <td>-4.710188</td>\n",
       "      <td>-244.032224</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.357606</td>\n",
       "      <td>-27.386472</td>\n",
       "      <td>-6.588975</td>\n",
       "      <td>-6.301148</td>\n",
       "      <td>15.004780</td>\n",
       "      <td>16.298910</td>\n",
       "      <td>12.415117</td>\n",
       "      <td>-27.285070</td>\n",
       "      <td>-18.151567</td>\n",
       "      <td>27.087421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505</th>\n",
       "      <td>-28.814689</td>\n",
       "      <td>17.622726</td>\n",
       "      <td>1.494340</td>\n",
       "      <td>-48.205649</td>\n",
       "      <td>-4.250267</td>\n",
       "      <td>9.618395</td>\n",
       "      <td>-22.061856</td>\n",
       "      <td>12.365642</td>\n",
       "      <td>5.403279</td>\n",
       "      <td>-389.890051</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.793875</td>\n",
       "      <td>-36.448217</td>\n",
       "      <td>6.202009</td>\n",
       "      <td>20.062270</td>\n",
       "      <td>32.877512</td>\n",
       "      <td>30.933061</td>\n",
       "      <td>3.028407</td>\n",
       "      <td>-34.323680</td>\n",
       "      <td>-23.117758</td>\n",
       "      <td>27.999421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20359</th>\n",
       "      <td>-15.456523</td>\n",
       "      <td>11.386994</td>\n",
       "      <td>-1.243168</td>\n",
       "      <td>-5.179543</td>\n",
       "      <td>-7.692556</td>\n",
       "      <td>6.225951</td>\n",
       "      <td>-6.352669</td>\n",
       "      <td>16.384140</td>\n",
       "      <td>5.040921</td>\n",
       "      <td>-191.946817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.863140</td>\n",
       "      <td>-17.304777</td>\n",
       "      <td>-0.833943</td>\n",
       "      <td>11.707282</td>\n",
       "      <td>10.782700</td>\n",
       "      <td>-2.588566</td>\n",
       "      <td>2.772143</td>\n",
       "      <td>-26.851832</td>\n",
       "      <td>-9.590791</td>\n",
       "      <td>11.865068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5009</th>\n",
       "      <td>-21.368167</td>\n",
       "      <td>12.139053</td>\n",
       "      <td>-0.364689</td>\n",
       "      <td>-27.377376</td>\n",
       "      <td>-4.902337</td>\n",
       "      <td>14.080169</td>\n",
       "      <td>-13.126132</td>\n",
       "      <td>18.236577</td>\n",
       "      <td>0.295992</td>\n",
       "      <td>-318.367954</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.904352</td>\n",
       "      <td>-18.965574</td>\n",
       "      <td>2.032536</td>\n",
       "      <td>5.890766</td>\n",
       "      <td>23.480982</td>\n",
       "      <td>-6.827021</td>\n",
       "      <td>14.100212</td>\n",
       "      <td>-47.077942</td>\n",
       "      <td>-18.528949</td>\n",
       "      <td>26.072687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>-8.618833</td>\n",
       "      <td>4.165064</td>\n",
       "      <td>-2.845305</td>\n",
       "      <td>-6.910155</td>\n",
       "      <td>-1.965333</td>\n",
       "      <td>6.724853</td>\n",
       "      <td>-3.901254</td>\n",
       "      <td>2.416759</td>\n",
       "      <td>5.996268</td>\n",
       "      <td>-95.375070</td>\n",
       "      <td>...</td>\n",
       "      <td>1.085352</td>\n",
       "      <td>-13.592272</td>\n",
       "      <td>0.253477</td>\n",
       "      <td>4.347166</td>\n",
       "      <td>6.478355</td>\n",
       "      <td>-6.569979</td>\n",
       "      <td>-1.992117</td>\n",
       "      <td>-10.146030</td>\n",
       "      <td>-1.859755</td>\n",
       "      <td>6.855932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14416</th>\n",
       "      <td>-47.537979</td>\n",
       "      <td>30.618257</td>\n",
       "      <td>-10.202258</td>\n",
       "      <td>-38.629245</td>\n",
       "      <td>-16.185994</td>\n",
       "      <td>23.227377</td>\n",
       "      <td>-37.912715</td>\n",
       "      <td>37.926766</td>\n",
       "      <td>10.131252</td>\n",
       "      <td>-552.731931</td>\n",
       "      <td>...</td>\n",
       "      <td>-46.702003</td>\n",
       "      <td>-57.484203</td>\n",
       "      <td>-7.913888</td>\n",
       "      <td>1.990643</td>\n",
       "      <td>40.062098</td>\n",
       "      <td>59.012575</td>\n",
       "      <td>11.033079</td>\n",
       "      <td>-85.028549</td>\n",
       "      <td>-37.595807</td>\n",
       "      <td>30.083490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23051</th>\n",
       "      <td>-9.079233</td>\n",
       "      <td>4.073491</td>\n",
       "      <td>0.027731</td>\n",
       "      <td>-13.619125</td>\n",
       "      <td>-0.521916</td>\n",
       "      <td>7.202331</td>\n",
       "      <td>-5.144737</td>\n",
       "      <td>3.395290</td>\n",
       "      <td>7.143688</td>\n",
       "      <td>-144.826991</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.568975</td>\n",
       "      <td>-19.848392</td>\n",
       "      <td>0.379204</td>\n",
       "      <td>9.465365</td>\n",
       "      <td>10.500387</td>\n",
       "      <td>8.141598</td>\n",
       "      <td>-5.470872</td>\n",
       "      <td>-9.424971</td>\n",
       "      <td>-8.297707</td>\n",
       "      <td>-1.807881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22317</th>\n",
       "      <td>-49.399930</td>\n",
       "      <td>46.003318</td>\n",
       "      <td>-5.172739</td>\n",
       "      <td>-53.547132</td>\n",
       "      <td>-14.748130</td>\n",
       "      <td>23.787145</td>\n",
       "      <td>-53.965387</td>\n",
       "      <td>13.670101</td>\n",
       "      <td>26.819177</td>\n",
       "      <td>-662.287211</td>\n",
       "      <td>...</td>\n",
       "      <td>-22.126225</td>\n",
       "      <td>-68.889265</td>\n",
       "      <td>-26.650699</td>\n",
       "      <td>38.327870</td>\n",
       "      <td>33.963182</td>\n",
       "      <td>39.108426</td>\n",
       "      <td>20.522845</td>\n",
       "      <td>-38.119001</td>\n",
       "      <td>-56.627183</td>\n",
       "      <td>35.492129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7926</th>\n",
       "      <td>-5.627715</td>\n",
       "      <td>7.810474</td>\n",
       "      <td>-2.228828</td>\n",
       "      <td>-8.777959</td>\n",
       "      <td>-2.673588</td>\n",
       "      <td>6.679962</td>\n",
       "      <td>-6.190254</td>\n",
       "      <td>8.296447</td>\n",
       "      <td>-0.297202</td>\n",
       "      <td>-125.375480</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.974094</td>\n",
       "      <td>-8.249290</td>\n",
       "      <td>-0.878574</td>\n",
       "      <td>5.394216</td>\n",
       "      <td>9.527484</td>\n",
       "      <td>-3.788370</td>\n",
       "      <td>3.124698</td>\n",
       "      <td>-17.971417</td>\n",
       "      <td>-4.121198</td>\n",
       "      <td>10.788305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17912</th>\n",
       "      <td>-16.102884</td>\n",
       "      <td>19.168871</td>\n",
       "      <td>-5.165883</td>\n",
       "      <td>-24.595761</td>\n",
       "      <td>-10.462374</td>\n",
       "      <td>16.940099</td>\n",
       "      <td>-20.270281</td>\n",
       "      <td>17.659166</td>\n",
       "      <td>8.713944</td>\n",
       "      <td>-269.063245</td>\n",
       "      <td>...</td>\n",
       "      <td>3.587868</td>\n",
       "      <td>-30.670710</td>\n",
       "      <td>-1.007824</td>\n",
       "      <td>1.672720</td>\n",
       "      <td>8.959662</td>\n",
       "      <td>7.168338</td>\n",
       "      <td>8.476560</td>\n",
       "      <td>-26.182089</td>\n",
       "      <td>-10.627233</td>\n",
       "      <td>12.679737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16486</th>\n",
       "      <td>-16.825651</td>\n",
       "      <td>12.756521</td>\n",
       "      <td>-4.191933</td>\n",
       "      <td>-21.748686</td>\n",
       "      <td>-6.046085</td>\n",
       "      <td>14.019968</td>\n",
       "      <td>-31.388806</td>\n",
       "      <td>8.813285</td>\n",
       "      <td>5.688096</td>\n",
       "      <td>-265.741032</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.553938</td>\n",
       "      <td>-16.927210</td>\n",
       "      <td>2.921756</td>\n",
       "      <td>7.703459</td>\n",
       "      <td>8.289859</td>\n",
       "      <td>20.423596</td>\n",
       "      <td>-1.545213</td>\n",
       "      <td>-33.463613</td>\n",
       "      <td>-21.657549</td>\n",
       "      <td>12.790451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9090</th>\n",
       "      <td>-21.911924</td>\n",
       "      <td>15.211503</td>\n",
       "      <td>1.587957</td>\n",
       "      <td>-19.323233</td>\n",
       "      <td>0.228148</td>\n",
       "      <td>5.515551</td>\n",
       "      <td>-9.824107</td>\n",
       "      <td>13.257933</td>\n",
       "      <td>4.186626</td>\n",
       "      <td>-177.198294</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.980449</td>\n",
       "      <td>-17.564874</td>\n",
       "      <td>-2.102587</td>\n",
       "      <td>20.192307</td>\n",
       "      <td>13.573605</td>\n",
       "      <td>24.677526</td>\n",
       "      <td>5.552973</td>\n",
       "      <td>-20.443322</td>\n",
       "      <td>-20.643177</td>\n",
       "      <td>10.038302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3052</th>\n",
       "      <td>-21.578017</td>\n",
       "      <td>21.918035</td>\n",
       "      <td>-0.966661</td>\n",
       "      <td>-35.850762</td>\n",
       "      <td>-7.133798</td>\n",
       "      <td>12.219780</td>\n",
       "      <td>-23.578330</td>\n",
       "      <td>2.372196</td>\n",
       "      <td>8.496632</td>\n",
       "      <td>-298.004025</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.486910</td>\n",
       "      <td>-32.086949</td>\n",
       "      <td>-9.276874</td>\n",
       "      <td>16.396562</td>\n",
       "      <td>16.829784</td>\n",
       "      <td>29.680861</td>\n",
       "      <td>9.965153</td>\n",
       "      <td>-19.461447</td>\n",
       "      <td>-29.345492</td>\n",
       "      <td>17.858166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16516</th>\n",
       "      <td>-12.544017</td>\n",
       "      <td>10.544052</td>\n",
       "      <td>-1.430470</td>\n",
       "      <td>-13.858827</td>\n",
       "      <td>-5.916324</td>\n",
       "      <td>19.975354</td>\n",
       "      <td>-22.594095</td>\n",
       "      <td>21.086396</td>\n",
       "      <td>10.137654</td>\n",
       "      <td>-252.390078</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.095608</td>\n",
       "      <td>-19.448360</td>\n",
       "      <td>-1.474634</td>\n",
       "      <td>2.860775</td>\n",
       "      <td>10.840618</td>\n",
       "      <td>-3.903696</td>\n",
       "      <td>6.535987</td>\n",
       "      <td>-40.892090</td>\n",
       "      <td>-12.257640</td>\n",
       "      <td>17.898517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6601</th>\n",
       "      <td>-12.871053</td>\n",
       "      <td>12.910406</td>\n",
       "      <td>8.102991</td>\n",
       "      <td>-20.818296</td>\n",
       "      <td>5.416308</td>\n",
       "      <td>15.687601</td>\n",
       "      <td>-12.962415</td>\n",
       "      <td>14.327832</td>\n",
       "      <td>-0.332546</td>\n",
       "      <td>-221.724132</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.881783</td>\n",
       "      <td>-17.293569</td>\n",
       "      <td>-6.810033</td>\n",
       "      <td>9.925469</td>\n",
       "      <td>5.280512</td>\n",
       "      <td>11.167089</td>\n",
       "      <td>-4.658772</td>\n",
       "      <td>-32.100915</td>\n",
       "      <td>-16.817238</td>\n",
       "      <td>11.428053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12748</th>\n",
       "      <td>-23.569871</td>\n",
       "      <td>13.846242</td>\n",
       "      <td>-3.360931</td>\n",
       "      <td>-21.079367</td>\n",
       "      <td>3.227504</td>\n",
       "      <td>1.905320</td>\n",
       "      <td>-16.438037</td>\n",
       "      <td>10.130042</td>\n",
       "      <td>8.149030</td>\n",
       "      <td>-260.025188</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.299092</td>\n",
       "      <td>-29.609689</td>\n",
       "      <td>2.283834</td>\n",
       "      <td>11.254971</td>\n",
       "      <td>13.643181</td>\n",
       "      <td>12.224009</td>\n",
       "      <td>11.018311</td>\n",
       "      <td>-32.791095</td>\n",
       "      <td>-18.693083</td>\n",
       "      <td>14.956568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24699</th>\n",
       "      <td>-9.144350</td>\n",
       "      <td>9.607980</td>\n",
       "      <td>2.121521</td>\n",
       "      <td>-15.069610</td>\n",
       "      <td>-8.382971</td>\n",
       "      <td>7.164528</td>\n",
       "      <td>-6.767282</td>\n",
       "      <td>7.572684</td>\n",
       "      <td>5.581232</td>\n",
       "      <td>-204.353230</td>\n",
       "      <td>...</td>\n",
       "      <td>8.043900</td>\n",
       "      <td>-21.325058</td>\n",
       "      <td>-3.090704</td>\n",
       "      <td>15.283707</td>\n",
       "      <td>5.849677</td>\n",
       "      <td>0.908872</td>\n",
       "      <td>3.596308</td>\n",
       "      <td>-24.674798</td>\n",
       "      <td>-12.437727</td>\n",
       "      <td>9.718276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5181</th>\n",
       "      <td>-24.194759</td>\n",
       "      <td>27.939618</td>\n",
       "      <td>-1.058265</td>\n",
       "      <td>-35.578429</td>\n",
       "      <td>-10.567310</td>\n",
       "      <td>18.402240</td>\n",
       "      <td>-20.863763</td>\n",
       "      <td>3.774452</td>\n",
       "      <td>16.134511</td>\n",
       "      <td>-385.110479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.234348</td>\n",
       "      <td>-47.239371</td>\n",
       "      <td>-1.472601</td>\n",
       "      <td>15.595833</td>\n",
       "      <td>24.162865</td>\n",
       "      <td>9.340959</td>\n",
       "      <td>4.714287</td>\n",
       "      <td>-32.066144</td>\n",
       "      <td>-16.091305</td>\n",
       "      <td>12.274174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8565</th>\n",
       "      <td>-9.069258</td>\n",
       "      <td>3.693763</td>\n",
       "      <td>0.455075</td>\n",
       "      <td>-7.859676</td>\n",
       "      <td>-2.278375</td>\n",
       "      <td>4.961231</td>\n",
       "      <td>-8.113900</td>\n",
       "      <td>1.919085</td>\n",
       "      <td>3.415568</td>\n",
       "      <td>-138.084420</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.276042</td>\n",
       "      <td>-13.501403</td>\n",
       "      <td>0.088573</td>\n",
       "      <td>1.557029</td>\n",
       "      <td>6.658139</td>\n",
       "      <td>7.282298</td>\n",
       "      <td>4.500297</td>\n",
       "      <td>-19.944578</td>\n",
       "      <td>-3.583152</td>\n",
       "      <td>5.021738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3321</th>\n",
       "      <td>-18.585797</td>\n",
       "      <td>15.365771</td>\n",
       "      <td>-7.309255</td>\n",
       "      <td>-11.928405</td>\n",
       "      <td>-1.968230</td>\n",
       "      <td>1.662107</td>\n",
       "      <td>-9.489773</td>\n",
       "      <td>0.244664</td>\n",
       "      <td>11.206425</td>\n",
       "      <td>-267.478540</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.734781</td>\n",
       "      <td>-27.213106</td>\n",
       "      <td>9.585713</td>\n",
       "      <td>22.529589</td>\n",
       "      <td>20.763729</td>\n",
       "      <td>5.118833</td>\n",
       "      <td>8.761155</td>\n",
       "      <td>-29.316355</td>\n",
       "      <td>-14.662856</td>\n",
       "      <td>17.806559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13422</th>\n",
       "      <td>-15.297209</td>\n",
       "      <td>11.617117</td>\n",
       "      <td>0.076409</td>\n",
       "      <td>-21.589515</td>\n",
       "      <td>-1.091989</td>\n",
       "      <td>10.053348</td>\n",
       "      <td>-15.843731</td>\n",
       "      <td>7.407363</td>\n",
       "      <td>14.253900</td>\n",
       "      <td>-201.984426</td>\n",
       "      <td>...</td>\n",
       "      <td>2.497287</td>\n",
       "      <td>-24.078114</td>\n",
       "      <td>-7.579529</td>\n",
       "      <td>12.532833</td>\n",
       "      <td>9.982068</td>\n",
       "      <td>16.052828</td>\n",
       "      <td>1.022759</td>\n",
       "      <td>-23.391173</td>\n",
       "      <td>-15.114188</td>\n",
       "      <td>7.392749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21605</th>\n",
       "      <td>-27.517174</td>\n",
       "      <td>13.557439</td>\n",
       "      <td>-12.332927</td>\n",
       "      <td>-26.711862</td>\n",
       "      <td>-11.689412</td>\n",
       "      <td>19.898192</td>\n",
       "      <td>-13.598514</td>\n",
       "      <td>35.608316</td>\n",
       "      <td>5.167653</td>\n",
       "      <td>-377.590617</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.374094</td>\n",
       "      <td>-41.327188</td>\n",
       "      <td>-1.080533</td>\n",
       "      <td>8.976866</td>\n",
       "      <td>35.863722</td>\n",
       "      <td>3.158020</td>\n",
       "      <td>12.198753</td>\n",
       "      <td>-64.625780</td>\n",
       "      <td>-13.168070</td>\n",
       "      <td>21.869074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4688 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2           3          4          5    \\\n",
       "1955   -2.709607   0.823322  -0.976731   -3.942001   2.466419   4.572990   \n",
       "7410  -23.788113  18.849995  -5.478467  -28.850559  -2.977987   0.345442   \n",
       "23895 -35.850669  15.220555   1.413695  -44.379361  -7.873908  31.699543   \n",
       "10769 -10.986825  10.719739   3.078759  -13.467352  -1.792570   5.015190   \n",
       "11934 -13.702834  19.802053  -4.625199  -19.982145  -6.132876   4.284878   \n",
       "20759 -22.053144  17.074883  -8.434952  -23.331036  -2.651745  11.588363   \n",
       "21388 -42.377950  32.686152  -2.381558  -64.923632 -20.516477  30.028512   \n",
       "13678  -7.541792   6.129093  -0.594800   -6.274873   1.609949  -0.213843   \n",
       "24133 -14.680323   5.433444  -7.738582  -14.968245  -1.674337   6.351103   \n",
       "4114   -5.544905   6.907739  15.471430  -13.956575   2.924074   3.308911   \n",
       "17178 -17.635152  15.024632  -5.425530  -11.174331  -3.894342   8.941169   \n",
       "24040 -34.137248  11.821856 -10.746266  -39.348166 -11.998774  20.910849   \n",
       "6956  -91.516994  52.274754 -52.616213 -114.367492 -61.598924  34.118924   \n",
       "20657 -61.914712  42.696366   0.541368  -50.407364 -23.787768  35.639725   \n",
       "958   -62.818643  38.271435   4.865248 -116.327583 -25.147169  19.254611   \n",
       "9722  -10.554987  18.293404   3.678934  -11.550363  -1.442203  10.294282   \n",
       "8550  -23.916559   4.901791 -12.783871  -25.181798 -11.468971   6.784252   \n",
       "6272  -10.524945   2.056857  -1.750165  -16.521204  -2.468580  13.028403   \n",
       "19673 -20.279389  16.537558  -7.966284  -46.427341  -7.548783  22.509497   \n",
       "7453  -12.497348  -2.265612   8.483896  -33.964158 -19.962167  39.119092   \n",
       "324    -8.694349   7.204436  -1.723532  -15.276075  -3.207695   8.371777   \n",
       "21798 -26.486942  12.155884  -9.232964  -22.039451  -1.888616   4.156054   \n",
       "15278 -69.856524  23.712133  -3.634923 -108.757975 -31.141270  54.226308   \n",
       "19037 -23.636054  11.513394  -4.329027  -28.124834  -6.147302  11.882611   \n",
       "3441  -56.276520  24.709320  16.943392  -94.586030 -40.146525  47.382002   \n",
       "22749 -29.855294   7.149188  -4.481483  -51.091488 -16.732281  14.174086   \n",
       "24147  -3.989927   4.686236  -0.501458   -5.200674  -2.823350  -0.130515   \n",
       "3921  -11.576901   6.835486   0.667105  -13.734924  -2.997944  -1.417964   \n",
       "13256 -13.422250  15.967827   0.541279  -20.880829  -9.052532   7.523625   \n",
       "16435 -29.162212  25.702320   9.399868  -28.383447 -18.962298  42.109186   \n",
       "...          ...        ...        ...         ...        ...        ...   \n",
       "18044 -17.299292  13.569815 -12.008714  -28.618839 -17.119542  12.354782   \n",
       "19063 -50.629381  25.236919   0.713171  -31.733271  -5.357775  -1.075426   \n",
       "14135  -5.936827   2.513216  -6.172071  -14.840147  -5.526911   3.915300   \n",
       "9001  -76.292290  37.614296   9.673225  -93.375728  -4.186533  25.551793   \n",
       "3732  -18.060386   4.617568   0.126421   -6.254873   2.915992   7.915182   \n",
       "22233 -20.753592   9.187054   3.264305  -21.347526 -14.310382  12.807159   \n",
       "3396  -18.642317   9.877814  -1.224489  -19.441003  -3.979040  10.278570   \n",
       "21321 -20.718045  19.945724 -11.095311  -22.968617 -10.075198  12.838906   \n",
       "7871  -16.732786   7.242041   6.477521  -32.065223  -4.657519  30.616734   \n",
       "2505  -28.814689  17.622726   1.494340  -48.205649  -4.250267   9.618395   \n",
       "20359 -15.456523  11.386994  -1.243168   -5.179543  -7.692556   6.225951   \n",
       "5009  -21.368167  12.139053  -0.364689  -27.377376  -4.902337  14.080169   \n",
       "745    -8.618833   4.165064  -2.845305   -6.910155  -1.965333   6.724853   \n",
       "14416 -47.537979  30.618257 -10.202258  -38.629245 -16.185994  23.227377   \n",
       "23051  -9.079233   4.073491   0.027731  -13.619125  -0.521916   7.202331   \n",
       "22317 -49.399930  46.003318  -5.172739  -53.547132 -14.748130  23.787145   \n",
       "7926   -5.627715   7.810474  -2.228828   -8.777959  -2.673588   6.679962   \n",
       "17912 -16.102884  19.168871  -5.165883  -24.595761 -10.462374  16.940099   \n",
       "16486 -16.825651  12.756521  -4.191933  -21.748686  -6.046085  14.019968   \n",
       "9090  -21.911924  15.211503   1.587957  -19.323233   0.228148   5.515551   \n",
       "3052  -21.578017  21.918035  -0.966661  -35.850762  -7.133798  12.219780   \n",
       "16516 -12.544017  10.544052  -1.430470  -13.858827  -5.916324  19.975354   \n",
       "6601  -12.871053  12.910406   8.102991  -20.818296   5.416308  15.687601   \n",
       "12748 -23.569871  13.846242  -3.360931  -21.079367   3.227504   1.905320   \n",
       "24699  -9.144350   9.607980   2.121521  -15.069610  -8.382971   7.164528   \n",
       "5181  -24.194759  27.939618  -1.058265  -35.578429 -10.567310  18.402240   \n",
       "8565   -9.069258   3.693763   0.455075   -7.859676  -2.278375   4.961231   \n",
       "3321  -18.585797  15.365771  -7.309255  -11.928405  -1.968230   1.662107   \n",
       "13422 -15.297209  11.617117   0.076409  -21.589515  -1.091989  10.053348   \n",
       "21605 -27.517174  13.557439 -12.332927  -26.711862 -11.689412  19.898192   \n",
       "\n",
       "              6          7          8            9       ...            290  \\\n",
       "1955    -5.011948  -0.157489   0.726081   -54.957779     ...      -0.171843   \n",
       "7410   -12.294386  16.698851  10.019337  -342.338497     ...      -2.129589   \n",
       "23895  -34.713713  37.065645   9.976540  -458.033963     ...     -20.993391   \n",
       "10769  -12.948430   4.535959   2.971679  -132.727480     ...      -5.892459   \n",
       "11934  -10.975221   5.712880  11.948668  -238.715968     ...      -1.195186   \n",
       "20759  -14.444987  11.108250  15.252464  -275.128910     ...      -5.502355   \n",
       "21388  -54.464985  39.650377  30.458246  -720.329316     ...      -5.827593   \n",
       "13678   -6.886704   6.109926   7.357906  -106.628828     ...      -0.370001   \n",
       "24133  -15.944126   6.924268   0.089956  -159.389650     ...      -6.140383   \n",
       "4114   -14.841988  29.147561   9.979552  -239.164830     ...      -1.826192   \n",
       "17178  -10.579772   2.531203  11.211590  -188.086081     ...       0.350070   \n",
       "24040  -25.526554  12.494822   9.639688  -431.715341     ...      -9.089056   \n",
       "6956  -110.584593  32.890318 -14.976800 -1580.101400     ...     -58.295342   \n",
       "20657  -40.469919  35.103623  16.111696  -685.626899     ...     -20.008726   \n",
       "958    -68.374034  75.122895  17.877368  -949.093840     ...     -45.790695   \n",
       "9722    -5.235103   8.549960  10.290837  -170.107648     ...      -6.824146   \n",
       "8550   -24.808197   5.720500  13.941881  -296.114011     ...      -7.341810   \n",
       "6272   -15.045720  26.526282  -6.916416  -211.754427     ...     -13.750727   \n",
       "19673  -21.977801  31.516395   0.531357  -332.587640     ...     -18.127918   \n",
       "7453   -36.806663  73.536870 -26.690043  -318.526219     ...     -43.859190   \n",
       "324    -11.845626   7.413905   0.645721  -155.346151     ...      -7.485414   \n",
       "21798  -17.112060   9.139269   4.758725  -256.882626     ...      -7.007603   \n",
       "15278  -81.540717  81.580393  16.293521 -1103.865205     ...     -20.284524   \n",
       "19037  -16.675913   7.222268  14.560933  -337.465830     ...      -2.643878   \n",
       "3441   -79.260050  72.378042   2.135351  -801.801134     ...     -63.209418   \n",
       "22749  -44.271995  28.421128  -5.544694  -371.868273     ...     -30.120596   \n",
       "24147   -2.690244   2.764236   2.858595   -81.611090     ...       0.556223   \n",
       "3921   -15.989688   3.036850   5.678731  -152.090173     ...       0.677100   \n",
       "13256  -26.246926  14.467357   6.543092  -275.658067     ...      -4.721702   \n",
       "16435  -41.805056  45.812356  -1.561793  -407.691049     ...     -28.159647   \n",
       "...           ...        ...        ...          ...     ...            ...   \n",
       "18044  -25.226864  31.875416   8.878036  -370.907944     ...      -3.961557   \n",
       "19063  -32.619189   0.096529  14.763721  -439.019189     ...      -5.774474   \n",
       "14135   -9.372801  10.879915   2.238434  -131.630420     ...      -0.644825   \n",
       "9001   -89.199038  43.561187  41.967603 -1007.815200     ...     -40.339309   \n",
       "3732    -5.539584   0.132867   6.709639  -191.267771     ...       3.333621   \n",
       "22233  -30.323598  24.669629  17.720187  -369.979979     ...     -17.267588   \n",
       "3396   -18.606195  12.696260  12.786393  -251.493070     ...      -5.385506   \n",
       "21321  -13.025911   5.571900  10.330121  -287.525323     ...       5.803207   \n",
       "7871   -25.281264  36.540971  -4.710188  -244.032224     ...     -23.357606   \n",
       "2505   -22.061856  12.365642   5.403279  -389.890051     ...     -17.793875   \n",
       "20359   -6.352669  16.384140   5.040921  -191.946817     ...       0.863140   \n",
       "5009   -13.126132  18.236577   0.295992  -318.367954     ...      -4.904352   \n",
       "745     -3.901254   2.416759   5.996268   -95.375070     ...       1.085352   \n",
       "14416  -37.912715  37.926766  10.131252  -552.731931     ...     -46.702003   \n",
       "23051   -5.144737   3.395290   7.143688  -144.826991     ...      -6.568975   \n",
       "22317  -53.965387  13.670101  26.819177  -662.287211     ...     -22.126225   \n",
       "7926    -6.190254   8.296447  -0.297202  -125.375480     ...      -2.974094   \n",
       "17912  -20.270281  17.659166   8.713944  -269.063245     ...       3.587868   \n",
       "16486  -31.388806   8.813285   5.688096  -265.741032     ...      -2.553938   \n",
       "9090    -9.824107  13.257933   4.186626  -177.198294     ...      -0.980449   \n",
       "3052   -23.578330   2.372196   8.496632  -298.004025     ...     -11.486910   \n",
       "16516  -22.594095  21.086396  10.137654  -252.390078     ...      -6.095608   \n",
       "6601   -12.962415  14.327832  -0.332546  -221.724132     ...      -3.881783   \n",
       "12748  -16.438037  10.130042   8.149030  -260.025188     ...      -1.299092   \n",
       "24699   -6.767282   7.572684   5.581232  -204.353230     ...       8.043900   \n",
       "5181   -20.863763   3.774452  16.134511  -385.110479     ...       0.234348   \n",
       "8565    -8.113900   1.919085   3.415568  -138.084420     ...      -0.276042   \n",
       "3321    -9.489773   0.244664  11.206425  -267.478540     ...      -1.734781   \n",
       "13422  -15.843731   7.407363  14.253900  -201.984426     ...       2.497287   \n",
       "21605  -13.598514  35.608316   5.167653  -377.590617     ...     -12.374094   \n",
       "\n",
       "              291        292        293         294         295        296  \\\n",
       "1955    -8.035381   1.083654   5.119356    1.752086    6.575515   2.586730   \n",
       "7410   -38.117047  -6.299583  23.221125   23.576995   22.822184   5.582503   \n",
       "23895  -43.211417 -20.704074   5.212338   36.088696   21.677928  11.571817   \n",
       "10769  -17.914209  -9.408483   6.720133    9.273998    5.329989   4.608194   \n",
       "11934  -27.646590   0.126074  14.156489    7.526001   19.592931   4.628328   \n",
       "20759  -34.103729   3.371719  10.743932    7.673473   -0.151231  -3.961266   \n",
       "21388  -77.953762 -18.054393  29.590047   46.779906    5.194965  16.711095   \n",
       "13678  -13.300510   1.454698   3.520301    8.904697    8.872277  -0.344592   \n",
       "24133  -15.928990  -3.467835   2.890319   12.100413    0.381100   8.298843   \n",
       "4114   -31.208858   1.823677   5.789113    5.687960   35.277971  -2.175263   \n",
       "17178  -27.712951   1.824640   7.358192   12.442729   -0.038419   6.188585   \n",
       "24040  -45.386066 -11.704773   6.196250   35.751139   11.901527  25.515976   \n",
       "6956  -162.102638 -14.835520  34.333048   78.321964   43.528371  89.786181   \n",
       "20657  -75.858099  -9.509220  17.381242   68.933670    6.970753  30.325388   \n",
       "958    -88.607616   9.718939   6.280769   72.204049  109.209820   7.305168   \n",
       "9722   -19.358070  -0.425231  10.595602    4.428870    0.338749   0.788579   \n",
       "8550   -32.060028  -2.630740   1.894903   21.141129   15.967242  15.508029   \n",
       "6272   -12.531376   2.316930  -2.453000   13.557028   -1.573377  11.605980   \n",
       "19673  -33.764294   0.834980  -0.211265   35.433119    7.023923  17.636970   \n",
       "7453   -10.297078   0.046708 -25.237965   41.006575  -26.674120  33.880869   \n",
       "324    -15.926581  -8.888863   9.101328   11.363404   -8.706879   1.195394   \n",
       "21798  -20.742316   7.513851   5.681077   17.617936    7.925859  11.999182   \n",
       "15278 -104.421857   3.525290  50.042522   73.953080   51.959726  27.216909   \n",
       "19037  -38.617855   3.077293   9.424583   16.966189   -0.971775  -2.018547   \n",
       "3441   -65.137210  -5.831389 -15.684218   46.809611   76.860060  49.717672   \n",
       "22749  -29.834163   4.205755   4.181478   38.896434    6.472579  15.531713   \n",
       "24147   -7.419544  -2.115418   8.344134    0.377605    2.453584   3.366217   \n",
       "3921   -20.877940  -1.113330   4.978338    0.390237   18.225844   3.766373   \n",
       "13256  -33.711442  -5.231274  10.041005    6.037990   22.913020  10.326905   \n",
       "16435  -32.248900 -10.401135   6.772282   28.876440   -6.342793  19.486085   \n",
       "...           ...        ...        ...         ...         ...        ...   \n",
       "18044  -48.946702   5.751652  14.417736   28.186744   25.170991  15.249298   \n",
       "19063  -38.748451 -17.961948  13.771792   33.810693   41.866615  14.847798   \n",
       "14135  -15.986249  -1.828669  -2.962533   12.704037   -0.981785   0.622333   \n",
       "9001   -96.080122 -24.307046  50.248572  102.028192  115.806666  26.248288   \n",
       "3732   -13.579564   3.448079   5.873173   10.429939   -2.004042  -2.281200   \n",
       "22233  -24.848232   0.048142  17.914561   25.488767   16.688674   8.901095   \n",
       "3396   -26.196744  -6.033366  10.388296   24.887487    1.054616   8.776082   \n",
       "21321  -36.551702  -0.903895   7.905721   10.756023   -0.496114   0.938126   \n",
       "7871   -27.386472  -6.588975  -6.301148   15.004780   16.298910  12.415117   \n",
       "2505   -36.448217   6.202009  20.062270   32.877512   30.933061   3.028407   \n",
       "20359  -17.304777  -0.833943  11.707282   10.782700   -2.588566   2.772143   \n",
       "5009   -18.965574   2.032536   5.890766   23.480982   -6.827021  14.100212   \n",
       "745    -13.592272   0.253477   4.347166    6.478355   -6.569979  -1.992117   \n",
       "14416  -57.484203  -7.913888   1.990643   40.062098   59.012575  11.033079   \n",
       "23051  -19.848392   0.379204   9.465365   10.500387    8.141598  -5.470872   \n",
       "22317  -68.889265 -26.650699  38.327870   33.963182   39.108426  20.522845   \n",
       "7926    -8.249290  -0.878574   5.394216    9.527484   -3.788370   3.124698   \n",
       "17912  -30.670710  -1.007824   1.672720    8.959662    7.168338   8.476560   \n",
       "16486  -16.927210   2.921756   7.703459    8.289859   20.423596  -1.545213   \n",
       "9090   -17.564874  -2.102587  20.192307   13.573605   24.677526   5.552973   \n",
       "3052   -32.086949  -9.276874  16.396562   16.829784   29.680861   9.965153   \n",
       "16516  -19.448360  -1.474634   2.860775   10.840618   -3.903696   6.535987   \n",
       "6601   -17.293569  -6.810033   9.925469    5.280512   11.167089  -4.658772   \n",
       "12748  -29.609689   2.283834  11.254971   13.643181   12.224009  11.018311   \n",
       "24699  -21.325058  -3.090704  15.283707    5.849677    0.908872   3.596308   \n",
       "5181   -47.239371  -1.472601  15.595833   24.162865    9.340959   4.714287   \n",
       "8565   -13.501403   0.088573   1.557029    6.658139    7.282298   4.500297   \n",
       "3321   -27.213106   9.585713  22.529589   20.763729    5.118833   8.761155   \n",
       "13422  -24.078114  -7.579529  12.532833    9.982068   16.052828   1.022759   \n",
       "21605  -41.327188  -1.080533   8.976866   35.863722    3.158020  12.198753   \n",
       "\n",
       "              297         298         299  \n",
       "1955    -8.790066   -1.765028    2.371102  \n",
       "7410   -37.512159  -21.140500   13.774973  \n",
       "23895  -69.118344  -21.848598   42.669947  \n",
       "10769   -9.265466  -12.741729   11.088184  \n",
       "11934  -28.717259  -19.414686    0.740996  \n",
       "20759  -26.844456   -8.766967    3.362617  \n",
       "21388  -75.315097  -41.500587   44.811492  \n",
       "13678  -13.677129   -7.637608    4.256431  \n",
       "24133  -25.018817   -7.148896   18.951194  \n",
       "4114   -33.342825  -22.623583    9.120666  \n",
       "17178  -27.028565  -11.562124    5.805018  \n",
       "24040  -49.792218  -30.145137   32.132744  \n",
       "6956  -183.885912 -103.693346  119.396108  \n",
       "20657  -88.020388  -45.870046   37.578448  \n",
       "958   -137.083864  -61.806656   39.806637  \n",
       "9722   -18.276870  -16.499237   -1.503728  \n",
       "8550   -29.628169  -16.566734   16.177913  \n",
       "6272   -36.639773   -3.663009   13.151600  \n",
       "19673  -41.483486  -14.070973   18.460387  \n",
       "7453   -74.359070    2.091837   31.534452  \n",
       "324    -19.194101   -3.148790   15.858376  \n",
       "21798  -31.533458   -8.043964   13.173580  \n",
       "15278 -145.854586  -61.183026   72.546931  \n",
       "19037  -29.752288  -20.689244   22.809218  \n",
       "3441  -100.850171  -65.155665   46.828688  \n",
       "22749  -34.791097  -21.409873   24.415801  \n",
       "24147   -5.990797   -4.235276    5.533039  \n",
       "3921   -11.577898  -15.167913    8.656410  \n",
       "13256  -35.332736  -24.098249   22.227937  \n",
       "16435  -77.279025    6.872116   35.621059  \n",
       "...           ...         ...         ...  \n",
       "18044  -45.859094  -19.161795   26.066287  \n",
       "19063  -42.521002  -37.862705   29.554397  \n",
       "14135  -17.848688   -2.275290    5.411064  \n",
       "9001  -102.357000  -76.521082   40.759889  \n",
       "3732   -25.085898   -9.946902   15.421309  \n",
       "22233  -54.772730  -18.370131   18.016334  \n",
       "3396   -29.509978   -5.587186   19.945198  \n",
       "21321  -36.570446  -11.452912   11.695590  \n",
       "7871   -27.285070  -18.151567   27.087421  \n",
       "2505   -34.323680  -23.117758   27.999421  \n",
       "20359  -26.851832   -9.590791   11.865068  \n",
       "5009   -47.077942  -18.528949   26.072687  \n",
       "745    -10.146030   -1.859755    6.855932  \n",
       "14416  -85.028549  -37.595807   30.083490  \n",
       "23051   -9.424971   -8.297707   -1.807881  \n",
       "22317  -38.119001  -56.627183   35.492129  \n",
       "7926   -17.971417   -4.121198   10.788305  \n",
       "17912  -26.182089  -10.627233   12.679737  \n",
       "16486  -33.463613  -21.657549   12.790451  \n",
       "9090   -20.443322  -20.643177   10.038302  \n",
       "3052   -19.461447  -29.345492   17.858166  \n",
       "16516  -40.892090  -12.257640   17.898517  \n",
       "6601   -32.100915  -16.817238   11.428053  \n",
       "12748  -32.791095  -18.693083   14.956568  \n",
       "24699  -24.674798  -12.437727    9.718276  \n",
       "5181   -32.066144  -16.091305   12.274174  \n",
       "8565   -19.944578   -3.583152    5.021738  \n",
       "3321   -29.316355  -14.662856   17.806559  \n",
       "13422  -23.391173  -15.114188    7.392749  \n",
       "21605  -64.625780  -13.168070   21.869074  \n",
       "\n",
       "[4688 rows x 300 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweaking\n",
    "\n",
    "Now we're trying with glove_score where we use adjectives and their previous adverbs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n"
     ]
    }
   ],
   "source": [
    "scores_df = pd.DataFrame()\n",
    "for i, row in reviews_df.iterrows():\n",
    "    s_df = glove_score(reviews_df.iloc[i,0], adj=True)\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    scores_df = scores_df.append(s_df,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(reviews_df.iloc[:,0], reviews_df.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9365"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3135"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = scores_df.iloc[y_train.index]\n",
    "X_test = scores_df.iloc[y_test.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7561</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23065</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18234</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23353</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14832</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7385</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14252</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9903</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3    4    5    6    7    8    9   ...   290  291  292  \\\n",
       "4996   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
       "7561   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
       "23065  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
       "18234  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
       "23353  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
       "14832  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
       "897    NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
       "7385   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
       "14252  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
       "9903   NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
       "\n",
       "       293  294  295  296  297  298  299  \n",
       "4996   NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "7561   NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "23065  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "18234  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "23353  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "14832  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "897    NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "7385   NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "14252  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "9903   NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[10 rows x 300 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[X_train.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.dropna()\n",
    "X_val = X_val.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = y_train[X_train.index]\n",
    "y_val = y_val[X_val.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7021"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3135"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jared\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LeakyReLU, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.05, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(300, input_dim=300))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1000))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(500))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(250))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(100))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(50))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14052 samples, validate on 4687 samples\n",
      "Epoch 1/10000\n",
      "14052/14052 [==============================] - 11s 806us/step - loss: 0.6109 - binary_crossentropy: 0.6109 - acc: 0.6731 - val_loss: 0.4963 - val_binary_crossentropy: 0.4963 - val_acc: 0.7732\n",
      "Epoch 2/10000\n",
      "14052/14052 [==============================] - 7s 465us/step - loss: 0.5196 - binary_crossentropy: 0.5196 - acc: 0.7528 - val_loss: 0.4755 - val_binary_crossentropy: 0.4755 - val_acc: 0.7796\n",
      "Epoch 3/10000\n",
      "14052/14052 [==============================] - 6s 459us/step - loss: 0.4975 - binary_crossentropy: 0.4975 - acc: 0.7679 - val_loss: 0.4536 - val_binary_crossentropy: 0.4536 - val_acc: 0.7881\n",
      "Epoch 4/10000\n",
      "14052/14052 [==============================] - 6s 461us/step - loss: 0.4896 - binary_crossentropy: 0.4896 - acc: 0.7690 - val_loss: 0.4774 - val_binary_crossentropy: 0.4774 - val_acc: 0.7800\n",
      "Epoch 5/10000\n",
      "14052/14052 [==============================] - 6s 462us/step - loss: 0.4793 - binary_crossentropy: 0.4793 - acc: 0.7781 - val_loss: 0.4511 - val_binary_crossentropy: 0.4511 - val_acc: 0.7924\n",
      "Epoch 6/10000\n",
      "14052/14052 [==============================] - 7s 465us/step - loss: 0.4709 - binary_crossentropy: 0.4709 - acc: 0.7821 - val_loss: 0.4536 - val_binary_crossentropy: 0.4536 - val_acc: 0.7958\n",
      "Epoch 7/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.4681 - binary_crossentropy: 0.4681 - acc: 0.7843 - val_loss: 0.4524 - val_binary_crossentropy: 0.4524 - val_acc: 0.7930\n",
      "Epoch 8/10000\n",
      "14052/14052 [==============================] - 7s 469us/step - loss: 0.4658 - binary_crossentropy: 0.4658 - acc: 0.7874 - val_loss: 0.4483 - val_binary_crossentropy: 0.4483 - val_acc: 0.7939\n",
      "Epoch 9/10000\n",
      "14052/14052 [==============================] - 7s 466us/step - loss: 0.4650 - binary_crossentropy: 0.4650 - acc: 0.7859 - val_loss: 0.4420 - val_binary_crossentropy: 0.4420 - val_acc: 0.7958\n",
      "Epoch 10/10000\n",
      "14052/14052 [==============================] - 7s 465us/step - loss: 0.4637 - binary_crossentropy: 0.4637 - acc: 0.7869 - val_loss: 0.4533 - val_binary_crossentropy: 0.4533 - val_acc: 0.7892\n",
      "Epoch 11/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.4615 - binary_crossentropy: 0.4615 - acc: 0.7862 - val_loss: 0.4480 - val_binary_crossentropy: 0.4480 - val_acc: 0.7962\n",
      "Epoch 12/10000\n",
      "14052/14052 [==============================] - 7s 469us/step - loss: 0.4602 - binary_crossentropy: 0.4602 - acc: 0.7896 - val_loss: 0.4453 - val_binary_crossentropy: 0.4453 - val_acc: 0.7984\n",
      "Epoch 13/10000\n",
      "14052/14052 [==============================] - 7s 467us/step - loss: 0.4571 - binary_crossentropy: 0.4571 - acc: 0.7884 - val_loss: 0.4419 - val_binary_crossentropy: 0.4419 - val_acc: 0.8005\n",
      "Epoch 14/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.4500 - binary_crossentropy: 0.4500 - acc: 0.7923 - val_loss: 0.4391 - val_binary_crossentropy: 0.4391 - val_acc: 0.7977\n",
      "Epoch 15/10000\n",
      "14052/14052 [==============================] - 7s 468us/step - loss: 0.4547 - binary_crossentropy: 0.4547 - acc: 0.7936 - val_loss: 0.4476 - val_binary_crossentropy: 0.4476 - val_acc: 0.7916\n",
      "Epoch 16/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.4527 - binary_crossentropy: 0.4527 - acc: 0.7935 - val_loss: 0.4416 - val_binary_crossentropy: 0.4416 - val_acc: 0.7975\n",
      "Epoch 17/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.4542 - binary_crossentropy: 0.4542 - acc: 0.7929 - val_loss: 0.4496 - val_binary_crossentropy: 0.4496 - val_acc: 0.8016\n",
      "Epoch 18/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.4466 - binary_crossentropy: 0.4466 - acc: 0.7959 - val_loss: 0.4488 - val_binary_crossentropy: 0.4488 - val_acc: 0.7896\n",
      "Epoch 19/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.4445 - binary_crossentropy: 0.4445 - acc: 0.7941 - val_loss: 0.4353 - val_binary_crossentropy: 0.4353 - val_acc: 0.7990\n",
      "Epoch 20/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.4494 - binary_crossentropy: 0.4494 - acc: 0.7949 - val_loss: 0.4393 - val_binary_crossentropy: 0.4393 - val_acc: 0.7950\n",
      "Epoch 21/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.4429 - binary_crossentropy: 0.4429 - acc: 0.8001 - val_loss: 0.4371 - val_binary_crossentropy: 0.4371 - val_acc: 0.7973\n",
      "Epoch 22/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.4491 - binary_crossentropy: 0.4491 - acc: 0.7918 - val_loss: 0.4513 - val_binary_crossentropy: 0.4513 - val_acc: 0.8016\n",
      "Epoch 23/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.4471 - binary_crossentropy: 0.4471 - acc: 0.7933 - val_loss: 0.4531 - val_binary_crossentropy: 0.4531 - val_acc: 0.8031\n",
      "Epoch 24/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.4431 - binary_crossentropy: 0.4431 - acc: 0.7969 - val_loss: 0.4477 - val_binary_crossentropy: 0.4477 - val_acc: 0.7937\n",
      "Epoch 25/10000\n",
      "14052/14052 [==============================] - 7s 478us/step - loss: 0.4397 - binary_crossentropy: 0.4397 - acc: 0.8012 - val_loss: 0.4375 - val_binary_crossentropy: 0.4375 - val_acc: 0.7980\n",
      "Epoch 26/10000\n",
      "14052/14052 [==============================] - 7s 476us/step - loss: 0.4429 - binary_crossentropy: 0.4429 - acc: 0.8025 - val_loss: 0.4428 - val_binary_crossentropy: 0.4428 - val_acc: 0.7886\n",
      "Epoch 27/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.4427 - binary_crossentropy: 0.4427 - acc: 0.7961 - val_loss: 0.4388 - val_binary_crossentropy: 0.4388 - val_acc: 0.7958\n",
      "Epoch 28/10000\n",
      "14052/14052 [==============================] - 7s 477us/step - loss: 0.4432 - binary_crossentropy: 0.4432 - acc: 0.7984 - val_loss: 0.4335 - val_binary_crossentropy: 0.4335 - val_acc: 0.8003\n",
      "Epoch 29/10000\n",
      "14052/14052 [==============================] - 7s 476us/step - loss: 0.4453 - binary_crossentropy: 0.4453 - acc: 0.7995 - val_loss: 0.4397 - val_binary_crossentropy: 0.4397 - val_acc: 0.8005\n",
      "Epoch 30/10000\n",
      "14052/14052 [==============================] - 7s 476us/step - loss: 0.4441 - binary_crossentropy: 0.4441 - acc: 0.8002 - val_loss: 0.4758 - val_binary_crossentropy: 0.4758 - val_acc: 0.7888\n",
      "Epoch 31/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.4406 - binary_crossentropy: 0.4406 - acc: 0.8032 - val_loss: 0.4370 - val_binary_crossentropy: 0.4370 - val_acc: 0.8033\n",
      "Epoch 32/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.4378 - binary_crossentropy: 0.4378 - acc: 0.8034 - val_loss: 0.4417 - val_binary_crossentropy: 0.4417 - val_acc: 0.7975\n",
      "Epoch 33/10000\n",
      "14052/14052 [==============================] - 7s 474us/step - loss: 0.4379 - binary_crossentropy: 0.4379 - acc: 0.8003 - val_loss: 0.4381 - val_binary_crossentropy: 0.4381 - val_acc: 0.7977\n",
      "Epoch 34/10000\n",
      "14052/14052 [==============================] - 7s 474us/step - loss: 0.4439 - binary_crossentropy: 0.4439 - acc: 0.7988 - val_loss: 0.4587 - val_binary_crossentropy: 0.4587 - val_acc: 0.7864\n",
      "Epoch 35/10000\n",
      "14052/14052 [==============================] - 7s 497us/step - loss: 0.4348 - binary_crossentropy: 0.4348 - acc: 0.8024 - val_loss: 0.4372 - val_binary_crossentropy: 0.4372 - val_acc: 0.7997\n",
      "Epoch 36/10000\n",
      "14052/14052 [==============================] - 7s 487us/step - loss: 0.4431 - binary_crossentropy: 0.4431 - acc: 0.7980 - val_loss: 0.4382 - val_binary_crossentropy: 0.4382 - val_acc: 0.7986\n",
      "Epoch 37/10000\n",
      "14052/14052 [==============================] - 7s 478us/step - loss: 0.4383 - binary_crossentropy: 0.4383 - acc: 0.8011 - val_loss: 0.4432 - val_binary_crossentropy: 0.4432 - val_acc: 0.7997\n",
      "Epoch 38/10000\n",
      "14052/14052 [==============================] - 7s 508us/step - loss: 0.4356 - binary_crossentropy: 0.4356 - acc: 0.8040 - val_loss: 0.4319 - val_binary_crossentropy: 0.4319 - val_acc: 0.8020\n",
      "Epoch 39/10000\n",
      "14052/14052 [==============================] - 7s 479us/step - loss: 0.4294 - binary_crossentropy: 0.4294 - acc: 0.8054 - val_loss: 0.4327 - val_binary_crossentropy: 0.4327 - val_acc: 0.8026\n",
      "Epoch 40/10000\n",
      "14052/14052 [==============================] - 6s 458us/step - loss: 0.4310 - binary_crossentropy: 0.4310 - acc: 0.8064 - val_loss: 0.4416 - val_binary_crossentropy: 0.4416 - val_acc: 0.7911\n",
      "Epoch 41/10000\n",
      "14052/14052 [==============================] - 6s 459us/step - loss: 0.4340 - binary_crossentropy: 0.4340 - acc: 0.8024 - val_loss: 0.4427 - val_binary_crossentropy: 0.4427 - val_acc: 0.7930\n",
      "Epoch 42/10000\n",
      "14052/14052 [==============================] - 6s 458us/step - loss: 0.4351 - binary_crossentropy: 0.4351 - acc: 0.8042 - val_loss: 0.4360 - val_binary_crossentropy: 0.4360 - val_acc: 0.7986\n",
      "Epoch 43/10000\n",
      "14052/14052 [==============================] - 7s 463us/step - loss: 0.4332 - binary_crossentropy: 0.4332 - acc: 0.8045 - val_loss: 0.4445 - val_binary_crossentropy: 0.4445 - val_acc: 0.8026\n",
      "Epoch 44/10000\n",
      "14052/14052 [==============================] - 7s 494us/step - loss: 0.4338 - binary_crossentropy: 0.4338 - acc: 0.8034 - val_loss: 0.4348 - val_binary_crossentropy: 0.4348 - val_acc: 0.8026\n",
      "Epoch 45/10000\n",
      "14052/14052 [==============================] - 6s 459us/step - loss: 0.4295 - binary_crossentropy: 0.4295 - acc: 0.8068 - val_loss: 0.4341 - val_binary_crossentropy: 0.4341 - val_acc: 0.7984\n",
      "Epoch 46/10000\n",
      "14052/14052 [==============================] - 6s 460us/step - loss: 0.4339 - binary_crossentropy: 0.4339 - acc: 0.8033 - val_loss: 0.4830 - val_binary_crossentropy: 0.4830 - val_acc: 0.7651\n",
      "Epoch 47/10000\n",
      "14052/14052 [==============================] - 7s 465us/step - loss: 0.4351 - binary_crossentropy: 0.4351 - acc: 0.8053 - val_loss: 0.4385 - val_binary_crossentropy: 0.4385 - val_acc: 0.8001\n",
      "Epoch 48/10000\n",
      "14052/14052 [==============================] - 6s 462us/step - loss: 0.4324 - binary_crossentropy: 0.4324 - acc: 0.8007 - val_loss: 0.4686 - val_binary_crossentropy: 0.4686 - val_acc: 0.7732\n",
      "Epoch 49/10000\n",
      "14052/14052 [==============================] - 7s 465us/step - loss: 0.4319 - binary_crossentropy: 0.4319 - acc: 0.8091 - val_loss: 0.4442 - val_binary_crossentropy: 0.4442 - val_acc: 0.7913\n",
      "Epoch 50/10000\n",
      "14052/14052 [==============================] - 7s 463us/step - loss: 0.4327 - binary_crossentropy: 0.4327 - acc: 0.8052 - val_loss: 0.4605 - val_binary_crossentropy: 0.4605 - val_acc: 0.7809\n",
      "Epoch 51/10000\n",
      "14052/14052 [==============================] - 7s 464us/step - loss: 0.4314 - binary_crossentropy: 0.4314 - acc: 0.8049 - val_loss: 0.4426 - val_binary_crossentropy: 0.4426 - val_acc: 0.7950\n",
      "Epoch 52/10000\n",
      "14052/14052 [==============================] - 7s 466us/step - loss: 0.4336 - binary_crossentropy: 0.4336 - acc: 0.8049 - val_loss: 0.4337 - val_binary_crossentropy: 0.4337 - val_acc: 0.7982\n",
      "Epoch 53/10000\n",
      "14052/14052 [==============================] - 7s 468us/step - loss: 0.4282 - binary_crossentropy: 0.4282 - acc: 0.8079 - val_loss: 0.4303 - val_binary_crossentropy: 0.4303 - val_acc: 0.8024\n",
      "Epoch 54/10000\n",
      "14052/14052 [==============================] - 7s 465us/step - loss: 0.4314 - binary_crossentropy: 0.4314 - acc: 0.8063 - val_loss: 0.4316 - val_binary_crossentropy: 0.4316 - val_acc: 0.8012\n",
      "Epoch 55/10000\n",
      "14052/14052 [==============================] - 7s 465us/step - loss: 0.4274 - binary_crossentropy: 0.4274 - acc: 0.8051 - val_loss: 0.4440 - val_binary_crossentropy: 0.4440 - val_acc: 0.7928\n",
      "Epoch 56/10000\n",
      "14052/14052 [==============================] - 7s 466us/step - loss: 0.4278 - binary_crossentropy: 0.4278 - acc: 0.8054 - val_loss: 0.4317 - val_binary_crossentropy: 0.4317 - val_acc: 0.7999\n",
      "Epoch 57/10000\n",
      "14052/14052 [==============================] - 7s 468us/step - loss: 0.4245 - binary_crossentropy: 0.4245 - acc: 0.8109 - val_loss: 0.4329 - val_binary_crossentropy: 0.4329 - val_acc: 0.8009\n",
      "Epoch 58/10000\n",
      "14052/14052 [==============================] - 7s 468us/step - loss: 0.4289 - binary_crossentropy: 0.4289 - acc: 0.8083 - val_loss: 0.4308 - val_binary_crossentropy: 0.4308 - val_acc: 0.8022\n",
      "Epoch 59/10000\n",
      "14052/14052 [==============================] - 7s 471us/step - loss: 0.4296 - binary_crossentropy: 0.4296 - acc: 0.8075 - val_loss: 0.4282 - val_binary_crossentropy: 0.4282 - val_acc: 0.8039\n",
      "Epoch 60/10000\n",
      "14052/14052 [==============================] - 7s 468us/step - loss: 0.4286 - binary_crossentropy: 0.4286 - acc: 0.8015 - val_loss: 0.4432 - val_binary_crossentropy: 0.4432 - val_acc: 0.8031\n",
      "Epoch 61/10000\n",
      "14052/14052 [==============================] - 7s 476us/step - loss: 0.4248 - binary_crossentropy: 0.4248 - acc: 0.8073 - val_loss: 0.4372 - val_binary_crossentropy: 0.4372 - val_acc: 0.7975\n",
      "Epoch 62/10000\n",
      "14052/14052 [==============================] - 7s 486us/step - loss: 0.4248 - binary_crossentropy: 0.4248 - acc: 0.8095 - val_loss: 0.4517 - val_binary_crossentropy: 0.4517 - val_acc: 0.7892\n",
      "Epoch 63/10000\n",
      "14052/14052 [==============================] - 7s 468us/step - loss: 0.4239 - binary_crossentropy: 0.4239 - acc: 0.8054 - val_loss: 0.4505 - val_binary_crossentropy: 0.4505 - val_acc: 0.7866\n",
      "Epoch 64/10000\n",
      "14052/14052 [==============================] - 7s 471us/step - loss: 0.4219 - binary_crossentropy: 0.4219 - acc: 0.8087 - val_loss: 0.4395 - val_binary_crossentropy: 0.4395 - val_acc: 0.7990\n",
      "Epoch 65/10000\n",
      "14052/14052 [==============================] - 7s 471us/step - loss: 0.4261 - binary_crossentropy: 0.4261 - acc: 0.8084 - val_loss: 0.4284 - val_binary_crossentropy: 0.4284 - val_acc: 0.8048\n",
      "Epoch 66/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.4256 - binary_crossentropy: 0.4256 - acc: 0.8057 - val_loss: 0.4273 - val_binary_crossentropy: 0.4273 - val_acc: 0.8056\n",
      "Epoch 67/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.4222 - binary_crossentropy: 0.4222 - acc: 0.8103 - val_loss: 0.4373 - val_binary_crossentropy: 0.4373 - val_acc: 0.8001\n",
      "Epoch 68/10000\n",
      "14052/14052 [==============================] - 7s 474us/step - loss: 0.4192 - binary_crossentropy: 0.4192 - acc: 0.8113 - val_loss: 0.4309 - val_binary_crossentropy: 0.4309 - val_acc: 0.7990\n",
      "Epoch 69/10000\n",
      "14052/14052 [==============================] - 7s 474us/step - loss: 0.4220 - binary_crossentropy: 0.4220 - acc: 0.8072 - val_loss: 0.4418 - val_binary_crossentropy: 0.4418 - val_acc: 0.7967\n",
      "Epoch 70/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.4247 - binary_crossentropy: 0.4247 - acc: 0.8066 - val_loss: 0.4304 - val_binary_crossentropy: 0.4304 - val_acc: 0.8018\n",
      "Epoch 71/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.4176 - binary_crossentropy: 0.4176 - acc: 0.8109 - val_loss: 0.4633 - val_binary_crossentropy: 0.4633 - val_acc: 0.7817\n",
      "Epoch 72/10000\n",
      "14052/14052 [==============================] - 7s 471us/step - loss: 0.4204 - binary_crossentropy: 0.4204 - acc: 0.8083 - val_loss: 0.4459 - val_binary_crossentropy: 0.4459 - val_acc: 0.7886\n",
      "Epoch 73/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.4111 - binary_crossentropy: 0.4111 - acc: 0.8181 - val_loss: 0.4436 - val_binary_crossentropy: 0.4436 - val_acc: 0.7918\n",
      "Epoch 74/10000\n",
      "14052/14052 [==============================] - 7s 484us/step - loss: 0.4140 - binary_crossentropy: 0.4140 - acc: 0.8140 - val_loss: 0.4328 - val_binary_crossentropy: 0.4328 - val_acc: 0.8007\n",
      "Epoch 75/10000\n",
      "14052/14052 [==============================] - 7s 474us/step - loss: 0.4150 - binary_crossentropy: 0.4150 - acc: 0.8155 - val_loss: 0.4286 - val_binary_crossentropy: 0.4286 - val_acc: 0.8001\n",
      "Epoch 76/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.4163 - binary_crossentropy: 0.4163 - acc: 0.8125 - val_loss: 0.4359 - val_binary_crossentropy: 0.4359 - val_acc: 0.7999\n",
      "Epoch 77/10000\n",
      "14052/14052 [==============================] - 7s 477us/step - loss: 0.4137 - binary_crossentropy: 0.4137 - acc: 0.8141 - val_loss: 0.4354 - val_binary_crossentropy: 0.4354 - val_acc: 0.7971\n",
      "Epoch 78/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.4160 - binary_crossentropy: 0.4160 - acc: 0.8128 - val_loss: 0.4291 - val_binary_crossentropy: 0.4291 - val_acc: 0.8009\n",
      "Epoch 79/10000\n",
      "14052/14052 [==============================] - 7s 478us/step - loss: 0.4148 - binary_crossentropy: 0.4148 - acc: 0.8150 - val_loss: 0.4284 - val_binary_crossentropy: 0.4284 - val_acc: 0.8050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/10000\n",
      "14052/14052 [==============================] - 6s 459us/step - loss: 0.4147 - binary_crossentropy: 0.4147 - acc: 0.8115 - val_loss: 0.4420 - val_binary_crossentropy: 0.4420 - val_acc: 0.7948\n",
      "Epoch 81/10000\n",
      "14052/14052 [==============================] - 7s 463us/step - loss: 0.4144 - binary_crossentropy: 0.4144 - acc: 0.8111 - val_loss: 0.4329 - val_binary_crossentropy: 0.4329 - val_acc: 0.7988\n",
      "Epoch 82/10000\n",
      "14052/14052 [==============================] - 6s 461us/step - loss: 0.4104 - binary_crossentropy: 0.4104 - acc: 0.8177 - val_loss: 0.4321 - val_binary_crossentropy: 0.4321 - val_acc: 0.7994\n",
      "Epoch 83/10000\n",
      "14052/14052 [==============================] - 7s 466us/step - loss: 0.4141 - binary_crossentropy: 0.4141 - acc: 0.8138 - val_loss: 0.4495 - val_binary_crossentropy: 0.4495 - val_acc: 0.7858\n",
      "Epoch 84/10000\n",
      "14052/14052 [==============================] - 6s 459us/step - loss: 0.4163 - binary_crossentropy: 0.4163 - acc: 0.8101 - val_loss: 0.4406 - val_binary_crossentropy: 0.4406 - val_acc: 0.7954\n",
      "Epoch 85/10000\n",
      "14052/14052 [==============================] - 6s 461us/step - loss: 0.4126 - binary_crossentropy: 0.4126 - acc: 0.8148 - val_loss: 0.4277 - val_binary_crossentropy: 0.4277 - val_acc: 0.8020\n",
      "Epoch 86/10000\n",
      "14052/14052 [==============================] - 7s 464us/step - loss: 0.4069 - binary_crossentropy: 0.4069 - acc: 0.8161 - val_loss: 0.4297 - val_binary_crossentropy: 0.4297 - val_acc: 0.8020\n",
      "Epoch 87/10000\n",
      "14052/14052 [==============================] - 7s 467us/step - loss: 0.4080 - binary_crossentropy: 0.4080 - acc: 0.8159 - val_loss: 0.4318 - val_binary_crossentropy: 0.4318 - val_acc: 0.8029\n",
      "Epoch 88/10000\n",
      "14052/14052 [==============================] - 7s 465us/step - loss: 0.4101 - binary_crossentropy: 0.4101 - acc: 0.8165 - val_loss: 0.4356 - val_binary_crossentropy: 0.4356 - val_acc: 0.7999\n",
      "Epoch 89/10000\n",
      "14052/14052 [==============================] - 7s 464us/step - loss: 0.4069 - binary_crossentropy: 0.4069 - acc: 0.8183 - val_loss: 0.4327 - val_binary_crossentropy: 0.4327 - val_acc: 0.7977\n",
      "Epoch 90/10000\n",
      "14052/14052 [==============================] - 7s 465us/step - loss: 0.4130 - binary_crossentropy: 0.4130 - acc: 0.8150 - val_loss: 0.4315 - val_binary_crossentropy: 0.4315 - val_acc: 0.7954\n",
      "Epoch 91/10000\n",
      "14052/14052 [==============================] - 7s 466us/step - loss: 0.4074 - binary_crossentropy: 0.4074 - acc: 0.8153 - val_loss: 0.4374 - val_binary_crossentropy: 0.4374 - val_acc: 0.7941\n",
      "Epoch 92/10000\n",
      "14052/14052 [==============================] - 7s 468us/step - loss: 0.4077 - binary_crossentropy: 0.4077 - acc: 0.8145 - val_loss: 0.5480 - val_binary_crossentropy: 0.5480 - val_acc: 0.7433\n",
      "Epoch 93/10000\n",
      "14052/14052 [==============================] - 7s 465us/step - loss: 0.4087 - binary_crossentropy: 0.4087 - acc: 0.8155 - val_loss: 0.4435 - val_binary_crossentropy: 0.4435 - val_acc: 0.7924\n",
      "Epoch 94/10000\n",
      "14052/14052 [==============================] - 7s 466us/step - loss: 0.4079 - binary_crossentropy: 0.4079 - acc: 0.8152 - val_loss: 0.4271 - val_binary_crossentropy: 0.4271 - val_acc: 0.8031\n",
      "Epoch 95/10000\n",
      "14052/14052 [==============================] - 7s 466us/step - loss: 0.4029 - binary_crossentropy: 0.4029 - acc: 0.8181 - val_loss: 0.4441 - val_binary_crossentropy: 0.4441 - val_acc: 0.7962\n",
      "Epoch 96/10000\n",
      "14052/14052 [==============================] - 7s 467us/step - loss: 0.4101 - binary_crossentropy: 0.4101 - acc: 0.8188 - val_loss: 0.4898 - val_binary_crossentropy: 0.4898 - val_acc: 0.7655\n",
      "Epoch 97/10000\n",
      "14052/14052 [==============================] - 7s 469us/step - loss: 0.4092 - binary_crossentropy: 0.4092 - acc: 0.8159 - val_loss: 0.4310 - val_binary_crossentropy: 0.4310 - val_acc: 0.7992\n",
      "Epoch 98/10000\n",
      "14052/14052 [==============================] - 7s 468us/step - loss: 0.4060 - binary_crossentropy: 0.4060 - acc: 0.8129 - val_loss: 0.4364 - val_binary_crossentropy: 0.4364 - val_acc: 0.7973\n",
      "Epoch 99/10000\n",
      "14052/14052 [==============================] - 7s 467us/step - loss: 0.4042 - binary_crossentropy: 0.4042 - acc: 0.8177 - val_loss: 0.4466 - val_binary_crossentropy: 0.4466 - val_acc: 0.7896\n",
      "Epoch 100/10000\n",
      "14052/14052 [==============================] - 7s 469us/step - loss: 0.4037 - binary_crossentropy: 0.4037 - acc: 0.8185 - val_loss: 0.4459 - val_binary_crossentropy: 0.4459 - val_acc: 0.7928\n",
      "Epoch 101/10000\n",
      "14052/14052 [==============================] - 7s 469us/step - loss: 0.4065 - binary_crossentropy: 0.4065 - acc: 0.8160 - val_loss: 0.4267 - val_binary_crossentropy: 0.4267 - val_acc: 0.8016\n",
      "Epoch 102/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.3992 - binary_crossentropy: 0.3992 - acc: 0.8215 - val_loss: 0.5068 - val_binary_crossentropy: 0.5068 - val_acc: 0.7583\n",
      "Epoch 103/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.4041 - binary_crossentropy: 0.4041 - acc: 0.8185 - val_loss: 0.4413 - val_binary_crossentropy: 0.4413 - val_acc: 0.7939\n",
      "Epoch 104/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.4037 - binary_crossentropy: 0.4037 - acc: 0.8192 - val_loss: 0.4290 - val_binary_crossentropy: 0.4290 - val_acc: 0.8003\n",
      "Epoch 105/10000\n",
      "14052/14052 [==============================] - 7s 471us/step - loss: 0.3999 - binary_crossentropy: 0.3999 - acc: 0.8158 - val_loss: 0.4314 - val_binary_crossentropy: 0.4314 - val_acc: 0.8014\n",
      "Epoch 106/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.4015 - binary_crossentropy: 0.4015 - acc: 0.8163 - val_loss: 0.4433 - val_binary_crossentropy: 0.4433 - val_acc: 0.7948\n",
      "Epoch 107/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.4079 - binary_crossentropy: 0.4079 - acc: 0.8150 - val_loss: 0.4341 - val_binary_crossentropy: 0.4341 - val_acc: 0.8016\n",
      "Epoch 108/10000\n",
      "14052/14052 [==============================] - 7s 476us/step - loss: 0.4043 - binary_crossentropy: 0.4043 - acc: 0.8159 - val_loss: 0.4636 - val_binary_crossentropy: 0.4636 - val_acc: 0.7826\n",
      "Epoch 109/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.4061 - binary_crossentropy: 0.4061 - acc: 0.8193 - val_loss: 0.4365 - val_binary_crossentropy: 0.4365 - val_acc: 0.7992\n",
      "Epoch 110/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.3981 - binary_crossentropy: 0.3981 - acc: 0.8221 - val_loss: 0.4473 - val_binary_crossentropy: 0.4473 - val_acc: 0.7918\n",
      "Epoch 111/10000\n",
      "14052/14052 [==============================] - 7s 471us/step - loss: 0.4020 - binary_crossentropy: 0.4020 - acc: 0.8211 - val_loss: 0.4441 - val_binary_crossentropy: 0.4441 - val_acc: 0.7945\n",
      "Epoch 112/10000\n",
      "14052/14052 [==============================] - 7s 471us/step - loss: 0.4004 - binary_crossentropy: 0.4004 - acc: 0.8242 - val_loss: 0.4907 - val_binary_crossentropy: 0.4907 - val_acc: 0.7651\n",
      "Epoch 113/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.3983 - binary_crossentropy: 0.3983 - acc: 0.8188 - val_loss: 0.4359 - val_binary_crossentropy: 0.4359 - val_acc: 0.7992\n",
      "Epoch 114/10000\n",
      "14052/14052 [==============================] - 7s 489us/step - loss: 0.3973 - binary_crossentropy: 0.3973 - acc: 0.8263 - val_loss: 0.5778 - val_binary_crossentropy: 0.5778 - val_acc: 0.7310\n",
      "Epoch 115/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.3988 - binary_crossentropy: 0.3988 - acc: 0.8178 - val_loss: 0.4395 - val_binary_crossentropy: 0.4395 - val_acc: 0.7992\n",
      "Epoch 116/10000\n",
      "14052/14052 [==============================] - 7s 476us/step - loss: 0.3963 - binary_crossentropy: 0.3963 - acc: 0.8247 - val_loss: 0.4340 - val_binary_crossentropy: 0.4340 - val_acc: 0.8014\n",
      "Epoch 117/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.3973 - binary_crossentropy: 0.3973 - acc: 0.8203 - val_loss: 0.4329 - val_binary_crossentropy: 0.4329 - val_acc: 0.8046\n",
      "Epoch 118/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.3974 - binary_crossentropy: 0.3974 - acc: 0.8210 - val_loss: 0.4356 - val_binary_crossentropy: 0.4356 - val_acc: 0.8012\n",
      "Epoch 119/10000\n",
      "14052/14052 [==============================] - 7s 480us/step - loss: 0.3969 - binary_crossentropy: 0.3969 - acc: 0.8185 - val_loss: 0.4405 - val_binary_crossentropy: 0.4405 - val_acc: 0.7933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/10000\n",
      "14052/14052 [==============================] - 7s 491us/step - loss: 0.3938 - binary_crossentropy: 0.3938 - acc: 0.8256 - val_loss: 0.4622 - val_binary_crossentropy: 0.4622 - val_acc: 0.7871\n",
      "Epoch 121/10000\n",
      "14052/14052 [==============================] - 6s 461us/step - loss: 0.3913 - binary_crossentropy: 0.3913 - acc: 0.8267 - val_loss: 0.4392 - val_binary_crossentropy: 0.4392 - val_acc: 0.7999\n",
      "Epoch 122/10000\n",
      "14052/14052 [==============================] - 6s 462us/step - loss: 0.3916 - binary_crossentropy: 0.3916 - acc: 0.8254 - val_loss: 0.4419 - val_binary_crossentropy: 0.4419 - val_acc: 0.7975\n",
      "Epoch 123/10000\n",
      "14052/14052 [==============================] - 6s 461us/step - loss: 0.3996 - binary_crossentropy: 0.3996 - acc: 0.8228 - val_loss: 1.1986 - val_binary_crossentropy: 1.1986 - val_acc: 0.5443\n",
      "Epoch 124/10000\n",
      "14052/14052 [==============================] - 6s 463us/step - loss: 0.4043 - binary_crossentropy: 0.4043 - acc: 0.8173 - val_loss: 0.4327 - val_binary_crossentropy: 0.4327 - val_acc: 0.8037\n",
      "Epoch 125/10000\n",
      "14052/14052 [==============================] - 7s 464us/step - loss: 0.3988 - binary_crossentropy: 0.3988 - acc: 0.8205 - val_loss: 0.4276 - val_binary_crossentropy: 0.4276 - val_acc: 0.8012\n",
      "Epoch 126/10000\n",
      "14052/14052 [==============================] - 7s 463us/step - loss: 0.3940 - binary_crossentropy: 0.3940 - acc: 0.8222 - val_loss: 0.4351 - val_binary_crossentropy: 0.4351 - val_acc: 0.8029\n",
      "Epoch 127/10000\n",
      "14052/14052 [==============================] - 7s 465us/step - loss: 0.3924 - binary_crossentropy: 0.3924 - acc: 0.8234 - val_loss: 0.4306 - val_binary_crossentropy: 0.4306 - val_acc: 0.8026\n",
      "Epoch 128/10000\n",
      "14052/14052 [==============================] - 7s 464us/step - loss: 0.3910 - binary_crossentropy: 0.3910 - acc: 0.8266 - val_loss: 0.4496 - val_binary_crossentropy: 0.4496 - val_acc: 0.7935\n",
      "Epoch 129/10000\n",
      "14052/14052 [==============================] - 7s 464us/step - loss: 0.3914 - binary_crossentropy: 0.3914 - acc: 0.8252 - val_loss: 0.4311 - val_binary_crossentropy: 0.4311 - val_acc: 0.8005\n",
      "Epoch 130/10000\n",
      "14052/14052 [==============================] - 7s 466us/step - loss: 0.3902 - binary_crossentropy: 0.3902 - acc: 0.8273 - val_loss: 0.4492 - val_binary_crossentropy: 0.4492 - val_acc: 0.7879\n",
      "Epoch 131/10000\n",
      "14052/14052 [==============================] - 7s 466us/step - loss: 0.3884 - binary_crossentropy: 0.3884 - acc: 0.8276 - val_loss: 0.4466 - val_binary_crossentropy: 0.4466 - val_acc: 0.7969\n",
      "Epoch 132/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.3905 - binary_crossentropy: 0.3905 - acc: 0.8301 - val_loss: 0.4441 - val_binary_crossentropy: 0.4441 - val_acc: 0.7943\n",
      "Epoch 133/10000\n",
      "14052/14052 [==============================] - 7s 464us/step - loss: 0.3881 - binary_crossentropy: 0.3881 - acc: 0.8278 - val_loss: 0.4453 - val_binary_crossentropy: 0.4453 - val_acc: 0.7950\n",
      "Epoch 134/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.3883 - binary_crossentropy: 0.3883 - acc: 0.8251 - val_loss: 0.4630 - val_binary_crossentropy: 0.4630 - val_acc: 0.7890\n",
      "Epoch 135/10000\n",
      "14052/14052 [==============================] - 7s 467us/step - loss: 0.3880 - binary_crossentropy: 0.3880 - acc: 0.8286 - val_loss: 0.4492 - val_binary_crossentropy: 0.4492 - val_acc: 0.7965\n",
      "Epoch 136/10000\n",
      "14052/14052 [==============================] - 7s 467us/step - loss: 0.3902 - binary_crossentropy: 0.3902 - acc: 0.8294 - val_loss: 0.4467 - val_binary_crossentropy: 0.4467 - val_acc: 0.7937\n",
      "Epoch 137/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.3875 - binary_crossentropy: 0.3875 - acc: 0.8287 - val_loss: 0.4481 - val_binary_crossentropy: 0.4481 - val_acc: 0.7909\n",
      "Epoch 138/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.3898 - binary_crossentropy: 0.3898 - acc: 0.8282 - val_loss: 0.4596 - val_binary_crossentropy: 0.4596 - val_acc: 0.7879\n",
      "Epoch 139/10000\n",
      "14052/14052 [==============================] - 7s 467us/step - loss: 0.3865 - binary_crossentropy: 0.3865 - acc: 0.8264 - val_loss: 0.4349 - val_binary_crossentropy: 0.4349 - val_acc: 0.8063\n",
      "Epoch 140/10000\n",
      "14052/14052 [==============================] - 7s 468us/step - loss: 0.3813 - binary_crossentropy: 0.3813 - acc: 0.8301 - val_loss: 0.4603 - val_binary_crossentropy: 0.4603 - val_acc: 0.7847\n",
      "Epoch 141/10000\n",
      "14052/14052 [==============================] - 7s 469us/step - loss: 0.3826 - binary_crossentropy: 0.3826 - acc: 0.8300 - val_loss: 0.4994 - val_binary_crossentropy: 0.4994 - val_acc: 0.7651\n",
      "Epoch 142/10000\n",
      "14052/14052 [==============================] - 7s 471us/step - loss: 0.3873 - binary_crossentropy: 0.3873 - acc: 0.8229 - val_loss: 0.4387 - val_binary_crossentropy: 0.4387 - val_acc: 0.7973\n",
      "Epoch 143/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.3795 - binary_crossentropy: 0.3795 - acc: 0.8297 - val_loss: 0.4382 - val_binary_crossentropy: 0.4382 - val_acc: 0.7997\n",
      "Epoch 144/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.3822 - binary_crossentropy: 0.3822 - acc: 0.8326 - val_loss: 0.4337 - val_binary_crossentropy: 0.4337 - val_acc: 0.8026\n",
      "Epoch 145/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.3817 - binary_crossentropy: 0.3817 - acc: 0.8319 - val_loss: 0.4550 - val_binary_crossentropy: 0.4550 - val_acc: 0.7896\n",
      "Epoch 146/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.3821 - binary_crossentropy: 0.3821 - acc: 0.8323 - val_loss: 0.4416 - val_binary_crossentropy: 0.4416 - val_acc: 0.7965\n",
      "Epoch 147/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.3781 - binary_crossentropy: 0.3781 - acc: 0.8313 - val_loss: 0.5034 - val_binary_crossentropy: 0.5034 - val_acc: 0.7610\n",
      "Epoch 148/10000\n",
      "14052/14052 [==============================] - 7s 471us/step - loss: 0.3790 - binary_crossentropy: 0.3790 - acc: 0.8311 - val_loss: 0.4941 - val_binary_crossentropy: 0.4941 - val_acc: 0.7683\n",
      "Epoch 149/10000\n",
      "14052/14052 [==============================] - 7s 471us/step - loss: 0.3823 - binary_crossentropy: 0.3823 - acc: 0.8317 - val_loss: 0.4316 - val_binary_crossentropy: 0.4316 - val_acc: 0.7990\n",
      "Epoch 150/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.3798 - binary_crossentropy: 0.3798 - acc: 0.8324 - val_loss: 0.5278 - val_binary_crossentropy: 0.5278 - val_acc: 0.7649\n",
      "Epoch 151/10000\n",
      "14052/14052 [==============================] - 7s 474us/step - loss: 0.3834 - binary_crossentropy: 0.3834 - acc: 0.8301 - val_loss: 0.5616 - val_binary_crossentropy: 0.5616 - val_acc: 0.7218\n",
      "Epoch 152/10000\n",
      "14052/14052 [==============================] - 7s 478us/step - loss: 0.3825 - binary_crossentropy: 0.3825 - acc: 0.8326 - val_loss: 0.4520 - val_binary_crossentropy: 0.4520 - val_acc: 0.7913\n",
      "Epoch 153/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.3778 - binary_crossentropy: 0.3778 - acc: 0.8330 - val_loss: 0.4897 - val_binary_crossentropy: 0.4897 - val_acc: 0.7790\n",
      "Epoch 154/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.3786 - binary_crossentropy: 0.3786 - acc: 0.8297 - val_loss: 0.4386 - val_binary_crossentropy: 0.4386 - val_acc: 0.7986\n",
      "Epoch 155/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.3752 - binary_crossentropy: 0.3752 - acc: 0.8339 - val_loss: 0.4406 - val_binary_crossentropy: 0.4406 - val_acc: 0.7969\n",
      "Epoch 156/10000\n",
      "14052/14052 [==============================] - 7s 474us/step - loss: 0.3772 - binary_crossentropy: 0.3772 - acc: 0.8343 - val_loss: 0.4334 - val_binary_crossentropy: 0.4334 - val_acc: 0.8018\n",
      "Epoch 157/10000\n",
      "14052/14052 [==============================] - 7s 471us/step - loss: 0.3746 - binary_crossentropy: 0.3746 - acc: 0.8326 - val_loss: 0.4765 - val_binary_crossentropy: 0.4765 - val_acc: 0.7852\n",
      "Epoch 158/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.3762 - binary_crossentropy: 0.3762 - acc: 0.8338 - val_loss: 0.4377 - val_binary_crossentropy: 0.4377 - val_acc: 0.7933\n",
      "Epoch 159/10000\n",
      "14052/14052 [==============================] - 6s 458us/step - loss: 0.3786 - binary_crossentropy: 0.3786 - acc: 0.8321 - val_loss: 0.4451 - val_binary_crossentropy: 0.4451 - val_acc: 0.7941\n",
      "Epoch 160/10000\n",
      "14052/14052 [==============================] - 6s 461us/step - loss: 0.3737 - binary_crossentropy: 0.3737 - acc: 0.8351 - val_loss: 0.4379 - val_binary_crossentropy: 0.4379 - val_acc: 0.8039\n",
      "Epoch 161/10000\n",
      "14052/14052 [==============================] - 6s 458us/step - loss: 0.3709 - binary_crossentropy: 0.3709 - acc: 0.8336 - val_loss: 0.4757 - val_binary_crossentropy: 0.4757 - val_acc: 0.7815\n",
      "Epoch 162/10000\n",
      "14052/14052 [==============================] - 6s 461us/step - loss: 0.3737 - binary_crossentropy: 0.3737 - acc: 0.8333 - val_loss: 0.4393 - val_binary_crossentropy: 0.4393 - val_acc: 0.8026\n",
      "Epoch 163/10000\n",
      "14052/14052 [==============================] - 6s 458us/step - loss: 0.3691 - binary_crossentropy: 0.3691 - acc: 0.8370 - val_loss: 0.4394 - val_binary_crossentropy: 0.4394 - val_acc: 0.7982\n",
      "Epoch 164/10000\n",
      "14052/14052 [==============================] - 6s 460us/step - loss: 0.3694 - binary_crossentropy: 0.3694 - acc: 0.8375 - val_loss: 0.4438 - val_binary_crossentropy: 0.4438 - val_acc: 0.8035\n",
      "Epoch 165/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.3646 - binary_crossentropy: 0.3646 - acc: 0.8392 - val_loss: 0.5766 - val_binary_crossentropy: 0.5766 - val_acc: 0.7378\n",
      "Epoch 166/10000\n",
      "14052/14052 [==============================] - 7s 490us/step - loss: 0.3746 - binary_crossentropy: 0.3746 - acc: 0.8327 - val_loss: 0.4608 - val_binary_crossentropy: 0.4608 - val_acc: 0.7901\n",
      "Epoch 167/10000\n",
      "14052/14052 [==============================] - 7s 463us/step - loss: 0.3649 - binary_crossentropy: 0.3649 - acc: 0.8380 - val_loss: 0.4355 - val_binary_crossentropy: 0.4355 - val_acc: 0.8009\n",
      "Epoch 168/10000\n",
      "14052/14052 [==============================] - 7s 463us/step - loss: 0.3683 - binary_crossentropy: 0.3683 - acc: 0.8421 - val_loss: 0.4919 - val_binary_crossentropy: 0.4919 - val_acc: 0.7687\n",
      "Epoch 169/10000\n",
      "14052/14052 [==============================] - 7s 494us/step - loss: 0.3701 - binary_crossentropy: 0.3701 - acc: 0.8348 - val_loss: 0.4441 - val_binary_crossentropy: 0.4441 - val_acc: 0.7956\n",
      "Epoch 170/10000\n",
      "14052/14052 [==============================] - 7s 474us/step - loss: 0.3706 - binary_crossentropy: 0.3706 - acc: 0.8357 - val_loss: 0.4712 - val_binary_crossentropy: 0.4712 - val_acc: 0.7802\n",
      "Epoch 171/10000\n",
      "14052/14052 [==============================] - 7s 476us/step - loss: 0.3688 - binary_crossentropy: 0.3688 - acc: 0.8367 - val_loss: 0.4418 - val_binary_crossentropy: 0.4418 - val_acc: 0.7988\n",
      "Epoch 172/10000\n",
      "14052/14052 [==============================] - 7s 466us/step - loss: 0.3701 - binary_crossentropy: 0.3701 - acc: 0.8374 - val_loss: 0.4403 - val_binary_crossentropy: 0.4403 - val_acc: 0.7969\n",
      "Epoch 173/10000\n",
      "14052/14052 [==============================] - 7s 467us/step - loss: 0.3732 - binary_crossentropy: 0.3732 - acc: 0.8349 - val_loss: 0.4455 - val_binary_crossentropy: 0.4455 - val_acc: 0.7992\n",
      "Epoch 174/10000\n",
      "14052/14052 [==============================] - 7s 469us/step - loss: 0.3678 - binary_crossentropy: 0.3678 - acc: 0.8329 - val_loss: 0.5064 - val_binary_crossentropy: 0.5064 - val_acc: 0.7704\n",
      "Epoch 175/10000\n",
      "14052/14052 [==============================] - 7s 468us/step - loss: 0.3647 - binary_crossentropy: 0.3647 - acc: 0.8409 - val_loss: 0.4973 - val_binary_crossentropy: 0.4973 - val_acc: 0.7824\n",
      "Epoch 176/10000\n",
      "14052/14052 [==============================] - 7s 469us/step - loss: 0.3605 - binary_crossentropy: 0.3605 - acc: 0.8431 - val_loss: 0.4406 - val_binary_crossentropy: 0.4406 - val_acc: 0.8031\n",
      "Epoch 177/10000\n",
      "14052/14052 [==============================] - 7s 468us/step - loss: 0.3707 - binary_crossentropy: 0.3707 - acc: 0.8347 - val_loss: 0.4475 - val_binary_crossentropy: 0.4475 - val_acc: 0.7988\n",
      "Epoch 178/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.3633 - binary_crossentropy: 0.3633 - acc: 0.8415 - val_loss: 0.4368 - val_binary_crossentropy: 0.4368 - val_acc: 0.7980\n",
      "Epoch 179/10000\n",
      "14052/14052 [==============================] - 7s 468us/step - loss: 0.3633 - binary_crossentropy: 0.3633 - acc: 0.8415 - val_loss: 0.4576 - val_binary_crossentropy: 0.4576 - val_acc: 0.7907\n",
      "Epoch 180/10000\n",
      "14052/14052 [==============================] - 7s 469us/step - loss: 0.3623 - binary_crossentropy: 0.3623 - acc: 0.8383 - val_loss: 0.4362 - val_binary_crossentropy: 0.4362 - val_acc: 0.8012\n",
      "Epoch 181/10000\n",
      "14052/14052 [==============================] - 7s 469us/step - loss: 0.3640 - binary_crossentropy: 0.3640 - acc: 0.8366 - val_loss: 0.4943 - val_binary_crossentropy: 0.4943 - val_acc: 0.7736\n",
      "Epoch 182/10000\n",
      "14052/14052 [==============================] - 7s 467us/step - loss: 0.3620 - binary_crossentropy: 0.3620 - acc: 0.8433 - val_loss: 0.6997 - val_binary_crossentropy: 0.6997 - val_acc: 0.7049\n",
      "Epoch 183/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.3690 - binary_crossentropy: 0.3690 - acc: 0.8370 - val_loss: 0.4914 - val_binary_crossentropy: 0.4914 - val_acc: 0.7785\n",
      "Epoch 184/10000\n",
      "14052/14052 [==============================] - 7s 474us/step - loss: 0.3599 - binary_crossentropy: 0.3599 - acc: 0.8419 - val_loss: 0.4447 - val_binary_crossentropy: 0.4447 - val_acc: 0.8009\n",
      "Epoch 185/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.3584 - binary_crossentropy: 0.3584 - acc: 0.8420 - val_loss: 0.4746 - val_binary_crossentropy: 0.4746 - val_acc: 0.7860\n",
      "Epoch 186/10000\n",
      "14052/14052 [==============================] - 7s 469us/step - loss: 0.3629 - binary_crossentropy: 0.3629 - acc: 0.8400 - val_loss: 0.4552 - val_binary_crossentropy: 0.4552 - val_acc: 0.7950\n",
      "Epoch 187/10000\n",
      "14052/14052 [==============================] - 7s 475us/step - loss: 0.3550 - binary_crossentropy: 0.3550 - acc: 0.8417 - val_loss: 0.4439 - val_binary_crossentropy: 0.4439 - val_acc: 0.7924\n",
      "Epoch 188/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.3642 - binary_crossentropy: 0.3642 - acc: 0.8406 - val_loss: 0.4460 - val_binary_crossentropy: 0.4460 - val_acc: 0.7928\n",
      "Epoch 189/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.3604 - binary_crossentropy: 0.3604 - acc: 0.8395 - val_loss: 0.4533 - val_binary_crossentropy: 0.4533 - val_acc: 0.7877\n",
      "Epoch 190/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.3618 - binary_crossentropy: 0.3618 - acc: 0.8382 - val_loss: 0.6327 - val_binary_crossentropy: 0.6327 - val_acc: 0.7054\n",
      "Epoch 191/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.3618 - binary_crossentropy: 0.3618 - acc: 0.8417 - val_loss: 0.4409 - val_binary_crossentropy: 0.4409 - val_acc: 0.7969\n",
      "Epoch 192/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.3587 - binary_crossentropy: 0.3587 - acc: 0.8434 - val_loss: 0.4400 - val_binary_crossentropy: 0.4400 - val_acc: 0.8009\n",
      "Epoch 193/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.3596 - binary_crossentropy: 0.3596 - acc: 0.8409 - val_loss: 0.4721 - val_binary_crossentropy: 0.4721 - val_acc: 0.7824\n",
      "Epoch 194/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.3583 - binary_crossentropy: 0.3583 - acc: 0.8450 - val_loss: 0.4453 - val_binary_crossentropy: 0.4453 - val_acc: 0.8003\n",
      "Epoch 195/10000\n",
      "14052/14052 [==============================] - 7s 474us/step - loss: 0.3518 - binary_crossentropy: 0.3518 - acc: 0.8454 - val_loss: 0.4412 - val_binary_crossentropy: 0.4412 - val_acc: 0.7960\n",
      "Epoch 196/10000\n",
      "14052/14052 [==============================] - 7s 480us/step - loss: 0.3561 - binary_crossentropy: 0.3561 - acc: 0.8466 - val_loss: 0.4422 - val_binary_crossentropy: 0.4422 - val_acc: 0.7973\n",
      "Epoch 197/10000\n",
      "14052/14052 [==============================] - 7s 478us/step - loss: 0.3541 - binary_crossentropy: 0.3541 - acc: 0.8466 - val_loss: 0.4404 - val_binary_crossentropy: 0.4404 - val_acc: 0.8022\n",
      "Epoch 198/10000\n",
      "14052/14052 [==============================] - 7s 474us/step - loss: 0.3624 - binary_crossentropy: 0.3624 - acc: 0.8396 - val_loss: 0.4475 - val_binary_crossentropy: 0.4475 - val_acc: 0.7969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199/10000\n",
      "14052/14052 [==============================] - 6s 461us/step - loss: 0.3522 - binary_crossentropy: 0.3522 - acc: 0.8476 - val_loss: 0.4410 - val_binary_crossentropy: 0.4410 - val_acc: 0.7977\n",
      "Epoch 200/10000\n",
      "14052/14052 [==============================] - 6s 458us/step - loss: 0.3495 - binary_crossentropy: 0.3495 - acc: 0.8442 - val_loss: 0.4530 - val_binary_crossentropy: 0.4530 - val_acc: 0.7967\n",
      "Epoch 201/10000\n",
      "14052/14052 [==============================] - 6s 457us/step - loss: 0.3541 - binary_crossentropy: 0.3541 - acc: 0.8472 - val_loss: 0.4708 - val_binary_crossentropy: 0.4708 - val_acc: 0.7926\n",
      "Epoch 202/10000\n",
      "14052/14052 [==============================] - 7s 468us/step - loss: 0.3545 - binary_crossentropy: 0.3545 - acc: 0.8451 - val_loss: 0.5150 - val_binary_crossentropy: 0.5150 - val_acc: 0.7621\n",
      "Epoch 203/10000\n",
      "14052/14052 [==============================] - 6s 459us/step - loss: 0.3520 - binary_crossentropy: 0.3520 - acc: 0.8447 - val_loss: 0.4448 - val_binary_crossentropy: 0.4448 - val_acc: 0.7988\n",
      "Epoch 204/10000\n",
      "14052/14052 [==============================] - 6s 459us/step - loss: 0.3487 - binary_crossentropy: 0.3487 - acc: 0.8497 - val_loss: 0.4484 - val_binary_crossentropy: 0.4484 - val_acc: 0.7952\n",
      "Epoch 205/10000\n",
      "14052/14052 [==============================] - 7s 463us/step - loss: 0.3517 - binary_crossentropy: 0.3517 - acc: 0.8487 - val_loss: 0.4724 - val_binary_crossentropy: 0.4724 - val_acc: 0.7811\n",
      "Epoch 206/10000\n",
      "14052/14052 [==============================] - 7s 463us/step - loss: 0.3509 - binary_crossentropy: 0.3509 - acc: 0.8447 - val_loss: 0.4843 - val_binary_crossentropy: 0.4843 - val_acc: 0.7830\n",
      "Epoch 207/10000\n",
      "14052/14052 [==============================] - 7s 465us/step - loss: 0.3481 - binary_crossentropy: 0.3481 - acc: 0.8466 - val_loss: 0.5413 - val_binary_crossentropy: 0.5413 - val_acc: 0.7655\n",
      "Epoch 208/10000\n",
      "14052/14052 [==============================] - 7s 463us/step - loss: 0.3568 - binary_crossentropy: 0.3568 - acc: 0.8407 - val_loss: 0.4562 - val_binary_crossentropy: 0.4562 - val_acc: 0.7879\n",
      "Epoch 209/10000\n",
      "14052/14052 [==============================] - 7s 464us/step - loss: 0.3473 - binary_crossentropy: 0.3473 - acc: 0.8482 - val_loss: 0.4741 - val_binary_crossentropy: 0.4741 - val_acc: 0.7905\n",
      "Epoch 210/10000\n",
      "14052/14052 [==============================] - 7s 464us/step - loss: 0.3496 - binary_crossentropy: 0.3496 - acc: 0.8461 - val_loss: 0.4772 - val_binary_crossentropy: 0.4772 - val_acc: 0.7924\n",
      "Epoch 211/10000\n",
      "14052/14052 [==============================] - 7s 487us/step - loss: 0.3490 - binary_crossentropy: 0.3490 - acc: 0.8448 - val_loss: 0.4818 - val_binary_crossentropy: 0.4818 - val_acc: 0.7847\n",
      "Epoch 212/10000\n",
      "14052/14052 [==============================] - 7s 467us/step - loss: 0.3457 - binary_crossentropy: 0.3457 - acc: 0.8460 - val_loss: 0.4608 - val_binary_crossentropy: 0.4608 - val_acc: 0.7971\n",
      "Epoch 213/10000\n",
      "14052/14052 [==============================] - 7s 466us/step - loss: 0.3528 - binary_crossentropy: 0.3528 - acc: 0.8443 - val_loss: 0.5852 - val_binary_crossentropy: 0.5852 - val_acc: 0.7378\n",
      "Epoch 214/10000\n",
      "14052/14052 [==============================] - 7s 466us/step - loss: 0.3503 - binary_crossentropy: 0.3503 - acc: 0.8488 - val_loss: 0.4671 - val_binary_crossentropy: 0.4671 - val_acc: 0.7939\n",
      "Epoch 215/10000\n",
      "14052/14052 [==============================] - 7s 468us/step - loss: 0.3452 - binary_crossentropy: 0.3452 - acc: 0.8496 - val_loss: 0.4450 - val_binary_crossentropy: 0.4450 - val_acc: 0.7969\n",
      "Epoch 216/10000\n",
      "14052/14052 [==============================] - 7s 467us/step - loss: 0.3485 - binary_crossentropy: 0.3485 - acc: 0.8471 - val_loss: 0.5164 - val_binary_crossentropy: 0.5164 - val_acc: 0.7683\n",
      "Epoch 217/10000\n",
      "14052/14052 [==============================] - 7s 466us/step - loss: 0.3453 - binary_crossentropy: 0.3453 - acc: 0.8500 - val_loss: 0.4569 - val_binary_crossentropy: 0.4569 - val_acc: 0.7958\n",
      "Epoch 218/10000\n",
      "14052/14052 [==============================] - 7s 466us/step - loss: 0.3466 - binary_crossentropy: 0.3466 - acc: 0.8496 - val_loss: 0.6100 - val_binary_crossentropy: 0.6100 - val_acc: 0.7523\n",
      "Epoch 219/10000\n",
      "14052/14052 [==============================] - 7s 469us/step - loss: 0.3483 - binary_crossentropy: 0.3483 - acc: 0.8483 - val_loss: 0.4707 - val_binary_crossentropy: 0.4707 - val_acc: 0.7905\n",
      "Epoch 220/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.3436 - binary_crossentropy: 0.3436 - acc: 0.8525 - val_loss: 0.4541 - val_binary_crossentropy: 0.4541 - val_acc: 0.7980\n",
      "Epoch 221/10000\n",
      "14052/14052 [==============================] - 7s 468us/step - loss: 0.3503 - binary_crossentropy: 0.3503 - acc: 0.8460 - val_loss: 0.4580 - val_binary_crossentropy: 0.4580 - val_acc: 0.7888\n",
      "Epoch 222/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.3446 - binary_crossentropy: 0.3446 - acc: 0.8513 - val_loss: 0.4977 - val_binary_crossentropy: 0.4977 - val_acc: 0.7709\n",
      "Epoch 223/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.3406 - binary_crossentropy: 0.3406 - acc: 0.8507 - val_loss: 0.4720 - val_binary_crossentropy: 0.4720 - val_acc: 0.7933\n",
      "Epoch 224/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.3431 - binary_crossentropy: 0.3431 - acc: 0.8512 - val_loss: 0.4592 - val_binary_crossentropy: 0.4592 - val_acc: 0.7826\n",
      "Epoch 225/10000\n",
      "14052/14052 [==============================] - 7s 476us/step - loss: 0.3464 - binary_crossentropy: 0.3464 - acc: 0.8471 - val_loss: 0.5820 - val_binary_crossentropy: 0.5820 - val_acc: 0.7414\n",
      "Epoch 226/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.3414 - binary_crossentropy: 0.3414 - acc: 0.8515 - val_loss: 0.4862 - val_binary_crossentropy: 0.4862 - val_acc: 0.7862\n",
      "Epoch 227/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.3402 - binary_crossentropy: 0.3402 - acc: 0.8510 - val_loss: 0.8333 - val_binary_crossentropy: 0.8333 - val_acc: 0.6823\n",
      "Epoch 228/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.3433 - binary_crossentropy: 0.3433 - acc: 0.8526 - val_loss: 0.5615 - val_binary_crossentropy: 0.5615 - val_acc: 0.7589\n",
      "Epoch 229/10000\n",
      "14052/14052 [==============================] - 7s 471us/step - loss: 0.3448 - binary_crossentropy: 0.3448 - acc: 0.8500 - val_loss: 0.4678 - val_binary_crossentropy: 0.4678 - val_acc: 0.7984\n",
      "Epoch 230/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.3440 - binary_crossentropy: 0.3440 - acc: 0.8481 - val_loss: 0.4468 - val_binary_crossentropy: 0.4468 - val_acc: 0.7950\n",
      "Epoch 231/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.3380 - binary_crossentropy: 0.3380 - acc: 0.8467 - val_loss: 0.8759 - val_binary_crossentropy: 0.8759 - val_acc: 0.6537\n",
      "Epoch 232/10000\n",
      "14052/14052 [==============================] - 7s 474us/step - loss: 0.3387 - binary_crossentropy: 0.3387 - acc: 0.8547 - val_loss: 0.5151 - val_binary_crossentropy: 0.5151 - val_acc: 0.7600\n",
      "Epoch 233/10000\n",
      "14052/14052 [==============================] - 7s 474us/step - loss: 0.3420 - binary_crossentropy: 0.3420 - acc: 0.8490 - val_loss: 0.4690 - val_binary_crossentropy: 0.4690 - val_acc: 0.7798\n",
      "Epoch 234/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.3354 - binary_crossentropy: 0.3354 - acc: 0.8511 - val_loss: 0.4540 - val_binary_crossentropy: 0.4540 - val_acc: 0.7952\n",
      "Epoch 235/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.3403 - binary_crossentropy: 0.3403 - acc: 0.8532 - val_loss: 0.4665 - val_binary_crossentropy: 0.4665 - val_acc: 0.7916\n",
      "Epoch 236/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.3399 - binary_crossentropy: 0.3399 - acc: 0.8520 - val_loss: 0.4524 - val_binary_crossentropy: 0.4524 - val_acc: 0.7956\n",
      "Epoch 237/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.3353 - binary_crossentropy: 0.3353 - acc: 0.8590 - val_loss: 0.4466 - val_binary_crossentropy: 0.4466 - val_acc: 0.7973\n",
      "Epoch 238/10000\n",
      "14052/14052 [==============================] - 6s 457us/step - loss: 0.3354 - binary_crossentropy: 0.3354 - acc: 0.8540 - val_loss: 0.4649 - val_binary_crossentropy: 0.4649 - val_acc: 0.7980\n",
      "Epoch 239/10000\n",
      "14052/14052 [==============================] - 6s 461us/step - loss: 0.3347 - binary_crossentropy: 0.3347 - acc: 0.8542 - val_loss: 0.4682 - val_binary_crossentropy: 0.4682 - val_acc: 0.7945\n",
      "Epoch 240/10000\n",
      "14052/14052 [==============================] - 6s 457us/step - loss: 0.3363 - binary_crossentropy: 0.3363 - acc: 0.8554 - val_loss: 0.4643 - val_binary_crossentropy: 0.4643 - val_acc: 0.7935\n",
      "Epoch 241/10000\n",
      "14052/14052 [==============================] - 6s 459us/step - loss: 0.3413 - binary_crossentropy: 0.3413 - acc: 0.8491 - val_loss: 0.4626 - val_binary_crossentropy: 0.4626 - val_acc: 0.7965\n",
      "Epoch 242/10000\n",
      "14052/14052 [==============================] - 6s 459us/step - loss: 0.3391 - binary_crossentropy: 0.3391 - acc: 0.8531 - val_loss: 0.4620 - val_binary_crossentropy: 0.4620 - val_acc: 0.7856\n",
      "Epoch 243/10000\n",
      "14052/14052 [==============================] - 7s 463us/step - loss: 0.3403 - binary_crossentropy: 0.3403 - acc: 0.8519 - val_loss: 0.5079 - val_binary_crossentropy: 0.5079 - val_acc: 0.7884\n",
      "Epoch 244/10000\n",
      "14052/14052 [==============================] - 6s 462us/step - loss: 0.3323 - binary_crossentropy: 0.3323 - acc: 0.8567 - val_loss: 0.4698 - val_binary_crossentropy: 0.4698 - val_acc: 0.7852\n",
      "Epoch 245/10000\n",
      "14052/14052 [==============================] - 7s 465us/step - loss: 0.3323 - binary_crossentropy: 0.3323 - acc: 0.8535 - val_loss: 0.4631 - val_binary_crossentropy: 0.4631 - val_acc: 0.7933\n",
      "Epoch 246/10000\n",
      "14052/14052 [==============================] - 6s 463us/step - loss: 0.3362 - binary_crossentropy: 0.3362 - acc: 0.8531 - val_loss: 0.5603 - val_binary_crossentropy: 0.5603 - val_acc: 0.7610\n",
      "Epoch 247/10000\n",
      "14052/14052 [==============================] - 7s 467us/step - loss: 0.3378 - binary_crossentropy: 0.3378 - acc: 0.8520 - val_loss: 0.5089 - val_binary_crossentropy: 0.5089 - val_acc: 0.7749\n",
      "Epoch 248/10000\n",
      "14052/14052 [==============================] - 7s 466us/step - loss: 0.3284 - binary_crossentropy: 0.3284 - acc: 0.8608 - val_loss: 0.4514 - val_binary_crossentropy: 0.4514 - val_acc: 0.7967\n",
      "Epoch 249/10000\n",
      "14052/14052 [==============================] - 7s 466us/step - loss: 0.3418 - binary_crossentropy: 0.3418 - acc: 0.8530 - val_loss: 0.4685 - val_binary_crossentropy: 0.4685 - val_acc: 0.7901\n",
      "Epoch 250/10000\n",
      "14052/14052 [==============================] - 7s 467us/step - loss: 0.3361 - binary_crossentropy: 0.3361 - acc: 0.8550 - val_loss: 0.4501 - val_binary_crossentropy: 0.4501 - val_acc: 0.7928\n",
      "Epoch 251/10000\n",
      "14052/14052 [==============================] - 7s 468us/step - loss: 0.3313 - binary_crossentropy: 0.3313 - acc: 0.8584 - val_loss: 0.4643 - val_binary_crossentropy: 0.4643 - val_acc: 0.7866\n",
      "Epoch 252/10000\n",
      "14052/14052 [==============================] - 7s 467us/step - loss: 0.3335 - binary_crossentropy: 0.3335 - acc: 0.8542 - val_loss: 0.4586 - val_binary_crossentropy: 0.4586 - val_acc: 0.7971\n",
      "Epoch 253/10000\n",
      "14052/14052 [==============================] - 7s 468us/step - loss: 0.3274 - binary_crossentropy: 0.3274 - acc: 0.8575 - val_loss: 0.4986 - val_binary_crossentropy: 0.4986 - val_acc: 0.7702\n",
      "Epoch 254/10000\n",
      "14052/14052 [==============================] - 7s 467us/step - loss: 0.3308 - binary_crossentropy: 0.3308 - acc: 0.8566 - val_loss: 0.4947 - val_binary_crossentropy: 0.4947 - val_acc: 0.7828\n",
      "Epoch 255/10000\n",
      "14052/14052 [==============================] - 7s 466us/step - loss: 0.3328 - binary_crossentropy: 0.3328 - acc: 0.8550 - val_loss: 0.4687 - val_binary_crossentropy: 0.4687 - val_acc: 0.7933\n",
      "Epoch 256/10000\n",
      "14052/14052 [==============================] - 7s 468us/step - loss: 0.3359 - binary_crossentropy: 0.3359 - acc: 0.8586 - val_loss: 0.5024 - val_binary_crossentropy: 0.5024 - val_acc: 0.7777\n",
      "Epoch 257/10000\n",
      "14052/14052 [==============================] - 7s 497us/step - loss: 0.3264 - binary_crossentropy: 0.3264 - acc: 0.8575 - val_loss: 0.4694 - val_binary_crossentropy: 0.4694 - val_acc: 0.7873\n",
      "Epoch 258/10000\n",
      "14052/14052 [==============================] - 7s 469us/step - loss: 0.3283 - binary_crossentropy: 0.3283 - acc: 0.8626 - val_loss: 0.4785 - val_binary_crossentropy: 0.4785 - val_acc: 0.7926\n",
      "Epoch 259/10000\n",
      "14052/14052 [==============================] - 7s 471us/step - loss: 0.3290 - binary_crossentropy: 0.3290 - acc: 0.8577 - val_loss: 2.8323 - val_binary_crossentropy: 2.8323 - val_acc: 0.5515\n",
      "Epoch 260/10000\n",
      "14052/14052 [==============================] - 7s 482us/step - loss: 0.3500 - binary_crossentropy: 0.3500 - acc: 0.8482 - val_loss: 0.4562 - val_binary_crossentropy: 0.4562 - val_acc: 0.7948\n",
      "Epoch 261/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.3352 - binary_crossentropy: 0.3352 - acc: 0.8570 - val_loss: 0.4746 - val_binary_crossentropy: 0.4746 - val_acc: 0.7871\n",
      "Epoch 262/10000\n",
      "14052/14052 [==============================] - 7s 474us/step - loss: 0.3298 - binary_crossentropy: 0.3298 - acc: 0.8566 - val_loss: 0.4824 - val_binary_crossentropy: 0.4824 - val_acc: 0.7798\n",
      "Epoch 263/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.3312 - binary_crossentropy: 0.3312 - acc: 0.8553 - val_loss: 0.4631 - val_binary_crossentropy: 0.4631 - val_acc: 0.7933\n",
      "Epoch 264/10000\n",
      "14052/14052 [==============================] - 7s 469us/step - loss: 0.3290 - binary_crossentropy: 0.3290 - acc: 0.8559 - val_loss: 0.4931 - val_binary_crossentropy: 0.4931 - val_acc: 0.7773\n",
      "Epoch 265/10000\n",
      "14052/14052 [==============================] - 7s 471us/step - loss: 0.3262 - binary_crossentropy: 0.3262 - acc: 0.8614 - val_loss: 0.7036 - val_binary_crossentropy: 0.7036 - val_acc: 0.7004\n",
      "Epoch 266/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.3255 - binary_crossentropy: 0.3255 - acc: 0.8584 - val_loss: 0.4917 - val_binary_crossentropy: 0.4917 - val_acc: 0.7813\n",
      "Epoch 267/10000\n",
      "14052/14052 [==============================] - 7s 471us/step - loss: 0.3231 - binary_crossentropy: 0.3231 - acc: 0.8597 - val_loss: 0.4638 - val_binary_crossentropy: 0.4638 - val_acc: 0.7956\n",
      "Epoch 268/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.3243 - binary_crossentropy: 0.3243 - acc: 0.8617 - val_loss: 0.4734 - val_binary_crossentropy: 0.4734 - val_acc: 0.7864\n",
      "Epoch 269/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.3246 - binary_crossentropy: 0.3246 - acc: 0.8594 - val_loss: 0.4652 - val_binary_crossentropy: 0.4652 - val_acc: 0.7971\n",
      "Epoch 270/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.3241 - binary_crossentropy: 0.3241 - acc: 0.8590 - val_loss: 0.4800 - val_binary_crossentropy: 0.4800 - val_acc: 0.7892\n",
      "Epoch 271/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.3231 - binary_crossentropy: 0.3231 - acc: 0.8595 - val_loss: 0.4645 - val_binary_crossentropy: 0.4645 - val_acc: 0.7992\n",
      "Epoch 272/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.3253 - binary_crossentropy: 0.3253 - acc: 0.8585 - val_loss: 0.5201 - val_binary_crossentropy: 0.5201 - val_acc: 0.7770\n",
      "Epoch 273/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.3254 - binary_crossentropy: 0.3254 - acc: 0.8609 - val_loss: 0.4608 - val_binary_crossentropy: 0.4608 - val_acc: 0.7962\n",
      "Epoch 274/10000\n",
      "14052/14052 [==============================] - 7s 476us/step - loss: 0.3211 - binary_crossentropy: 0.3211 - acc: 0.8587 - val_loss: 0.4638 - val_binary_crossentropy: 0.4638 - val_acc: 0.7901\n",
      "Epoch 275/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.3202 - binary_crossentropy: 0.3202 - acc: 0.8619 - val_loss: 0.5222 - val_binary_crossentropy: 0.5222 - val_acc: 0.7820\n",
      "Epoch 276/10000\n",
      "14052/14052 [==============================] - 7s 476us/step - loss: 0.3201 - binary_crossentropy: 0.3201 - acc: 0.8624 - val_loss: 0.4550 - val_binary_crossentropy: 0.4550 - val_acc: 0.7937\n",
      "Epoch 277/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.3184 - binary_crossentropy: 0.3184 - acc: 0.8643 - val_loss: 0.4629 - val_binary_crossentropy: 0.4629 - val_acc: 0.7930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 278/10000\n",
      "14052/14052 [==============================] - 6s 458us/step - loss: 0.3255 - binary_crossentropy: 0.3255 - acc: 0.8606 - val_loss: 0.4563 - val_binary_crossentropy: 0.4563 - val_acc: 0.7898\n",
      "Epoch 279/10000\n",
      "14052/14052 [==============================] - 6s 458us/step - loss: 0.3136 - binary_crossentropy: 0.3136 - acc: 0.8644 - val_loss: 0.4956 - val_binary_crossentropy: 0.4956 - val_acc: 0.7802\n",
      "Epoch 280/10000\n",
      "14052/14052 [==============================] - 6s 461us/step - loss: 0.3200 - binary_crossentropy: 0.3200 - acc: 0.8601 - val_loss: 0.4853 - val_binary_crossentropy: 0.4853 - val_acc: 0.7841\n",
      "Epoch 281/10000\n",
      "14052/14052 [==============================] - 7s 465us/step - loss: 0.3228 - binary_crossentropy: 0.3228 - acc: 0.8582 - val_loss: 0.4592 - val_binary_crossentropy: 0.4592 - val_acc: 0.7939\n",
      "Epoch 282/10000\n",
      "14052/14052 [==============================] - 6s 460us/step - loss: 0.3218 - binary_crossentropy: 0.3218 - acc: 0.8640 - val_loss: 0.4747 - val_binary_crossentropy: 0.4747 - val_acc: 0.7881\n",
      "Epoch 283/10000\n",
      "14052/14052 [==============================] - 6s 460us/step - loss: 0.3193 - binary_crossentropy: 0.3193 - acc: 0.8595 - val_loss: 0.4779 - val_binary_crossentropy: 0.4779 - val_acc: 0.7926\n",
      "Epoch 284/10000\n",
      "14052/14052 [==============================] - 6s 460us/step - loss: 0.3194 - binary_crossentropy: 0.3194 - acc: 0.8641 - val_loss: 0.4716 - val_binary_crossentropy: 0.4716 - val_acc: 0.7918\n",
      "Epoch 285/10000\n",
      "14052/14052 [==============================] - 6s 462us/step - loss: 0.3185 - binary_crossentropy: 0.3185 - acc: 0.8645 - val_loss: 0.4747 - val_binary_crossentropy: 0.4747 - val_acc: 0.7884\n",
      "Epoch 286/10000\n",
      "14052/14052 [==============================] - 7s 463us/step - loss: 0.3148 - binary_crossentropy: 0.3148 - acc: 0.8661 - val_loss: 0.6725 - val_binary_crossentropy: 0.6725 - val_acc: 0.7312\n",
      "Epoch 287/10000\n",
      "14052/14052 [==============================] - 7s 467us/step - loss: 0.3189 - binary_crossentropy: 0.3189 - acc: 0.8620 - val_loss: 0.8699 - val_binary_crossentropy: 0.8699 - val_acc: 0.6906\n",
      "Epoch 288/10000\n",
      "14052/14052 [==============================] - 7s 466us/step - loss: 0.3227 - binary_crossentropy: 0.3227 - acc: 0.8651 - val_loss: 0.4750 - val_binary_crossentropy: 0.4750 - val_acc: 0.7884\n",
      "Epoch 289/10000\n",
      "14052/14052 [==============================] - 7s 466us/step - loss: 0.3148 - binary_crossentropy: 0.3148 - acc: 0.8644 - val_loss: 0.6057 - val_binary_crossentropy: 0.6057 - val_acc: 0.7506\n",
      "Epoch 290/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.3184 - binary_crossentropy: 0.3184 - acc: 0.8614 - val_loss: 0.4682 - val_binary_crossentropy: 0.4682 - val_acc: 0.7890\n",
      "Epoch 291/10000\n",
      "14052/14052 [==============================] - 7s 466us/step - loss: 0.3193 - binary_crossentropy: 0.3193 - acc: 0.8644 - val_loss: 0.5091 - val_binary_crossentropy: 0.5091 - val_acc: 0.7890\n",
      "Epoch 292/10000\n",
      "14052/14052 [==============================] - 7s 467us/step - loss: 0.3100 - binary_crossentropy: 0.3100 - acc: 0.8671 - val_loss: 0.4750 - val_binary_crossentropy: 0.4750 - val_acc: 0.7834\n",
      "Epoch 293/10000\n",
      "14052/14052 [==============================] - 7s 466us/step - loss: 0.3132 - binary_crossentropy: 0.3132 - acc: 0.8635 - val_loss: 0.5142 - val_binary_crossentropy: 0.5142 - val_acc: 0.7736\n",
      "Epoch 294/10000\n",
      "14052/14052 [==============================] - 7s 466us/step - loss: 0.3052 - binary_crossentropy: 0.3052 - acc: 0.8733 - val_loss: 0.4724 - val_binary_crossentropy: 0.4724 - val_acc: 0.7909\n",
      "Epoch 295/10000\n",
      "14052/14052 [==============================] - 7s 468us/step - loss: 0.3165 - binary_crossentropy: 0.3165 - acc: 0.8624 - val_loss: 0.5904 - val_binary_crossentropy: 0.5904 - val_acc: 0.7497\n",
      "Epoch 296/10000\n",
      "14052/14052 [==============================] - 7s 468us/step - loss: 0.3096 - binary_crossentropy: 0.3096 - acc: 0.8654 - val_loss: 0.5139 - val_binary_crossentropy: 0.5139 - val_acc: 0.7820\n",
      "Epoch 297/10000\n",
      "14052/14052 [==============================] - 7s 469us/step - loss: 0.3050 - binary_crossentropy: 0.3050 - acc: 0.8722 - val_loss: 0.5158 - val_binary_crossentropy: 0.5158 - val_acc: 0.7512\n",
      "Epoch 298/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.3278 - binary_crossentropy: 0.3278 - acc: 0.8562 - val_loss: 0.4779 - val_binary_crossentropy: 0.4779 - val_acc: 0.7849\n",
      "Epoch 299/10000\n",
      "14052/14052 [==============================] - 7s 469us/step - loss: 0.3204 - binary_crossentropy: 0.3204 - acc: 0.8644 - val_loss: 0.4628 - val_binary_crossentropy: 0.4628 - val_acc: 0.7939\n",
      "Epoch 300/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.3135 - binary_crossentropy: 0.3135 - acc: 0.8648 - val_loss: 0.4943 - val_binary_crossentropy: 0.4943 - val_acc: 0.7937\n",
      "Epoch 301/10000\n",
      "14052/14052 [==============================] - 7s 469us/step - loss: 0.3122 - binary_crossentropy: 0.3122 - acc: 0.8683 - val_loss: 1.1808 - val_binary_crossentropy: 1.1808 - val_acc: 0.6343\n",
      "Epoch 302/10000\n",
      "14052/14052 [==============================] - 7s 469us/step - loss: 0.3268 - binary_crossentropy: 0.3268 - acc: 0.8614 - val_loss: 0.4788 - val_binary_crossentropy: 0.4788 - val_acc: 0.7962\n",
      "Epoch 303/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.3194 - binary_crossentropy: 0.3194 - acc: 0.8642 - val_loss: 0.5128 - val_binary_crossentropy: 0.5128 - val_acc: 0.7706\n",
      "Epoch 304/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.3150 - binary_crossentropy: 0.3150 - acc: 0.8641 - val_loss: 0.4654 - val_binary_crossentropy: 0.4654 - val_acc: 0.7916\n",
      "Epoch 305/10000\n",
      "14052/14052 [==============================] - 7s 471us/step - loss: 0.3110 - binary_crossentropy: 0.3110 - acc: 0.8683 - val_loss: 0.4738 - val_binary_crossentropy: 0.4738 - val_acc: 0.7941\n",
      "Epoch 306/10000\n",
      "14052/14052 [==============================] - 7s 474us/step - loss: 0.3135 - binary_crossentropy: 0.3135 - acc: 0.8672 - val_loss: 0.6339 - val_binary_crossentropy: 0.6339 - val_acc: 0.7335\n",
      "Epoch 307/10000\n",
      "14052/14052 [==============================] - 7s 475us/step - loss: 0.3140 - binary_crossentropy: 0.3140 - acc: 0.8638 - val_loss: 0.4812 - val_binary_crossentropy: 0.4812 - val_acc: 0.7930\n",
      "Epoch 308/10000\n",
      "14052/14052 [==============================] - 7s 474us/step - loss: 0.3104 - binary_crossentropy: 0.3104 - acc: 0.8656 - val_loss: 0.5911 - val_binary_crossentropy: 0.5911 - val_acc: 0.7453\n",
      "Epoch 309/10000\n",
      "14052/14052 [==============================] - 7s 474us/step - loss: 0.3111 - binary_crossentropy: 0.3111 - acc: 0.8655 - val_loss: 0.4961 - val_binary_crossentropy: 0.4961 - val_acc: 0.7869\n",
      "Epoch 310/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.3127 - binary_crossentropy: 0.3127 - acc: 0.8636 - val_loss: 0.4913 - val_binary_crossentropy: 0.4913 - val_acc: 0.7884\n",
      "Epoch 311/10000\n",
      "14052/14052 [==============================] - 7s 491us/step - loss: 0.3132 - binary_crossentropy: 0.3132 - acc: 0.8658 - val_loss: 0.4872 - val_binary_crossentropy: 0.4872 - val_acc: 0.7907\n",
      "Epoch 312/10000\n",
      "14052/14052 [==============================] - 7s 474us/step - loss: 0.3074 - binary_crossentropy: 0.3074 - acc: 0.8676 - val_loss: 0.5424 - val_binary_crossentropy: 0.5424 - val_acc: 0.7822\n",
      "Epoch 313/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.3084 - binary_crossentropy: 0.3084 - acc: 0.8671 - val_loss: 0.4703 - val_binary_crossentropy: 0.4703 - val_acc: 0.7924\n",
      "Epoch 314/10000\n",
      "14052/14052 [==============================] - 7s 489us/step - loss: 0.3127 - binary_crossentropy: 0.3127 - acc: 0.8666 - val_loss: 0.4825 - val_binary_crossentropy: 0.4825 - val_acc: 0.7898\n",
      "Epoch 315/10000\n",
      "14052/14052 [==============================] - 7s 487us/step - loss: 0.3130 - binary_crossentropy: 0.3130 - acc: 0.8702 - val_loss: 0.4802 - val_binary_crossentropy: 0.4802 - val_acc: 0.7888\n",
      "Epoch 316/10000\n",
      "14052/14052 [==============================] - 7s 478us/step - loss: 0.3023 - binary_crossentropy: 0.3023 - acc: 0.8710 - val_loss: 0.6440 - val_binary_crossentropy: 0.6440 - val_acc: 0.7512\n",
      "Epoch 317/10000\n",
      "14052/14052 [==============================] - 6s 460us/step - loss: 0.3044 - binary_crossentropy: 0.3044 - acc: 0.8686 - val_loss: 0.4744 - val_binary_crossentropy: 0.4744 - val_acc: 0.7956\n",
      "Epoch 318/10000\n",
      "14052/14052 [==============================] - 6s 458us/step - loss: 0.3080 - binary_crossentropy: 0.3080 - acc: 0.8680 - val_loss: 0.4903 - val_binary_crossentropy: 0.4903 - val_acc: 0.7805\n",
      "Epoch 319/10000\n",
      "14052/14052 [==============================] - 6s 457us/step - loss: 0.3027 - binary_crossentropy: 0.3027 - acc: 0.8708 - val_loss: 0.5308 - val_binary_crossentropy: 0.5308 - val_acc: 0.7734\n",
      "Epoch 320/10000\n",
      "14052/14052 [==============================] - 7s 464us/step - loss: 0.3080 - binary_crossentropy: 0.3080 - acc: 0.8688 - val_loss: 0.4870 - val_binary_crossentropy: 0.4870 - val_acc: 0.7837\n",
      "Epoch 321/10000\n",
      "14052/14052 [==============================] - 6s 459us/step - loss: 0.3004 - binary_crossentropy: 0.3004 - acc: 0.8725 - val_loss: 0.4938 - val_binary_crossentropy: 0.4938 - val_acc: 0.7841\n",
      "Epoch 322/10000\n",
      "14052/14052 [==============================] - 6s 461us/step - loss: 0.3051 - binary_crossentropy: 0.3051 - acc: 0.8699 - val_loss: 0.7890 - val_binary_crossentropy: 0.7890 - val_acc: 0.7120\n",
      "Epoch 323/10000\n",
      "14052/14052 [==============================] - 7s 464us/step - loss: 0.3072 - binary_crossentropy: 0.3072 - acc: 0.8698 - val_loss: 0.5367 - val_binary_crossentropy: 0.5367 - val_acc: 0.7751\n",
      "Epoch 324/10000\n",
      "14052/14052 [==============================] - 6s 462us/step - loss: 0.3033 - binary_crossentropy: 0.3033 - acc: 0.8713 - val_loss: 0.4956 - val_binary_crossentropy: 0.4956 - val_acc: 0.7922\n",
      "Epoch 325/10000\n",
      "14052/14052 [==============================] - 7s 464us/step - loss: 0.3143 - binary_crossentropy: 0.3143 - acc: 0.8648 - val_loss: 0.4794 - val_binary_crossentropy: 0.4794 - val_acc: 0.7907\n",
      "Epoch 326/10000\n",
      "14052/14052 [==============================] - 7s 466us/step - loss: 0.3041 - binary_crossentropy: 0.3041 - acc: 0.8690 - val_loss: 0.4790 - val_binary_crossentropy: 0.4790 - val_acc: 0.7920\n",
      "Epoch 327/10000\n",
      "14052/14052 [==============================] - 7s 466us/step - loss: 0.3069 - binary_crossentropy: 0.3069 - acc: 0.8654 - val_loss: 0.4825 - val_binary_crossentropy: 0.4825 - val_acc: 0.7903\n",
      "Epoch 328/10000\n",
      "14052/14052 [==============================] - 7s 465us/step - loss: 0.2941 - binary_crossentropy: 0.2941 - acc: 0.8749 - val_loss: 0.5001 - val_binary_crossentropy: 0.5001 - val_acc: 0.7864\n",
      "Epoch 329/10000\n",
      "14052/14052 [==============================] - 7s 468us/step - loss: 0.3020 - binary_crossentropy: 0.3020 - acc: 0.8698 - val_loss: 0.4917 - val_binary_crossentropy: 0.4917 - val_acc: 0.7843\n",
      "Epoch 330/10000\n",
      "14052/14052 [==============================] - 7s 467us/step - loss: 0.2991 - binary_crossentropy: 0.2991 - acc: 0.8740 - val_loss: 0.4882 - val_binary_crossentropy: 0.4882 - val_acc: 0.7901\n",
      "Epoch 331/10000\n",
      "14052/14052 [==============================] - 7s 469us/step - loss: 0.3020 - binary_crossentropy: 0.3020 - acc: 0.8725 - val_loss: 0.5069 - val_binary_crossentropy: 0.5069 - val_acc: 0.7898\n",
      "Epoch 332/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.3011 - binary_crossentropy: 0.3011 - acc: 0.8698 - val_loss: 1.0898 - val_binary_crossentropy: 1.0898 - val_acc: 0.6774\n",
      "Epoch 333/10000\n",
      "14052/14052 [==============================] - 7s 468us/step - loss: 0.2987 - binary_crossentropy: 0.2987 - acc: 0.8748 - val_loss: 0.5127 - val_binary_crossentropy: 0.5127 - val_acc: 0.7764\n",
      "Epoch 334/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.3050 - binary_crossentropy: 0.3050 - acc: 0.8706 - val_loss: 0.4790 - val_binary_crossentropy: 0.4790 - val_acc: 0.7901\n",
      "Epoch 335/10000\n",
      "14052/14052 [==============================] - 7s 471us/step - loss: 0.2995 - binary_crossentropy: 0.2995 - acc: 0.8731 - val_loss: 0.5177 - val_binary_crossentropy: 0.5177 - val_acc: 0.7815\n",
      "Epoch 336/10000\n",
      "14052/14052 [==============================] - 7s 469us/step - loss: 0.3025 - binary_crossentropy: 0.3025 - acc: 0.8724 - val_loss: 0.5048 - val_binary_crossentropy: 0.5048 - val_acc: 0.7820\n",
      "Epoch 337/10000\n",
      "14052/14052 [==============================] - 7s 471us/step - loss: 0.2970 - binary_crossentropy: 0.2970 - acc: 0.8735 - val_loss: 0.7726 - val_binary_crossentropy: 0.7726 - val_acc: 0.7184\n",
      "Epoch 338/10000\n",
      "14052/14052 [==============================] - 7s 471us/step - loss: 0.2973 - binary_crossentropy: 0.2973 - acc: 0.8736 - val_loss: 0.4982 - val_binary_crossentropy: 0.4982 - val_acc: 0.7922\n",
      "Epoch 339/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.2938 - binary_crossentropy: 0.2938 - acc: 0.8775 - val_loss: 0.5678 - val_binary_crossentropy: 0.5678 - val_acc: 0.7689\n",
      "Epoch 340/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.3016 - binary_crossentropy: 0.3016 - acc: 0.8736 - val_loss: 0.4747 - val_binary_crossentropy: 0.4747 - val_acc: 0.7907\n",
      "Epoch 341/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.3001 - binary_crossentropy: 0.3001 - acc: 0.8772 - val_loss: 0.7917 - val_binary_crossentropy: 0.7917 - val_acc: 0.6940\n",
      "Epoch 342/10000\n",
      "14052/14052 [==============================] - 7s 476us/step - loss: 0.3004 - binary_crossentropy: 0.3004 - acc: 0.8723 - val_loss: 0.4863 - val_binary_crossentropy: 0.4863 - val_acc: 0.7894\n",
      "Epoch 343/10000\n",
      "14052/14052 [==============================] - 7s 471us/step - loss: 0.2940 - binary_crossentropy: 0.2940 - acc: 0.8736 - val_loss: 0.6041 - val_binary_crossentropy: 0.6041 - val_acc: 0.7630\n",
      "Epoch 344/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.2923 - binary_crossentropy: 0.2923 - acc: 0.8715 - val_loss: 0.5320 - val_binary_crossentropy: 0.5320 - val_acc: 0.7830\n",
      "Epoch 345/10000\n",
      "14052/14052 [==============================] - 7s 471us/step - loss: 0.2985 - binary_crossentropy: 0.2985 - acc: 0.8726 - val_loss: 0.4947 - val_binary_crossentropy: 0.4947 - val_acc: 0.7907\n",
      "Epoch 346/10000\n",
      "14052/14052 [==============================] - 7s 471us/step - loss: 0.2994 - binary_crossentropy: 0.2994 - acc: 0.8709 - val_loss: 0.5503 - val_binary_crossentropy: 0.5503 - val_acc: 0.7732\n",
      "Epoch 347/10000\n",
      "14052/14052 [==============================] - 7s 487us/step - loss: 0.2948 - binary_crossentropy: 0.2948 - acc: 0.8789 - val_loss: 0.4855 - val_binary_crossentropy: 0.4855 - val_acc: 0.7958\n",
      "Epoch 348/10000\n",
      "14052/14052 [==============================] - 7s 471us/step - loss: 0.2956 - binary_crossentropy: 0.2956 - acc: 0.8739 - val_loss: 0.5002 - val_binary_crossentropy: 0.5002 - val_acc: 0.7911\n",
      "Epoch 349/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.2910 - binary_crossentropy: 0.2910 - acc: 0.8755 - val_loss: 0.5009 - val_binary_crossentropy: 0.5009 - val_acc: 0.7916\n",
      "Epoch 350/10000\n",
      "14052/14052 [==============================] - 7s 474us/step - loss: 0.2972 - binary_crossentropy: 0.2972 - acc: 0.8715 - val_loss: 0.5000 - val_binary_crossentropy: 0.5000 - val_acc: 0.7911\n",
      "Epoch 351/10000\n",
      "14052/14052 [==============================] - 7s 474us/step - loss: 0.2994 - binary_crossentropy: 0.2994 - acc: 0.8755 - val_loss: 0.5804 - val_binary_crossentropy: 0.5804 - val_acc: 0.7557\n",
      "Epoch 352/10000\n",
      "14052/14052 [==============================] - 7s 477us/step - loss: 0.2911 - binary_crossentropy: 0.2911 - acc: 0.8776 - val_loss: 0.5222 - val_binary_crossentropy: 0.5222 - val_acc: 0.7395\n",
      "Epoch 353/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.3142 - binary_crossentropy: 0.3142 - acc: 0.8680 - val_loss: 0.5818 - val_binary_crossentropy: 0.5818 - val_acc: 0.7472\n",
      "Epoch 354/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.3049 - binary_crossentropy: 0.3049 - acc: 0.8680 - val_loss: 0.5912 - val_binary_crossentropy: 0.5912 - val_acc: 0.7510\n",
      "Epoch 355/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.3041 - binary_crossentropy: 0.3041 - acc: 0.8692 - val_loss: 0.4875 - val_binary_crossentropy: 0.4875 - val_acc: 0.7884\n",
      "Epoch 356/10000\n",
      "14052/14052 [==============================] - 7s 478us/step - loss: 0.3023 - binary_crossentropy: 0.3023 - acc: 0.8708 - val_loss: 0.4924 - val_binary_crossentropy: 0.4924 - val_acc: 0.7898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 357/10000\n",
      "14052/14052 [==============================] - 6s 458us/step - loss: 0.2933 - binary_crossentropy: 0.2933 - acc: 0.8789 - val_loss: 0.5151 - val_binary_crossentropy: 0.5151 - val_acc: 0.7926\n",
      "Epoch 358/10000\n",
      "14052/14052 [==============================] - 6s 458us/step - loss: 0.2917 - binary_crossentropy: 0.2917 - acc: 0.8788 - val_loss: 0.5516 - val_binary_crossentropy: 0.5516 - val_acc: 0.7826\n",
      "Epoch 359/10000\n",
      "14052/14052 [==============================] - 6s 459us/step - loss: 0.2929 - binary_crossentropy: 0.2929 - acc: 0.8750 - val_loss: 0.5307 - val_binary_crossentropy: 0.5307 - val_acc: 0.7768\n",
      "Epoch 360/10000\n",
      "14052/14052 [==============================] - 6s 461us/step - loss: 0.3005 - binary_crossentropy: 0.3005 - acc: 0.8735 - val_loss: 0.5026 - val_binary_crossentropy: 0.5026 - val_acc: 0.7873\n",
      "Epoch 361/10000\n",
      "14052/14052 [==============================] - 7s 464us/step - loss: 0.2874 - binary_crossentropy: 0.2874 - acc: 0.8781 - val_loss: 0.5962 - val_binary_crossentropy: 0.5962 - val_acc: 0.7683\n",
      "Epoch 362/10000\n",
      "14052/14052 [==============================] - 6s 462us/step - loss: 0.2947 - binary_crossentropy: 0.2947 - acc: 0.8738 - val_loss: 0.5603 - val_binary_crossentropy: 0.5603 - val_acc: 0.7760\n",
      "Epoch 363/10000\n",
      "14052/14052 [==============================] - 6s 460us/step - loss: 0.2935 - binary_crossentropy: 0.2935 - acc: 0.8765 - val_loss: 0.5159 - val_binary_crossentropy: 0.5159 - val_acc: 0.7839\n",
      "Epoch 364/10000\n",
      "14052/14052 [==============================] - 6s 461us/step - loss: 0.2953 - binary_crossentropy: 0.2953 - acc: 0.8733 - val_loss: 0.5050 - val_binary_crossentropy: 0.5050 - val_acc: 0.7879\n",
      "Epoch 365/10000\n",
      "14052/14052 [==============================] - 7s 464us/step - loss: 0.2918 - binary_crossentropy: 0.2918 - acc: 0.8751 - val_loss: 0.5413 - val_binary_crossentropy: 0.5413 - val_acc: 0.7578\n",
      "Epoch 366/10000\n",
      "14052/14052 [==============================] - 7s 464us/step - loss: 0.3071 - binary_crossentropy: 0.3071 - acc: 0.8679 - val_loss: 0.4891 - val_binary_crossentropy: 0.4891 - val_acc: 0.7886\n",
      "Epoch 367/10000\n",
      "14052/14052 [==============================] - 7s 463us/step - loss: 0.3012 - binary_crossentropy: 0.3012 - acc: 0.8738 - val_loss: 0.5078 - val_binary_crossentropy: 0.5078 - val_acc: 0.7822\n",
      "Epoch 368/10000\n",
      "14052/14052 [==============================] - 7s 467us/step - loss: 0.2984 - binary_crossentropy: 0.2984 - acc: 0.8744 - val_loss: 0.4848 - val_binary_crossentropy: 0.4848 - val_acc: 0.7877\n",
      "Epoch 369/10000\n",
      "14052/14052 [==============================] - 7s 468us/step - loss: 0.2968 - binary_crossentropy: 0.2968 - acc: 0.8734 - val_loss: 0.4861 - val_binary_crossentropy: 0.4861 - val_acc: 0.7869\n",
      "Epoch 370/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.2989 - binary_crossentropy: 0.2989 - acc: 0.8733 - val_loss: 0.4951 - val_binary_crossentropy: 0.4951 - val_acc: 0.7854\n",
      "Epoch 371/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.2927 - binary_crossentropy: 0.2927 - acc: 0.8754 - val_loss: 0.5650 - val_binary_crossentropy: 0.5650 - val_acc: 0.7662\n",
      "Epoch 372/10000\n",
      "14052/14052 [==============================] - 7s 467us/step - loss: 0.2916 - binary_crossentropy: 0.2916 - acc: 0.8809 - val_loss: 0.5122 - val_binary_crossentropy: 0.5122 - val_acc: 0.7928\n",
      "Epoch 373/10000\n",
      "14052/14052 [==============================] - 7s 466us/step - loss: 0.2950 - binary_crossentropy: 0.2950 - acc: 0.8784 - val_loss: 0.5521 - val_binary_crossentropy: 0.5521 - val_acc: 0.7374\n",
      "Epoch 374/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.2950 - binary_crossentropy: 0.2950 - acc: 0.8749 - val_loss: 0.4986 - val_binary_crossentropy: 0.4986 - val_acc: 0.7809\n",
      "Epoch 375/10000\n",
      "14052/14052 [==============================] - 7s 469us/step - loss: 0.2869 - binary_crossentropy: 0.2869 - acc: 0.8784 - val_loss: 0.6745 - val_binary_crossentropy: 0.6745 - val_acc: 0.7145\n",
      "Epoch 376/10000\n",
      "14052/14052 [==============================] - 7s 471us/step - loss: 0.2887 - binary_crossentropy: 0.2887 - acc: 0.8792 - val_loss: 0.5119 - val_binary_crossentropy: 0.5119 - val_acc: 0.7858\n",
      "Epoch 377/10000\n",
      "14052/14052 [==============================] - 7s 467us/step - loss: 0.2890 - binary_crossentropy: 0.2890 - acc: 0.8772 - val_loss: 0.4998 - val_binary_crossentropy: 0.4998 - val_acc: 0.7856\n",
      "Epoch 378/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.2873 - binary_crossentropy: 0.2873 - acc: 0.8775 - val_loss: 0.5024 - val_binary_crossentropy: 0.5024 - val_acc: 0.7798\n",
      "Epoch 379/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.2925 - binary_crossentropy: 0.2925 - acc: 0.8760 - val_loss: 0.4887 - val_binary_crossentropy: 0.4887 - val_acc: 0.7924\n",
      "Epoch 380/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.2864 - binary_crossentropy: 0.2864 - acc: 0.8802 - val_loss: 0.6143 - val_binary_crossentropy: 0.6143 - val_acc: 0.7448\n",
      "Epoch 381/10000\n",
      "14052/14052 [==============================] - 7s 476us/step - loss: 0.3031 - binary_crossentropy: 0.3031 - acc: 0.8729 - val_loss: 0.4785 - val_binary_crossentropy: 0.4785 - val_acc: 0.7901\n",
      "Epoch 382/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.2902 - binary_crossentropy: 0.2902 - acc: 0.8765 - val_loss: 0.4811 - val_binary_crossentropy: 0.4811 - val_acc: 0.7879\n",
      "Epoch 383/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.2883 - binary_crossentropy: 0.2883 - acc: 0.8787 - val_loss: 0.5231 - val_binary_crossentropy: 0.5231 - val_acc: 0.7847\n",
      "Epoch 384/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.2850 - binary_crossentropy: 0.2850 - acc: 0.8812 - val_loss: 0.4965 - val_binary_crossentropy: 0.4965 - val_acc: 0.7943\n",
      "Epoch 385/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.2828 - binary_crossentropy: 0.2828 - acc: 0.8797 - val_loss: 0.5189 - val_binary_crossentropy: 0.5189 - val_acc: 0.7845\n",
      "Epoch 386/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.2787 - binary_crossentropy: 0.2787 - acc: 0.8831 - val_loss: 0.5073 - val_binary_crossentropy: 0.5073 - val_acc: 0.7843\n",
      "Epoch 387/10000\n",
      "14052/14052 [==============================] - 7s 482us/step - loss: 0.2810 - binary_crossentropy: 0.2810 - acc: 0.8796 - val_loss: 0.4956 - val_binary_crossentropy: 0.4956 - val_acc: 0.7875\n",
      "Epoch 388/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.2891 - binary_crossentropy: 0.2891 - acc: 0.8785 - val_loss: 0.4961 - val_binary_crossentropy: 0.4961 - val_acc: 0.7805\n",
      "Epoch 389/10000\n",
      "14052/14052 [==============================] - 7s 473us/step - loss: 0.2898 - binary_crossentropy: 0.2898 - acc: 0.8798 - val_loss: 0.5128 - val_binary_crossentropy: 0.5128 - val_acc: 0.7913\n",
      "Epoch 390/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.2888 - binary_crossentropy: 0.2888 - acc: 0.8781 - val_loss: 0.5021 - val_binary_crossentropy: 0.5021 - val_acc: 0.7781\n",
      "Epoch 391/10000\n",
      "14052/14052 [==============================] - 7s 471us/step - loss: 0.2851 - binary_crossentropy: 0.2851 - acc: 0.8785 - val_loss: 0.5036 - val_binary_crossentropy: 0.5036 - val_acc: 0.7909\n",
      "Epoch 392/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.2836 - binary_crossentropy: 0.2836 - acc: 0.8774 - val_loss: 0.6306 - val_binary_crossentropy: 0.6306 - val_acc: 0.7587\n",
      "Epoch 393/10000\n",
      "14052/14052 [==============================] - 7s 498us/step - loss: 0.2857 - binary_crossentropy: 0.2857 - acc: 0.8802 - val_loss: 0.5063 - val_binary_crossentropy: 0.5063 - val_acc: 0.7909\n",
      "Epoch 394/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.2807 - binary_crossentropy: 0.2807 - acc: 0.8824 - val_loss: 0.5229 - val_binary_crossentropy: 0.5229 - val_acc: 0.7852\n",
      "Epoch 395/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.2852 - binary_crossentropy: 0.2852 - acc: 0.8786 - val_loss: 0.5142 - val_binary_crossentropy: 0.5142 - val_acc: 0.7888\n",
      "Epoch 396/10000\n",
      "14052/14052 [==============================] - 6s 458us/step - loss: 0.2828 - binary_crossentropy: 0.2828 - acc: 0.8826 - val_loss: 1.1380 - val_binary_crossentropy: 1.1380 - val_acc: 0.6499\n",
      "Epoch 397/10000\n",
      "14052/14052 [==============================] - 6s 458us/step - loss: 0.3028 - binary_crossentropy: 0.3028 - acc: 0.8718 - val_loss: 0.4849 - val_binary_crossentropy: 0.4849 - val_acc: 0.7877\n",
      "Epoch 398/10000\n",
      "14052/14052 [==============================] - 6s 456us/step - loss: 0.2780 - binary_crossentropy: 0.2780 - acc: 0.8809 - val_loss: 0.5261 - val_binary_crossentropy: 0.5261 - val_acc: 0.7845\n",
      "Epoch 399/10000\n",
      "14052/14052 [==============================] - 6s 457us/step - loss: 0.2914 - binary_crossentropy: 0.2914 - acc: 0.8799 - val_loss: 0.6936 - val_binary_crossentropy: 0.6936 - val_acc: 0.7060\n",
      "Epoch 400/10000\n",
      "14052/14052 [==============================] - 6s 457us/step - loss: 0.2976 - binary_crossentropy: 0.2976 - acc: 0.8734 - val_loss: 0.5059 - val_binary_crossentropy: 0.5059 - val_acc: 0.7768\n",
      "Epoch 401/10000\n",
      "14052/14052 [==============================] - 6s 458us/step - loss: 0.2882 - binary_crossentropy: 0.2882 - acc: 0.8768 - val_loss: 0.5008 - val_binary_crossentropy: 0.5008 - val_acc: 0.7928\n",
      "Epoch 402/10000\n",
      "14052/14052 [==============================] - 6s 462us/step - loss: 0.2864 - binary_crossentropy: 0.2864 - acc: 0.8780 - val_loss: 0.6022 - val_binary_crossentropy: 0.6022 - val_acc: 0.7549\n",
      "Epoch 403/10000\n",
      "14052/14052 [==============================] - 6s 460us/step - loss: 0.2925 - binary_crossentropy: 0.2925 - acc: 0.8754 - val_loss: 0.5318 - val_binary_crossentropy: 0.5318 - val_acc: 0.7832\n",
      "Epoch 404/10000\n",
      "14052/14052 [==============================] - 7s 464us/step - loss: 0.2868 - binary_crossentropy: 0.2868 - acc: 0.8772 - val_loss: 0.5088 - val_binary_crossentropy: 0.5088 - val_acc: 0.7875\n",
      "Epoch 405/10000\n",
      "14052/14052 [==============================] - 6s 462us/step - loss: 0.2873 - binary_crossentropy: 0.2873 - acc: 0.8762 - val_loss: 0.6586 - val_binary_crossentropy: 0.6586 - val_acc: 0.7438\n",
      "Epoch 406/10000\n",
      "14052/14052 [==============================] - 7s 464us/step - loss: 0.2861 - binary_crossentropy: 0.2861 - acc: 0.8810 - val_loss: 0.4932 - val_binary_crossentropy: 0.4932 - val_acc: 0.7852\n",
      "Epoch 407/10000\n",
      "14052/14052 [==============================] - 7s 467us/step - loss: 0.2834 - binary_crossentropy: 0.2834 - acc: 0.8831 - val_loss: 0.5131 - val_binary_crossentropy: 0.5131 - val_acc: 0.7871\n",
      "Epoch 408/10000\n",
      "14052/14052 [==============================] - 7s 465us/step - loss: 0.2861 - binary_crossentropy: 0.2861 - acc: 0.8772 - val_loss: 0.5447 - val_binary_crossentropy: 0.5447 - val_acc: 0.7487\n",
      "Epoch 409/10000\n",
      "14052/14052 [==============================] - 7s 463us/step - loss: 0.2901 - binary_crossentropy: 0.2901 - acc: 0.8743 - val_loss: 0.4851 - val_binary_crossentropy: 0.4851 - val_acc: 0.7896\n",
      "Epoch 410/10000\n",
      "14052/14052 [==============================] - 7s 468us/step - loss: 0.2864 - binary_crossentropy: 0.2864 - acc: 0.8769 - val_loss: 0.5484 - val_binary_crossentropy: 0.5484 - val_acc: 0.7747\n",
      "Epoch 411/10000\n",
      "14052/14052 [==============================] - 7s 465us/step - loss: 0.2828 - binary_crossentropy: 0.2828 - acc: 0.8814 - val_loss: 0.5022 - val_binary_crossentropy: 0.5022 - val_acc: 0.7903\n",
      "Epoch 412/10000\n",
      "14052/14052 [==============================] - 7s 464us/step - loss: 0.2840 - binary_crossentropy: 0.2840 - acc: 0.8797 - val_loss: 0.4972 - val_binary_crossentropy: 0.4972 - val_acc: 0.7888\n",
      "Epoch 413/10000\n",
      "14052/14052 [==============================] - 7s 465us/step - loss: 0.2862 - binary_crossentropy: 0.2862 - acc: 0.8791 - val_loss: 1.0835 - val_binary_crossentropy: 1.0835 - val_acc: 0.6407\n",
      "Epoch 414/10000\n",
      "14052/14052 [==============================] - 7s 467us/step - loss: 0.2928 - binary_crossentropy: 0.2928 - acc: 0.8760 - val_loss: 0.4933 - val_binary_crossentropy: 0.4933 - val_acc: 0.7901\n",
      "Epoch 415/10000\n",
      "14052/14052 [==============================] - 7s 469us/step - loss: 0.2908 - binary_crossentropy: 0.2908 - acc: 0.8797 - val_loss: 0.5015 - val_binary_crossentropy: 0.5015 - val_acc: 0.7886\n",
      "Epoch 416/10000\n",
      "14052/14052 [==============================] - 7s 466us/step - loss: 0.2801 - binary_crossentropy: 0.2801 - acc: 0.8826 - val_loss: 0.4945 - val_binary_crossentropy: 0.4945 - val_acc: 0.7847\n",
      "Epoch 417/10000\n",
      "14052/14052 [==============================] - 7s 471us/step - loss: 0.2746 - binary_crossentropy: 0.2746 - acc: 0.8865 - val_loss: 0.5142 - val_binary_crossentropy: 0.5142 - val_acc: 0.7877\n",
      "Epoch 418/10000\n",
      "14052/14052 [==============================] - 7s 465us/step - loss: 0.2815 - binary_crossentropy: 0.2815 - acc: 0.8787 - val_loss: 0.5430 - val_binary_crossentropy: 0.5430 - val_acc: 0.7798\n",
      "Epoch 419/10000\n",
      "14052/14052 [==============================] - 7s 468us/step - loss: 0.2850 - binary_crossentropy: 0.2850 - acc: 0.8787 - val_loss: 0.5203 - val_binary_crossentropy: 0.5203 - val_acc: 0.7834\n",
      "Epoch 420/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.2721 - binary_crossentropy: 0.2721 - acc: 0.8840 - val_loss: 0.5560 - val_binary_crossentropy: 0.5560 - val_acc: 0.7775\n",
      "Epoch 421/10000\n",
      "14052/14052 [==============================] - 7s 468us/step - loss: 0.2863 - binary_crossentropy: 0.2863 - acc: 0.8795 - val_loss: 0.6148 - val_binary_crossentropy: 0.6148 - val_acc: 0.7531\n",
      "Epoch 422/10000\n",
      "14052/14052 [==============================] - 7s 467us/step - loss: 0.2839 - binary_crossentropy: 0.2839 - acc: 0.8793 - val_loss: 0.5194 - val_binary_crossentropy: 0.5194 - val_acc: 0.7802\n",
      "Epoch 423/10000\n",
      "14052/14052 [==============================] - 7s 475us/step - loss: 0.2806 - binary_crossentropy: 0.2806 - acc: 0.8818 - val_loss: 0.5064 - val_binary_crossentropy: 0.5064 - val_acc: 0.7820\n",
      "Epoch 424/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.2683 - binary_crossentropy: 0.2683 - acc: 0.8907 - val_loss: 0.5210 - val_binary_crossentropy: 0.5210 - val_acc: 0.7854\n",
      "Epoch 425/10000\n",
      "14052/14052 [==============================] - 7s 475us/step - loss: 0.2735 - binary_crossentropy: 0.2735 - acc: 0.8844 - val_loss: 0.5155 - val_binary_crossentropy: 0.5155 - val_acc: 0.7843\n",
      "Epoch 426/10000\n",
      "14052/14052 [==============================] - 7s 472us/step - loss: 0.2733 - binary_crossentropy: 0.2733 - acc: 0.8838 - val_loss: 0.5058 - val_binary_crossentropy: 0.5058 - val_acc: 0.7892\n",
      "Epoch 427/10000\n",
      "14052/14052 [==============================] - 7s 527us/step - loss: 0.2809 - binary_crossentropy: 0.2809 - acc: 0.8809 - val_loss: 0.8053 - val_binary_crossentropy: 0.8053 - val_acc: 0.7179\n",
      "Epoch 428/10000\n",
      "14052/14052 [==============================] - 7s 476us/step - loss: 0.2839 - binary_crossentropy: 0.2839 - acc: 0.8775 - val_loss: 0.5103 - val_binary_crossentropy: 0.5103 - val_acc: 0.7888\n",
      "Epoch 429/10000\n",
      "14052/14052 [==============================] - 7s 474us/step - loss: 0.2766 - binary_crossentropy: 0.2766 - acc: 0.8833 - val_loss: 0.5276 - val_binary_crossentropy: 0.5276 - val_acc: 0.7918\n",
      "Epoch 430/10000\n",
      "14052/14052 [==============================] - 7s 471us/step - loss: 0.2788 - binary_crossentropy: 0.2788 - acc: 0.8789 - val_loss: 0.5107 - val_binary_crossentropy: 0.5107 - val_acc: 0.7916\n",
      "Epoch 431/10000\n",
      "14052/14052 [==============================] - 7s 470us/step - loss: 0.2788 - binary_crossentropy: 0.2788 - acc: 0.8827 - val_loss: 0.5060 - val_binary_crossentropy: 0.5060 - val_acc: 0.7898\n",
      "Epoch 432/10000\n",
      "14052/14052 [==============================] - 7s 478us/step - loss: 0.2787 - binary_crossentropy: 0.2787 - acc: 0.8839 - val_loss: 0.8695 - val_binary_crossentropy: 0.8695 - val_acc: 0.6889\n",
      "Epoch 433/10000\n",
      "14052/14052 [==============================] - 7s 478us/step - loss: 0.2906 - binary_crossentropy: 0.2906 - acc: 0.8794 - val_loss: 0.5117 - val_binary_crossentropy: 0.5117 - val_acc: 0.7849\n",
      "Epoch 434/10000\n",
      "14052/14052 [==============================] - 7s 485us/step - loss: 0.2789 - binary_crossentropy: 0.2789 - acc: 0.8819 - val_loss: 0.5143 - val_binary_crossentropy: 0.5143 - val_acc: 0.7813\n",
      "Epoch 435/10000\n",
      "14052/14052 [==============================] - 7s 505us/step - loss: 0.2756 - binary_crossentropy: 0.2756 - acc: 0.8843 - val_loss: 0.6726 - val_binary_crossentropy: 0.6726 - val_acc: 0.7120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 436/10000\n",
      "14052/14052 [==============================] - 7s 494us/step - loss: 0.2945 - binary_crossentropy: 0.2945 - acc: 0.8732 - val_loss: 0.5238 - val_binary_crossentropy: 0.5238 - val_acc: 0.7659\n",
      "Epoch 437/10000\n",
      "  384/14052 [..............................] - ETA: 6s - loss: 0.2813 - binary_crossentropy: 0.2813 - acc: 0.8854"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-92fc8843f557>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhist2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1002\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1003\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1236\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1237\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2482\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2483\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['binary_crossentropy', 'accuracy'])\n",
    "hist2 = model.fit(X_train, y_train, epochs=10000, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4978668941979522"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.accuracy_score(y_val, model.predict(X_val).round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22928</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23065</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3    4    5    6    7    8    9   ...   290  291  292  \\\n",
       "22928  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
       "23065  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN  NaN   \n",
       "\n",
       "       293  294  295  296  297  298  299  \n",
       "22928  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "23065  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[2 rows x 300 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prepping Data for Conv Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_scores = np.load('./data/review_scores.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 50)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_scores.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shuffled_ind = list(range(len(review_scores)))\n",
    "random.shuffle(shuffled_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.save('./data/shuffled_ind.npy', shuffled_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shuffled_ind = np.load('./data/shuffled_ind.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'review_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-4b3135be397f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'review_scores' is not defined"
     ]
    }
   ],
   "source": [
    "len(review_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ind = shuffled_ind[0: int(0.6*25000)]\n",
    "val_ind = shuffled_ind[int(0.6*25000) : int(0.6*25000 + 0.2*25000)]\n",
    "test_ind = shuffled_ind[int(0.6*25000 + 0.2*25000):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = review_scores[train_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"./data/X_train_conv\", X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_val = review_scores[val_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"./data/X_val_conv\", X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del review_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_df = pd.read_csv(\"./data/reviews_df.csv\", encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = reviews_df.iloc[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = y[train_ind]\n",
    "y_val = y[val_ind]\n",
    "y_test = y[test_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train.to_pickle('./data/y_train_conv')\n",
    "y_val.to_pickle('./data/y_val_conv')\n",
    "y_test.to_pickle('./data/y_test_conv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jared\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Convolution1D, MaxPooling1D,Flatten, Dense, Dropout, LeakyReLU\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, Nadam\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.load('./data/X_train_conv.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_val = np.load('./data/X_val_conv.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train[:-4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = pd.read_pickle('./data/y_train_conv')\n",
    "y_val = pd.read_pickle('./data/y_val_conv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = y_train[:-4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#highest is 70\n",
    "model = Sequential()\n",
    "model.add(Convolution1D(128, 10,\n",
    "                 input_shape=X_train.shape[1:]))\n",
    "\n",
    "\n",
    "model.add(Convolution1D(64, 5))\n",
    "\n",
    "model.add(Convolution1D(32, 5))\n",
    "\n",
    "\n",
    "model.add(Convolution1D(16, 5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500))\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(100))\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#highest is 80\n",
    "model = Sequential()\n",
    "model.add(Convolution1D(128, 10,\n",
    "                 input_shape=X_train.shape[1:]))\n",
    "model.add(LeakyReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=8, strides=2))\n",
    "\n",
    "model.add(Convolution1D(64, 5))\n",
    "model.add(LeakyReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=8, strides=2))\n",
    "\n",
    "model.add(Convolution1D(32, 5))\n",
    "model.add(LeakyReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=8, strides=2))\n",
    "\n",
    "model.add(Convolution1D(16, 5))\n",
    "model.add(LeakyReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=8, strides=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500))\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(100))\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.001, nesterov=True)\n",
    "nadam = Nadam(lr=0.0001, schedule_decay=0.00004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11000 samples, validate on 5000 samples\n",
      "Epoch 1/1000\n",
      "11000/11000 [==============================] - 128s 12ms/step - loss: 0.7142 - acc: 0.5099 - val_loss: 0.6917 - val_acc: 0.5396\n",
      "Epoch 2/1000\n",
      "11000/11000 [==============================] - 18s 2ms/step - loss: 0.6780 - acc: 0.5753 - val_loss: 0.6664 - val_acc: 0.5976\n",
      "Epoch 3/1000\n",
      "11000/11000 [==============================] - 18s 2ms/step - loss: 0.6362 - acc: 0.6409 - val_loss: 0.6298 - val_acc: 0.6554\n",
      "Epoch 4/1000\n",
      "11000/11000 [==============================] - 18s 2ms/step - loss: 0.6022 - acc: 0.6799 - val_loss: 0.6075 - val_acc: 0.6728\n",
      "Epoch 5/1000\n",
      "11000/11000 [==============================] - 19s 2ms/step - loss: 0.5776 - acc: 0.7008 - val_loss: 0.5925 - val_acc: 0.6806\n",
      "Epoch 6/1000\n",
      "11000/11000 [==============================] - 19s 2ms/step - loss: 0.5606 - acc: 0.7167 - val_loss: 0.5839 - val_acc: 0.6960\n",
      "Epoch 7/1000\n",
      "11000/11000 [==============================] - 18s 2ms/step - loss: 0.5472 - acc: 0.7275 - val_loss: 0.6147 - val_acc: 0.6720\n",
      "Epoch 8/1000\n",
      "11000/11000 [==============================] - 18s 2ms/step - loss: 0.5415 - acc: 0.7310 - val_loss: 0.7620 - val_acc: 0.5898\n",
      "Epoch 9/1000\n",
      "11000/11000 [==============================] - 19s 2ms/step - loss: 0.5321 - acc: 0.7361 - val_loss: 0.5708 - val_acc: 0.7002\n",
      "Epoch 10/1000\n",
      "11000/11000 [==============================] - 20s 2ms/step - loss: 0.5210 - acc: 0.7484 - val_loss: 0.5659 - val_acc: 0.7092\n",
      "Epoch 11/1000\n",
      "11000/11000 [==============================] - 19s 2ms/step - loss: 0.5171 - acc: 0.7472 - val_loss: 0.5884 - val_acc: 0.6924\n",
      "Epoch 12/1000\n",
      "11000/11000 [==============================] - 19s 2ms/step - loss: 0.5105 - acc: 0.7536 - val_loss: 0.5637 - val_acc: 0.7074\n",
      "Epoch 13/1000\n",
      "11000/11000 [==============================] - 19s 2ms/step - loss: 0.5012 - acc: 0.7572 - val_loss: 0.5624 - val_acc: 0.7114\n",
      "Epoch 14/1000\n",
      "11000/11000 [==============================] - 20s 2ms/step - loss: 0.4957 - acc: 0.7611 - val_loss: 0.5626 - val_acc: 0.7106\n",
      "Epoch 15/1000\n",
      "11000/11000 [==============================] - 19s 2ms/step - loss: 0.4883 - acc: 0.7675 - val_loss: 0.5662 - val_acc: 0.7132\n",
      "Epoch 16/1000\n",
      "11000/11000 [==============================] - 19s 2ms/step - loss: 0.4845 - acc: 0.7685 - val_loss: 0.5679 - val_acc: 0.7078\n",
      "Epoch 17/1000\n",
      "11000/11000 [==============================] - 19s 2ms/step - loss: 0.4751 - acc: 0.7739 - val_loss: 0.5693 - val_acc: 0.7088\n",
      "Epoch 18/1000\n",
      "11000/11000 [==============================] - 19s 2ms/step - loss: 0.4711 - acc: 0.7792 - val_loss: 0.5842 - val_acc: 0.6998\n",
      "Epoch 19/1000\n",
      "11000/11000 [==============================] - 19s 2ms/step - loss: 0.4679 - acc: 0.7826 - val_loss: 0.5755 - val_acc: 0.7018\n",
      "Epoch 20/1000\n",
      "11000/11000 [==============================] - 19s 2ms/step - loss: 0.4582 - acc: 0.7854 - val_loss: 0.5712 - val_acc: 0.7072\n",
      "Epoch 21/1000\n",
      "11000/11000 [==============================] - 20s 2ms/step - loss: 0.4550 - acc: 0.7871 - val_loss: 0.5776 - val_acc: 0.7074\n",
      "Epoch 22/1000\n",
      "11000/11000 [==============================] - 20s 2ms/step - loss: 0.4426 - acc: 0.7960 - val_loss: 0.5746 - val_acc: 0.7070\n",
      "Epoch 23/1000\n",
      "11000/11000 [==============================] - 19s 2ms/step - loss: 0.4414 - acc: 0.7955 - val_loss: 0.5764 - val_acc: 0.7036\n",
      "Epoch 24/1000\n",
      "11000/11000 [==============================] - 19s 2ms/step - loss: 0.4352 - acc: 0.7992 - val_loss: 0.5973 - val_acc: 0.7004\n",
      "Epoch 25/1000\n",
      "11000/11000 [==============================] - 19s 2ms/step - loss: 0.4255 - acc: 0.8075 - val_loss: 0.5875 - val_acc: 0.7002\n",
      "Epoch 26/1000\n",
      "11000/11000 [==============================] - 20s 2ms/step - loss: 0.4205 - acc: 0.8061 - val_loss: 0.5938 - val_acc: 0.7054\n",
      "Epoch 27/1000\n",
      "11000/11000 [==============================] - 19s 2ms/step - loss: 0.4157 - acc: 0.8145 - val_loss: 0.6382 - val_acc: 0.6794\n",
      "Epoch 28/1000\n",
      "11000/11000 [==============================] - 19s 2ms/step - loss: 0.4075 - acc: 0.8169 - val_loss: 0.6174 - val_acc: 0.6894\n",
      "Epoch 29/1000\n",
      " 2240/11000 [=====>........................] - ETA: 12s - loss: 0.4052 - acc: 0.8192"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-0eb40b1b13eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhist2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1002\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1003\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1236\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1237\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2482\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2483\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1102\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1103\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1104\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m     \"\"\"\n\u001b[1;32m--> 492\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "hist2 = model.fit(X_train, y_train, epochs=1000, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
